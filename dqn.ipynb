{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea1c753-0343-4c5c-b296-dae63cfdf905",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "\n",
    "The interaction between these two models is key to the functioning of DDPG:\n",
    "\n",
    "1. The Actor generates an action given the current state.\n",
    "\n",
    "2. This action, along with the state, is evaluated by the Critic to compute the value.\n",
    "\n",
    "3. The Criticâ€™s output (the value) is used to update both the Critic and the Actor.\n",
    "   The Critic updates its weights to better predict future rewards, while the Actor\n",
    "   uses the value gradient (provided by the Critic) to update its policy to generate\n",
    "   more rewarding actions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457094eb-a0e3-4cba-bb12-24a0304a9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cc191e-32ae-4023-804f-f65fad9370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = self.reset()\n",
    "        self.goal = np.zeros((256,))  # The goal is a zero vector of size 256\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize a state with 256 continuous values between -1 and 1\n",
    "        self.state = np.random.uniform(-1, 1, size=(256,))\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)  # Ensure each action component is between -1 and 1\n",
    "        self.state += action  # Update the state by taking the action.\n",
    "        self.state = np.clip(self.state, -1, 1)  # Ensure state remains within the desired range\n",
    "        reward = -np.linalg.norm(self.state - self.goal)  # Use norm as the reward function\n",
    "        done = np.linalg.norm(self.state - self.goal) < 0.1  # Termination condition based on distance\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.uniform(-1, 1, size=(256,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7e2ede-cc8c-43da-aa6d-c31a1728f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(action_dimension) * mu\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8ac4eb-cd8f-4f92-a5a3-4219ac1e98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    \"\"\"\n",
    "    The Actor model, also known as the policy network, is responsible for directly determining the action\n",
    "    to take given the current state of the environment.\n",
    "     \n",
    "    Its main objectives are:\n",
    "\n",
    "    1. Action Selection:\n",
    "\n",
    "       The Actor takes the current state of the environment as input and outputs the best\n",
    "       perceived action to take.\n",
    "\n",
    "    2. Policy Approximation:\n",
    "    \n",
    "       The Actor model represents the policy function of the agent. It approximates\n",
    "       the optimal policy mapping states to actions that maximize the long-term reward.\n",
    "\n",
    "    3. Exploration and Exploitation:\n",
    "    \n",
    "       During training, the Actor's policy is often augmented with noise (e.g., Ornstein-Uhlenbeck process)\n",
    "       to encourage exploration of the state space. This helps in discovering more about the environment\n",
    "       and avoids the pitfall of getting trapped in local optima.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        inputs = layers.Input(shape=(256,))\n",
    "        out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "        out = layers.Dense(512, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(256, activation=\"tanh\")(out)  # Output a 256-dimensional action vector\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0314d542-b8f2-4f9d-bf23-c4f85fbe4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \"\"\"\n",
    "    The Critic model, also known as the value network, evaluates the action taken by the Actor by computing\n",
    "    the value function. The main functions of the Critic in DDPG are:\n",
    "\n",
    "    1. Value Estimation:\n",
    "    \n",
    "       The Critic estimates the value of the current state-action pair. This value is a\n",
    "       measure of the expected future rewards that can be obtained from that state-action pair. It helps\n",
    "       in evaluating how good the action taken by the Actor is.\n",
    "\n",
    "    2. Training the Actor:\n",
    "    \n",
    "       The Critic assists in training the Actor by providing gradients of the value\n",
    "       function with respect to the actions taken. This feedback helps the Actor adjust its parameters\n",
    "       to produce better actions that can lead to higher rewards.\n",
    "\n",
    "    3. Temporal Difference Learning:\n",
    "    \n",
    "       The Critic uses a technique called Temporal Difference (TD) learning\n",
    "       to update its own weights. It compares the predicted value of the current state-action pair with\n",
    "       the reward received from the environment plus the predicted value of the next state-action pair,\n",
    "       allowing it to adjust its value predictions to be more accurate.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = layers.Input(shape=(256,))\n",
    "        state_out = layers.Dense(512, activation=\"relu\")(state_input)\n",
    "        action_input = layers.Input(shape=(256,))\n",
    "        action_out = layers.Dense(512, activation=\"relu\")(action_input)\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "        outputs = layers.Dense(1)(out)  # Output a single value representing the Q-value\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e797063c-12d3-4450-b88d-6b530e2e1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"\n",
    "        Ensure that each experience added is long enough\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample only those entries that have a complete sequence\n",
    "        \"\"\"\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d97eed-1bbc-4143-8ade-a519c3a73c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(0.002)\n",
    "\n",
    "env = Environment()\n",
    "noise_process = OUNoise(action_dimension=256)\n",
    "buffer = ReplayBuffer(10000)\n",
    "\n",
    "TOTAL_EPISODES = 100\n",
    "DISCOUNT_FACTOR = 0.97\n",
    "STEPS_FORWARD = 10\n",
    "TRAIN_BUFFER_SIZE = 10\n",
    "MAX_STEPS_PER_EPISODE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63946bf4-8e4b-4bf9-a354-1953f753cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_and_critic(batch):\n",
    "    print(f\"Training with {len(batch)} episode sequences.\")\n",
    "    for sequence in batch:\n",
    "        states, actions, rewards, next_states, dones = zip(*sequence)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones)\n",
    "        update_networks(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81ed9dd9-7386-4dc3-9fd4-4ae2cc73603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_networks(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Actor and Critic networks using a batch of experience tuples.\n",
    "    \n",
    "    This function performs two main tasks:\n",
    "\n",
    "    1. Updates the Critic network by minimizing the Mean Squared Error between the Critic's predicted value \n",
    "       of the current state-action pairs and the target values. The target values are calculated using the \n",
    "       Bellman equation, which incorporates rewards obtained and the discounted future values estimated by \n",
    "       the target Critic network for the next state-action pairs.\n",
    "       \n",
    "    2. Updates the Actor network by maximizing the expected return as estimated by the Critic network. \n",
    "       This is achieved by using the gradient of the Critic's output with respect to the actions, which are \n",
    "       generated by the Actor network. The Actor's parameters are updated in the direction that improves \n",
    "       its policy, leading to higher predicted values from the Critic.\n",
    "    \"\"\"\n",
    "    max_step = min(len(rewards), STEPS_FORWARD)\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Calculate the Critic model's value of the given state-action pairs.\n",
    "        critic_value = critic.model([states, actions], training=True)\n",
    "        assert critic_value.shape[0] == max_step\n",
    "\n",
    "        # Estimate the best action for the given next states\n",
    "        target_actions = actor.model(next_states.reshape(-1, 256), training=True)\n",
    "        assert target_actions.shape[0] == max_step\n",
    "\n",
    "        # Estimate the value of the optimal future rewards that can be obtained from a given state-action pair\n",
    "        future_rewards = critic.model([next_states.reshape(-1, 256), target_actions], training=True).numpy().flatten()\n",
    "        assert future_rewards.shape[0] == max_step\n",
    "\n",
    "        # Estimate the value of the optimal future rewards that can be obtained from a given state-action pair,\n",
    "        # using n-step TD target calculation\n",
    "        n_step_rewards = np.array([\n",
    "            DISCOUNT_FACTOR**i * rewards[i] * (1 - dones[i])\n",
    "            for i in range(max_step - 1)\n",
    "        ])\n",
    "        assert n_step_rewards.shape[0] == max_step - 1\n",
    "\n",
    "        # Calculate the target values using the n-step rewards and future rewards\n",
    "        y = n_step_rewards + (DISCOUNT_FACTOR**max_step) * future_rewards[-1] * (1 - dones[-1])\n",
    "\n",
    "        # Calculate the loss between the predicted value and the target value.\n",
    "        # The goal is to minimize the difference between the estimated value of the current state\n",
    "        # (as predicted by the Critic) and the expected optimal future value (also estimated by\n",
    "        # the Critic but based on the next state and the action proposed by the Actor).\n",
    "        critic_loss = tf.reduce_mean(tf.square(y - critic_value))\n",
    "        print(f\"Critic loss: {critic_loss}\")\n",
    "\n",
    "    # Calculate the gradients of the Critic model with respect to the loss.\n",
    "    critic_grad = tape.gradient(critic_loss, critic.model.trainable_variables)\n",
    "\n",
    "    # Update the Critic model's weights using the gradients. \n",
    "    critic_optimizer.apply_gradients(zip(critic_grad, critic.model.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Calculate the loss for the Actor model. The goal is to maximize the expected return.\n",
    "        actions = actor.model(states)\n",
    "\n",
    "        # The Critic's value of the state-action pairs is used as the loss for the Actor.\n",
    "        critic_value = critic.model([states, actions])\n",
    "\n",
    "        # The Actor's loss is the negative of the Critic's value of the state-action pairs.\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "        print(f\"Actor loss: {actor_loss}\")\n",
    "\n",
    "    # Calculate the gradients of the Actor model with respect to the loss.\n",
    "    actor_grad = tape.gradient(actor_loss, actor.model.trainable_variables)\n",
    "\n",
    "    # Update the Actor model's weights using the gradients.\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5b3fa-6fe5-47c5-bebb-e9db3c5a8797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 0, Step: 0\n",
      "Next Action: [-1.819\n",
      "Step reward: -11.913473802709847, Next State: [-0.\n",
      "Total episode reward: -11.913473802709847\n",
      "Episode: 0, Step: 1\n",
      "Next Action: [-1.729\n",
      "Step reward: -14.38648371206444, Next State: [-1. \n",
      "Total episode reward: -26.299957514774285\n",
      "Episode: 0, Step: 2\n",
      "Next Action: [-1.397\n",
      "Step reward: -15.09034817951579, Next State: [-1. \n",
      "Total episode reward: -41.39030569429008\n",
      "Episode: 0, Step: 3\n",
      "Next Action: [-1.485\n",
      "Step reward: -15.406369484842314, Next State: [-1.\n",
      "Total episode reward: -56.79667517913239\n",
      "Episode: 0, Step: 4\n",
      "Next Action: [-1.246\n",
      "Step reward: -15.618709195725415, Next State: [-1.\n",
      "Total episode reward: -72.41538437485781\n",
      "Episode: 0, Step: 5\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.730933105177495, Next State: [-1.\n",
      "Total episode reward: -88.14631748003531\n",
      "Episode: 0, Step: 6\n",
      "Next Action: [-0.960\n",
      "Step reward: -15.76522431843969, Next State: [-1. \n",
      "Total episode reward: -103.911541798475\n",
      "Episode: 0, Step: 7\n",
      "Next Action: [-1.121\n",
      "Step reward: -15.774982643922167, Next State: [-1.\n",
      "Total episode reward: -119.68652444239717\n",
      "Episode: 0, Step: 8\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.78001173765827, Next State: [-1. \n",
      "Total episode reward: -135.46653618005544\n",
      "Episode: 0, Step: 9\n",
      "Next Action: [-1.067\n",
      "Step reward: -15.82894106132573, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.29547724138118\n",
      "Episode: 0, Step: 10\n",
      "Next Action: [-0.823\n",
      "Step reward: -15.869635208147033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.1651124495282\n",
      "Episode: 0, Step: 11\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.87173954681073, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.03685199633895\n",
      "Episode: 0, Step: 12\n",
      "Next Action: [-1.575\n",
      "Step reward: -15.881010640823256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -198.91786263716222\n",
      "Episode: 0, Step: 13\n",
      "Next Action: [-1.495\n",
      "Step reward: -15.877654230533206, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.79551686769543\n",
      "Episode: 0, Step: 14\n",
      "Next Action: [-1.192\n",
      "Step reward: -15.853435238577761, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.64895210627319\n",
      "Episode: 0, Step: 15\n",
      "Next Action: [-1.170\n",
      "Step reward: -15.8340520765291, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.4830041828023\n",
      "Episode: 0, Step: 16\n",
      "Next Action: [-0.981\n",
      "Step reward: -15.811816944192037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.29482112699435\n",
      "Episode: 0, Step: 17\n",
      "Next Action: [-0.660\n",
      "Step reward: -15.818204605033833, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.1130257320282\n",
      "Episode: 0, Step: 18\n",
      "Next Action: [-0.688\n",
      "Step reward: -15.864305053588176, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -293.97733078561635\n",
      "Episode: 0, Step: 19\n",
      "Next Action: [-0.648\n",
      "Step reward: -15.900319110346224, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -309.8776498959626\n",
      "Episode: 0, Step: 20\n",
      "Next Action: [-0.461\n",
      "Step reward: -15.954168526509145, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -325.8318184224717\n",
      "Episode: 0, Step: 21\n",
      "Next Action: [-0.740\n",
      "Step reward: -15.925427423015352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.7572458454871\n",
      "Episode: 0, Step: 22\n",
      "Next Action: [-0.657\n",
      "Step reward: -15.909220345226705, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -357.6664661907138\n",
      "Episode: 0, Step: 23\n",
      "Next Action: [-0.836\n",
      "Step reward: -15.875335046297625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -373.5418012370114\n",
      "Episode: 0, Step: 24\n",
      "Next Action: [-1.007\n",
      "Step reward: -15.915007213689913, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.4568084507013\n",
      "Episode: 0, Step: 25\n",
      "Next Action: [-0.626\n",
      "Step reward: -15.921459657429397, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.3782681081307\n",
      "Episode: 0, Step: 26\n",
      "Next Action: [-0.291\n",
      "Step reward: -15.88671618765853, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.26498429578925\n",
      "Episode: 0, Step: 27\n",
      "Next Action: [-3.715\n",
      "Step reward: -15.875577291383413, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.14056158717267\n",
      "Episode: 0, Step: 28\n",
      "Next Action: [-0.497\n",
      "Step reward: -15.8516411584131, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -452.99220274558576\n",
      "Episode: 0, Step: 29\n",
      "Next Action: [-0.831\n",
      "Step reward: -15.8463899567253, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -468.8385927023111\n",
      "Episode: 0, Step: 30\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.819746777420136, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -484.65833947973124\n",
      "Episode: 0, Step: 31\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.772244039617632, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -500.4305835193489\n",
      "Episode: 0, Step: 32\n",
      "Next Action: [-0.872\n",
      "Step reward: -15.73082346493998, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.1614069842889\n",
      "Episode: 0, Step: 33\n",
      "Next Action: [-0.895\n",
      "Step reward: -15.707674662441724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -531.8690816467306\n",
      "Episode: 0, Step: 34\n",
      "Next Action: [-0.858\n",
      "Step reward: -15.72286859053645, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -547.591950237267\n",
      "Episode: 0, Step: 35\n",
      "Next Action: [-0.784\n",
      "Step reward: -15.735148832952817, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -563.3270990702199\n",
      "Episode: 0, Step: 36\n",
      "Next Action: [-1.058\n",
      "Step reward: -15.799340741653955, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -579.1264398118739\n",
      "Episode: 0, Step: 37\n",
      "Next Action: [-0.701\n",
      "Step reward: -15.867236741605076, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -594.993676553479\n",
      "Episode: 0, Step: 38\n",
      "Next Action: [-0.647\n",
      "Step reward: -15.889011581350355, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -610.8826881348293\n",
      "Episode: 0, Step: 39\n",
      "Next Action: [-8.925\n",
      "Step reward: -15.891686684848347, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -626.7743748196776\n",
      "Episode: 0, Step: 40\n",
      "Next Action: [-0.723\n",
      "Step reward: -15.88273776409924, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -642.6571125837769\n",
      "Episode: 0, Step: 41\n",
      "Next Action: [-0.790\n",
      "Step reward: -15.873890169220092, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -658.531002752997\n",
      "Episode: 0, Step: 42\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.854270233902506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -674.3852729868995\n",
      "Episode: 0, Step: 43\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.836269499361903, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.2215424862615\n",
      "Episode: 0, Step: 44\n",
      "Next Action: [-1.083\n",
      "Step reward: -15.83700832561814, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.0585508118796\n",
      "Episode: 0, Step: 45\n",
      "Next Action: [-1.261\n",
      "Step reward: -15.821764072218825, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -721.8803148840984\n",
      "Episode: 0, Step: 46\n",
      "Next Action: [-1.413\n",
      "Step reward: -15.803715547487622, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -737.684030431586\n",
      "Episode: 0, Step: 47\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.780358390265942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -753.464388821852\n",
      "Episode: 0, Step: 48\n",
      "Next Action: [-1.163\n",
      "Step reward: -15.7806778342415, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -769.2450666560935\n",
      "Episode: 0, Step: 49\n",
      "Next Action: [-0.951\n",
      "Step reward: -15.755440765504801, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -785.0005074215983\n",
      "Episode: 0, Step: 50\n",
      "Next Action: [-1.129\n",
      "Step reward: -15.8165176228897, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -800.817025044488\n",
      "Episode: 0, Step: 51\n",
      "Next Action: [-0.901\n",
      "Step reward: -15.829505049563133, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -816.6465300940512\n",
      "Episode: 0, Step: 52\n",
      "Next Action: [-0.745\n",
      "Step reward: -15.822475000283443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -832.4690050943346\n",
      "Episode: 0, Step: 53\n",
      "Next Action: [-6.342\n",
      "Step reward: -15.863111055981264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -848.3321161503159\n",
      "Episode: 0, Step: 54\n",
      "Next Action: [-0.610\n",
      "Step reward: -15.882209551329348, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -864.2143257016452\n",
      "Episode: 0, Step: 55\n",
      "Next Action: [-0.502\n",
      "Step reward: -15.883334983969803, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -880.097660685615\n",
      "Episode: 0, Step: 56\n",
      "Next Action: [-0.697\n",
      "Step reward: -15.89469379557008, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -895.9923544811851\n",
      "Episode: 0, Step: 57\n",
      "Next Action: [-6.287\n",
      "Step reward: -15.891083161842976, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -911.8834376430281\n",
      "Episode: 0, Step: 58\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.863863741967153, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -927.7473013849952\n",
      "Episode: 0, Step: 59\n",
      "Next Action: [-0.583\n",
      "Step reward: -15.86501516959119, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -943.6123165545864\n",
      "Episode: 0, Step: 60\n",
      "Next Action: [-0.474\n",
      "Step reward: -15.864209063442253, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -959.4765256180286\n",
      "Episode: 0, Step: 61\n",
      "Next Action: [-0.600\n",
      "Step reward: -15.845268494704147, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -975.3217941127327\n",
      "Episode: 0, Step: 62\n",
      "Next Action: [-0.751\n",
      "Step reward: -15.812141231803514, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -991.1339353445362\n",
      "Episode: 0, Step: 63\n",
      "Next Action: [-0.795\n",
      "Step reward: -15.821241538648804, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1006.955176883185\n",
      "Episode: 0, Step: 64\n",
      "Next Action: [-7.619\n",
      "Step reward: -15.865331020155248, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1022.8205079033403\n",
      "Episode: 0, Step: 65\n",
      "Next Action: [-6.933\n",
      "Step reward: -15.891220769752962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1038.7117286730931\n",
      "Episode: 0, Step: 66\n",
      "Next Action: [-1.038\n",
      "Step reward: -15.859012482714409, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1054.5707411558076\n",
      "Episode: 0, Step: 67\n",
      "Next Action: [-0.833\n",
      "Step reward: -15.847397581703426, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1070.418138737511\n",
      "Episode: 0, Step: 68\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.86709562286627, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1086.2852343603774\n",
      "Episode: 0, Step: 69\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.8729716177952, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1102.1582059781726\n",
      "Episode: 0, Step: 70\n",
      "Next Action: [-1.250\n",
      "Step reward: -15.849470325446385, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1118.007676303619\n",
      "Episode: 0, Step: 71\n",
      "Next Action: [-1.509\n",
      "Step reward: -15.835147035654794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1133.8428233392738\n",
      "Episode: 0, Step: 72\n",
      "Next Action: [-1.527\n",
      "Step reward: -15.827951169717707, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1149.6707745089914\n",
      "Episode: 0, Step: 73\n",
      "Next Action: [-1.614\n",
      "Step reward: -15.825994487715265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1165.4967689967066\n",
      "Episode: 0, Step: 74\n",
      "Next Action: [-1.557\n",
      "Step reward: -15.79334657571296, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1181.2901155724196\n",
      "Episode: 0, Step: 75\n",
      "Next Action: [-1.617\n",
      "Step reward: -15.831388898336897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1197.1215044707565\n",
      "Episode: 0, Step: 76\n",
      "Next Action: [-1.748\n",
      "Step reward: -15.872225231492214, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1212.9937297022486\n",
      "Episode: 0, Step: 77\n",
      "Next Action: [-1.694\n",
      "Step reward: -15.880613307268789, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1228.8743430095174\n",
      "Episode: 0, Step: 78\n",
      "Next Action: [-1.337\n",
      "Step reward: -15.874928373138674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1244.749271382656\n",
      "Episode: 0, Step: 79\n",
      "Next Action: [-1.253\n",
      "Step reward: -15.887405644617386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1260.6366770272732\n",
      "Episode: 0, Step: 80\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.891587811849416, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1276.5282648391226\n",
      "Episode: 0, Step: 81\n",
      "Next Action: [-0.874\n",
      "Step reward: -15.924219721716684, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1292.4524845608394\n",
      "Episode: 0, Step: 82\n",
      "Next Action: [-0.935\n",
      "Step reward: -15.903760022733627, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1308.356244583573\n",
      "Episode: 0, Step: 83\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.898383441179213, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1324.2546280247523\n",
      "Episode: 0, Step: 84\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.898524198736071, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1340.1531522234884\n",
      "Episode: 0, Step: 85\n",
      "Next Action: [-1.245\n",
      "Step reward: -15.916898061880591, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1356.070050285369\n",
      "Episode: 0, Step: 86\n",
      "Next Action: [-1.171\n",
      "Step reward: -15.90065745892495, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1371.970707744294\n",
      "Episode: 0, Step: 87\n",
      "Next Action: [-1.491\n",
      "Step reward: -15.893651710008408, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1387.8643594543025\n",
      "Episode: 0, Step: 88\n",
      "Next Action: [-1.227\n",
      "Step reward: -15.876332001111349, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1403.740691455414\n",
      "Episode: 0, Step: 89\n",
      "Next Action: [-1.220\n",
      "Step reward: -15.840603296540058, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1419.5812947519541\n",
      "Episode: 0, Step: 90\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.78826846966278, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1435.369563221617\n",
      "Episode: 0, Step: 91\n",
      "Next Action: [-0.761\n",
      "Step reward: -15.762543721736414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1451.1321069433534\n",
      "Episode: 0, Step: 92\n",
      "Next Action: [-0.922\n",
      "Step reward: -15.804206175238866, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1466.9363131185924\n",
      "Episode: 0, Step: 93\n",
      "Next Action: [-0.817\n",
      "Step reward: -15.783596056676624, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1482.7199091752689\n",
      "Episode: 0, Step: 94\n",
      "Next Action: [-0.816\n",
      "Step reward: -15.80833404060032, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1498.5282432158692\n",
      "Episode: 0, Step: 95\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.790755768519539, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1514.3189989843888\n",
      "Episode: 0, Step: 96\n",
      "Next Action: [-8.704\n",
      "Step reward: -15.79913007130647, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1530.1181290556954\n",
      "Episode: 0, Step: 97\n",
      "Next Action: [-0.821\n",
      "Step reward: -15.775556907044852, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1545.8936859627402\n",
      "Episode: 0, Step: 98\n",
      "Next Action: [-0.777\n",
      "Step reward: -15.730054678438234, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1561.6237406411783\n",
      "Episode: 0, Step: 99\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.7164505954871, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1577.3401912366653\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 55.98374557495117\n",
      "Actor loss: 63.00311279296875\n",
      "Critic loss: 45.43841552734375\n",
      "Actor loss: 42.27207565307617\n",
      "Critic loss: 13.291372299194336\n",
      "Actor loss: 41.95943069458008\n",
      "Critic loss: 1.7687842845916748\n",
      "Actor loss: 46.61201477050781\n",
      "Critic loss: 5.240729331970215\n",
      "Actor loss: 40.43035888671875\n",
      "Critic loss: 6.654526710510254\n",
      "Actor loss: 35.89545822143555\n",
      "Critic loss: 30.635679244995117\n",
      "Actor loss: 42.774574279785156\n",
      "Critic loss: 12.811288833618164\n",
      "Actor loss: 36.60053634643555\n",
      "Critic loss: 29.09845733642578\n",
      "Actor loss: 47.350425720214844\n",
      "Critic loss: 15.34473991394043\n",
      "Actor loss: 29.404983520507812\n",
      "Episode: 1\n",
      "Episode: 1, Step: 0\n",
      "Next Action: [-0.368\n",
      "Step reward: -11.721757371892057, Next State: [ 0.\n",
      "Total episode reward: -11.721757371892057\n",
      "Episode: 1, Step: 1\n",
      "Next Action: [-0.444\n",
      "Step reward: -14.792708992646125, Next State: [ 0.\n",
      "Total episode reward: -26.51446636453818\n",
      "Episode: 1, Step: 2\n",
      "Next Action: [-0.603\n",
      "Step reward: -15.314174850004385, Next State: [-0.\n",
      "Total episode reward: -41.828641214542564\n",
      "Episode: 1, Step: 3\n",
      "Next Action: [-6.777\n",
      "Step reward: -15.582412646193514, Next State: [-1.\n",
      "Total episode reward: -57.411053860736075\n",
      "Episode: 1, Step: 4\n",
      "Next Action: [-0.450\n",
      "Step reward: -15.616159256229878, Next State: [-1.\n",
      "Total episode reward: -73.02721311696595\n",
      "Episode: 1, Step: 5\n",
      "Next Action: [-6.582\n",
      "Step reward: -15.633045840466774, Next State: [-1.\n",
      "Total episode reward: -88.66025895743273\n",
      "Episode: 1, Step: 6\n",
      "Next Action: [-0.698\n",
      "Step reward: -15.719778276988803, Next State: [-1.\n",
      "Total episode reward: -104.38003723442154\n",
      "Episode: 1, Step: 7\n",
      "Next Action: [-0.849\n",
      "Step reward: -15.75048668304148, Next State: [-1. \n",
      "Total episode reward: -120.13052391746302\n",
      "Episode: 1, Step: 8\n",
      "Next Action: [-1.032\n",
      "Step reward: -15.790661617877987, Next State: [-1.\n",
      "Total episode reward: -135.921185535341\n",
      "Episode: 1, Step: 9\n",
      "Next Action: [-1.489\n",
      "Step reward: -15.84278718929073, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.76397272463174\n",
      "Episode: 1, Step: 10\n",
      "Next Action: [-1.411\n",
      "Step reward: -15.82286972480553, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.58684244943728\n",
      "Episode: 1, Step: 11\n",
      "Next Action: [-1.537\n",
      "Step reward: -15.82391070504666, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.41075315448396\n",
      "Episode: 1, Step: 12\n",
      "Next Action: [-1.545\n",
      "Step reward: -15.853934788217865, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.2646879427018\n",
      "Episode: 1, Step: 13\n",
      "Next Action: [-1.545\n",
      "Step reward: -15.8773002414493, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -215.1419881841511\n",
      "Episode: 1, Step: 14\n",
      "Next Action: [-1.560\n",
      "Step reward: -15.879129519603412, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.02111770375453\n",
      "Episode: 1, Step: 15\n",
      "Next Action: [-1.352\n",
      "Step reward: -15.875933782831838, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.89705148658638\n",
      "Episode: 1, Step: 16\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.900483079554466, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.7975345661408\n",
      "Episode: 1, Step: 17\n",
      "Next Action: [-0.970\n",
      "Step reward: -15.911737040155675, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.7092716062965\n",
      "Episode: 1, Step: 18\n",
      "Next Action: [-0.687\n",
      "Step reward: -15.907381203040512, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.61665280933704\n",
      "Episode: 1, Step: 19\n",
      "Next Action: [-0.685\n",
      "Step reward: -15.896475769844065, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.5131285791811\n",
      "Episode: 1, Step: 20\n",
      "Next Action: [-0.473\n",
      "Step reward: -15.898041688887755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.4111702680689\n",
      "Episode: 1, Step: 21\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.898301659339978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -342.3094719274088\n",
      "Episode: 1, Step: 22\n",
      "Next Action: [-1.035\n",
      "Step reward: -15.87505547816454, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.18452740557336\n",
      "Episode: 1, Step: 23\n",
      "Next Action: [-0.799\n",
      "Step reward: -15.835317752587024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.01984515816036\n",
      "Episode: 1, Step: 24\n",
      "Next Action: [-0.712\n",
      "Step reward: -15.792329453623605, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.812174611784\n",
      "Episode: 1, Step: 25\n",
      "Next Action: [-0.521\n",
      "Step reward: -15.805465724506071, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.61764033629004\n",
      "Episode: 1, Step: 26\n",
      "Next Action: [-0.436\n",
      "Step reward: -15.866825224295537, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.4844655605856\n",
      "Episode: 1, Step: 27\n",
      "Next Action: [-0.865\n",
      "Step reward: -15.896652680779507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.3811182413651\n",
      "Episode: 1, Step: 28\n",
      "Next Action: [-0.987\n",
      "Step reward: -15.902057519125554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.2831757604906\n",
      "Episode: 1, Step: 29\n",
      "Next Action: [-1.086\n",
      "Step reward: -15.917248009223837, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -469.20042376971446\n",
      "Episode: 1, Step: 30\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.915957316942178, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -485.11638108665665\n",
      "Episode: 1, Step: 31\n",
      "Next Action: [-1.255\n",
      "Step reward: -15.913590614987884, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -501.0299717016445\n",
      "Episode: 1, Step: 32\n",
      "Next Action: [-1.337\n",
      "Step reward: -15.890358098326237, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.9203297999708\n",
      "Episode: 1, Step: 33\n",
      "Next Action: [-1.543\n",
      "Step reward: -15.874854617842368, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.7951844178132\n",
      "Episode: 1, Step: 34\n",
      "Next Action: [-1.342\n",
      "Step reward: -15.862149344344168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.6573337621574\n",
      "Episode: 1, Step: 35\n",
      "Next Action: [-1.332\n",
      "Step reward: -15.86061474829546, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.5179485104528\n",
      "Episode: 1, Step: 36\n",
      "Next Action: [-1.597\n",
      "Step reward: -15.832910252023156, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.3508587624759\n",
      "Episode: 1, Step: 37\n",
      "Next Action: [-1.359\n",
      "Step reward: -15.838632616745116, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -596.1894913792211\n",
      "Episode: 1, Step: 38\n",
      "Next Action: [-1.705\n",
      "Step reward: -15.832327374771248, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -612.0218187539923\n",
      "Episode: 1, Step: 39\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.807478287976988, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -627.8292970419693\n",
      "Episode: 1, Step: 40\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.814012046726647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -643.643309088696\n",
      "Episode: 1, Step: 41\n",
      "Next Action: [-9.228\n",
      "Step reward: -15.807608109014163, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -659.4509171977102\n",
      "Episode: 1, Step: 42\n",
      "Next Action: [-0.984\n",
      "Step reward: -15.788901523346079, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -675.2398187210563\n",
      "Episode: 1, Step: 43\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.747290523247836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.9871092443041\n",
      "Episode: 1, Step: 44\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.778521951166942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.765631195471\n",
      "Episode: 1, Step: 45\n",
      "Next Action: [-0.986\n",
      "Step reward: -15.823568929291733, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -722.5892001247627\n",
      "Episode: 1, Step: 46\n",
      "Next Action: [-0.598\n",
      "Step reward: -15.87091061852197, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -738.4601107432846\n",
      "Episode: 1, Step: 47\n",
      "Next Action: [-7.421\n",
      "Step reward: -15.892581223963864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -754.3526919672485\n",
      "Episode: 1, Step: 48\n",
      "Next Action: [-0.833\n",
      "Step reward: -15.876184934126696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.2288769013752\n",
      "Episode: 1, Step: 49\n",
      "Next Action: [-0.557\n",
      "Step reward: -15.882498460965477, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -786.1113753623407\n",
      "Episode: 1, Step: 50\n",
      "Next Action: [-0.928\n",
      "Step reward: -15.898210936969813, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -802.0095862993105\n",
      "Episode: 1, Step: 51\n",
      "Next Action: [-1.124\n",
      "Step reward: -15.944522038389866, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.9541083377004\n",
      "Episode: 1, Step: 52\n",
      "Next Action: [-1.170\n",
      "Step reward: -15.946822546128079, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -833.9009308838284\n",
      "Episode: 1, Step: 53\n",
      "Next Action: [-1.663\n",
      "Step reward: -15.928855919987061, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -849.8297868038155\n",
      "Episode: 1, Step: 54\n",
      "Next Action: [-1.476\n",
      "Step reward: -15.904072263662028, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -865.7338590674775\n",
      "Episode: 1, Step: 55\n",
      "Next Action: [-1.072\n",
      "Step reward: -15.90253461372635, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -881.6363936812039\n",
      "Episode: 1, Step: 56\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.949974544968914, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -897.5863682261728\n",
      "Episode: 1, Step: 57\n",
      "Next Action: [-0.843\n",
      "Step reward: -15.945460521971544, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -913.5318287481443\n",
      "Episode: 1, Step: 58\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.957841824094244, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -929.4896705722385\n",
      "Episode: 1, Step: 59\n",
      "Next Action: [-0.960\n",
      "Step reward: -15.958143625791449, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -945.44781419803\n",
      "Episode: 1, Step: 60\n",
      "Next Action: [-1.279\n",
      "Step reward: -15.955260729714501, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.4030749277445\n",
      "Episode: 1, Step: 61\n",
      "Next Action: [-1.293\n",
      "Step reward: -15.961572512870287, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.3646474406147\n",
      "Episode: 1, Step: 62\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.940228411438857, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.3048758520536\n",
      "Episode: 1, Step: 63\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.923556391698808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.2284322437523\n",
      "Episode: 1, Step: 64\n",
      "Next Action: [-1.012\n",
      "Step reward: -15.910854015275866, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.1392862590283\n",
      "Episode: 1, Step: 65\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.915641541320037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1041.0549278003482\n",
      "Episode: 1, Step: 66\n",
      "Next Action: [-1.029\n",
      "Step reward: -15.917772354487187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1056.9727001548354\n",
      "Episode: 1, Step: 67\n",
      "Next Action: [-0.921\n",
      "Step reward: -15.892650798056538, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1072.865350952892\n",
      "Episode: 1, Step: 68\n",
      "Next Action: [-0.780\n",
      "Step reward: -15.878777386639866, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1088.744128339532\n",
      "Episode: 1, Step: 69\n",
      "Next Action: [-0.885\n",
      "Step reward: -15.875780106028348, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1104.6199084455602\n",
      "Episode: 1, Step: 70\n",
      "Next Action: [-0.790\n",
      "Step reward: -15.885073328113956, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1120.5049817736742\n",
      "Episode: 1, Step: 71\n",
      "Next Action: [-0.967\n",
      "Step reward: -15.889144215656554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1136.3941259893309\n",
      "Episode: 1, Step: 72\n",
      "Next Action: [-0.682\n",
      "Step reward: -15.907045676406202, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1152.301171665737\n",
      "Episode: 1, Step: 73\n",
      "Next Action: [-0.480\n",
      "Step reward: -15.914450907777006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1168.215622573514\n",
      "Episode: 1, Step: 74\n",
      "Next Action: [-0.582\n",
      "Step reward: -15.899775176158698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1184.1153977496729\n",
      "Episode: 1, Step: 75\n",
      "Next Action: [-0.718\n",
      "Step reward: -15.893108130146102, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1200.008505879819\n",
      "Episode: 1, Step: 76\n",
      "Next Action: [-0.851\n",
      "Step reward: -15.89524282312437, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1215.9037487029434\n",
      "Episode: 1, Step: 77\n",
      "Next Action: [-0.862\n",
      "Step reward: -15.864086355300419, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1231.767835058244\n",
      "Episode: 1, Step: 78\n",
      "Next Action: [-0.853\n",
      "Step reward: -15.900796963000777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1247.6686320212448\n",
      "Episode: 1, Step: 79\n",
      "Next Action: [-0.970\n",
      "Step reward: -15.911546953342953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1263.5801789745876\n",
      "Episode: 1, Step: 80\n",
      "Next Action: [-0.938\n",
      "Step reward: -15.902156713574664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1279.4823356881623\n",
      "Episode: 1, Step: 81\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.940690250957744, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1295.42302593912\n",
      "Episode: 1, Step: 82\n",
      "Next Action: [-1.119\n",
      "Step reward: -15.935580141793347, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1311.3586060809132\n",
      "Episode: 1, Step: 83\n",
      "Next Action: [-0.997\n",
      "Step reward: -15.920806000708, Next State: [-1.   \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1327.279412081621\n",
      "Episode: 1, Step: 84\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.923370749862205, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1343.2027828314833\n",
      "Episode: 1, Step: 85\n",
      "Next Action: [-0.816\n",
      "Step reward: -15.914306211505993, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1359.1170890429894\n",
      "Episode: 1, Step: 86\n",
      "Next Action: [-0.841\n",
      "Step reward: -15.921594991981687, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1375.038684034971\n",
      "Episode: 1, Step: 87\n",
      "Next Action: [-0.933\n",
      "Step reward: -15.908775897798762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1390.9474599327698\n",
      "Episode: 1, Step: 88\n",
      "Next Action: [-9.504\n",
      "Step reward: -15.872736655322491, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1406.8201965880924\n",
      "Episode: 1, Step: 89\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.874383421457567, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1422.69458000955\n",
      "Episode: 1, Step: 90\n",
      "Next Action: [-1.305\n",
      "Step reward: -15.898856855680519, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1438.5934368652306\n",
      "Episode: 1, Step: 91\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.923741827465275, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1454.5171786926958\n",
      "Episode: 1, Step: 92\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.900760217711044, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1470.4179389104067\n",
      "Episode: 1, Step: 93\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.906202792179226, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1486.324141702586\n",
      "Episode: 1, Step: 94\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.859099294189244, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1502.1832409967753\n",
      "Episode: 1, Step: 95\n",
      "Next Action: [-1.168\n",
      "Step reward: -15.844454906622216, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1518.0276959033974\n",
      "Episode: 1, Step: 96\n",
      "Next Action: [-1.389\n",
      "Step reward: -15.864617491386154, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1533.8923133947835\n",
      "Episode: 1, Step: 97\n",
      "Next Action: [-1.143\n",
      "Step reward: -15.839695148269767, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1549.7320085430533\n",
      "Episode: 1, Step: 98\n",
      "Next Action: [-6.508\n",
      "Step reward: -15.85151022474299, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1565.5835187677962\n",
      "Episode: 1, Step: 99\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.84759529625585, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1581.431114064052\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 86.53705596923828\n",
      "Actor loss: 20.57675552368164\n",
      "Critic loss: 36.98616409301758\n",
      "Actor loss: 33.69622802734375\n",
      "Critic loss: 41.445404052734375\n",
      "Actor loss: 37.36647415161133\n",
      "Critic loss: 1.7274928092956543\n",
      "Actor loss: 51.35953903198242\n",
      "Critic loss: 3.0113956928253174\n",
      "Actor loss: 48.50690841674805\n",
      "Critic loss: 27.675838470458984\n",
      "Actor loss: 36.919288635253906\n",
      "Critic loss: 74.5488052368164\n",
      "Actor loss: 59.904449462890625\n",
      "Critic loss: 31.37294578552246\n",
      "Actor loss: 34.56584548950195\n",
      "Critic loss: 6.703145503997803\n",
      "Actor loss: 44.74639129638672\n",
      "Critic loss: 17.5454044342041\n",
      "Actor loss: 32.696006774902344\n",
      "Episode: 2\n",
      "Episode: 2, Step: 0\n",
      "Next Action: [-0.334\n",
      "Step reward: -11.730762082291342, Next State: [ 0.\n",
      "Total episode reward: -11.730762082291342\n",
      "Episode: 2, Step: 1\n",
      "Next Action: [ 6.427\n",
      "Step reward: -14.335724655788384, Next State: [ 0.\n",
      "Total episode reward: -26.066486738079725\n",
      "Episode: 2, Step: 2\n",
      "Next Action: [ 1.089\n",
      "Step reward: -15.352050224799193, Next State: [ 1.\n",
      "Total episode reward: -41.41853696287892\n",
      "Episode: 2, Step: 3\n",
      "Next Action: [ 1.408\n",
      "Step reward: -15.605900475091689, Next State: [ 1.\n",
      "Total episode reward: -57.024437437970604\n",
      "Episode: 2, Step: 4\n",
      "Next Action: [ 1.293\n",
      "Step reward: -15.815294615749245, Next State: [ 1.\n",
      "Total episode reward: -72.83973205371984\n",
      "Episode: 2, Step: 5\n",
      "Next Action: [ 0.999\n",
      "Step reward: -15.879441974955572, Next State: [ 1.\n",
      "Total episode reward: -88.71917402867541\n",
      "Episode: 2, Step: 6\n",
      "Next Action: [ 0.752\n",
      "Step reward: -15.945460268694521, Next State: [ 1.\n",
      "Total episode reward: -104.66463429736993\n",
      "Episode: 2, Step: 7\n",
      "Next Action: [ 0.966\n",
      "Step reward: -15.966248285770504, Next State: [ 1.\n",
      "Total episode reward: -120.63088258314043\n",
      "Episode: 2, Step: 8\n",
      "Next Action: [ 1.137\n",
      "Step reward: -15.93138989334385, Next State: [ 1. \n",
      "Total episode reward: -136.56227247648428\n",
      "Episode: 2, Step: 9\n",
      "Next Action: [ 1.109\n",
      "Step reward: -15.906828960130344, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.46910143661464\n",
      "Episode: 2, Step: 10\n",
      "Next Action: [ 1.191\n",
      "Step reward: -15.896708180941895, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.36580961755652\n",
      "Episode: 2, Step: 11\n",
      "Next Action: [ 1.057\n",
      "Step reward: -15.87294281133134, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.23875242888786\n",
      "Episode: 2, Step: 12\n",
      "Next Action: [ 0.985\n",
      "Step reward: -15.888126095067536, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.1268785239554\n",
      "Episode: 2, Step: 13\n",
      "Next Action: [ 0.927\n",
      "Step reward: -15.93567316863278, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.0625516925882\n",
      "Episode: 2, Step: 14\n",
      "Next Action: [ 0.803\n",
      "Step reward: -15.940949083951317, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.0035007765395\n",
      "Episode: 2, Step: 15\n",
      "Next Action: [ 0.870\n",
      "Step reward: -15.94589004955576, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.94939082609528\n",
      "Episode: 2, Step: 16\n",
      "Next Action: [ 1.041\n",
      "Step reward: -15.930418748754004, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.8798095748493\n",
      "Episode: 2, Step: 17\n",
      "Next Action: [ 1.226\n",
      "Step reward: -15.904029636678594, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.7838392115279\n",
      "Episode: 2, Step: 18\n",
      "Next Action: [ 0.735\n",
      "Step reward: -15.869394863560945, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -295.65323407508885\n",
      "Episode: 2, Step: 19\n",
      "Next Action: [ 0.836\n",
      "Step reward: -15.894360488364052, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -311.5475945634529\n",
      "Episode: 2, Step: 20\n",
      "Next Action: [ 0.838\n",
      "Step reward: -15.907984490559198, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -327.4555790540121\n",
      "Episode: 2, Step: 21\n",
      "Next Action: [ 0.778\n",
      "Step reward: -15.866716810700922, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -343.32229586471306\n",
      "Episode: 2, Step: 22\n",
      "Next Action: [ 0.838\n",
      "Step reward: -15.845770961963401, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -359.16806682667647\n",
      "Episode: 2, Step: 23\n",
      "Next Action: [ 9.573\n",
      "Step reward: -15.844261460564232, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.0123282872407\n",
      "Episode: 2, Step: 24\n",
      "Next Action: [ 1.458\n",
      "Step reward: -15.858902539919002, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.8712308271597\n",
      "Episode: 2, Step: 25\n",
      "Next Action: [ 1.656\n",
      "Step reward: -15.860120480392732, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -406.7313513075524\n",
      "Episode: 2, Step: 26\n",
      "Next Action: [ 1.207\n",
      "Step reward: -15.878125540807229, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -422.60947684835963\n",
      "Episode: 2, Step: 27\n",
      "Next Action: [ 1.319\n",
      "Step reward: -15.864886879269994, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -438.4743637276296\n",
      "Episode: 2, Step: 28\n",
      "Next Action: [ 1.159\n",
      "Step reward: -15.865472103810621, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -454.33983583144027\n",
      "Episode: 2, Step: 29\n",
      "Next Action: [ 1.062\n",
      "Step reward: -15.863561178033379, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -470.20339700947363\n",
      "Episode: 2, Step: 30\n",
      "Next Action: [ 0.889\n",
      "Step reward: -15.903021725465, Next State: [ 1.   \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -486.10641873493864\n",
      "Episode: 2, Step: 31\n",
      "Next Action: [ 1.056\n",
      "Step reward: -15.911986534929916, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -502.01840526986854\n",
      "Episode: 2, Step: 32\n",
      "Next Action: [ 1.288\n",
      "Step reward: -15.857798056443459, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -517.876203326312\n",
      "Episode: 2, Step: 33\n",
      "Next Action: [ 1.111\n",
      "Step reward: -15.835843732450789, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -533.7120470587628\n",
      "Episode: 2, Step: 34\n",
      "Next Action: [ 0.794\n",
      "Step reward: -15.812894883748891, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -549.5249419425116\n",
      "Episode: 2, Step: 35\n",
      "Next Action: [ 0.476\n",
      "Step reward: -15.848594891528942, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -565.3735368340406\n",
      "Episode: 2, Step: 36\n",
      "Next Action: [ 0.521\n",
      "Step reward: -15.866698513701603, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -581.2402353477422\n",
      "Episode: 2, Step: 37\n",
      "Next Action: [ 0.826\n",
      "Step reward: -15.884839140431378, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -597.1250744881736\n",
      "Episode: 2, Step: 38\n",
      "Next Action: [ 1.330\n",
      "Step reward: -15.878824558397753, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -613.0038990465714\n",
      "Episode: 2, Step: 39\n",
      "Next Action: [ 1.211\n",
      "Step reward: -15.848341153343899, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -628.8522401999153\n",
      "Episode: 2, Step: 40\n",
      "Next Action: [ 1.346\n",
      "Step reward: -15.817365092425613, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -644.6696052923409\n",
      "Episode: 2, Step: 41\n",
      "Next Action: [ 1.440\n",
      "Step reward: -15.792498511908331, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -660.4621038042492\n",
      "Episode: 2, Step: 42\n",
      "Next Action: [ 1.563\n",
      "Step reward: -15.775109838767424, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -676.2372136430166\n",
      "Episode: 2, Step: 43\n",
      "Next Action: [ 1.355\n",
      "Step reward: -15.79735532065448, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -692.0345689636711\n",
      "Episode: 2, Step: 44\n",
      "Next Action: [ 0.964\n",
      "Step reward: -15.748754969352273, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -707.7833239330233\n",
      "Episode: 2, Step: 45\n",
      "Next Action: [ 0.724\n",
      "Step reward: -15.76269477078265, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -723.546018703806\n",
      "Episode: 2, Step: 46\n",
      "Next Action: [ 0.413\n",
      "Step reward: -15.835616073833096, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -739.3816347776391\n",
      "Episode: 2, Step: 47\n",
      "Next Action: [ 0.504\n",
      "Step reward: -15.818854050891993, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -755.2004888285311\n",
      "Episode: 2, Step: 48\n",
      "Next Action: [ 0.800\n",
      "Step reward: -15.884655543925563, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -771.0851443724566\n",
      "Episode: 2, Step: 49\n",
      "Next Action: [ 0.808\n",
      "Step reward: -15.88032261668637, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -786.965466989143\n",
      "Episode: 2, Step: 50\n",
      "Next Action: [ 0.664\n",
      "Step reward: -15.872139941046042, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -802.837606930189\n",
      "Episode: 2, Step: 51\n",
      "Next Action: [ 0.827\n",
      "Step reward: -15.90043238171455, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -818.7380393119035\n",
      "Episode: 2, Step: 52\n",
      "Next Action: [ 0.782\n",
      "Step reward: -15.883945033118222, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -834.6219843450218\n",
      "Episode: 2, Step: 53\n",
      "Next Action: [ 0.833\n",
      "Step reward: -15.865380723237957, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -850.4873650682598\n",
      "Episode: 2, Step: 54\n",
      "Next Action: [ 0.822\n",
      "Step reward: -15.88692687306362, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -866.3742919413235\n",
      "Episode: 2, Step: 55\n",
      "Next Action: [ 0.822\n",
      "Step reward: -15.880567486384571, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -882.2548594277081\n",
      "Episode: 2, Step: 56\n",
      "Next Action: [ 0.653\n",
      "Step reward: -15.857605752451002, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -898.1124651801591\n",
      "Episode: 2, Step: 57\n",
      "Next Action: [ 0.360\n",
      "Step reward: -15.842618844581274, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -913.9550840247404\n",
      "Episode: 2, Step: 58\n",
      "Next Action: [ 0.573\n",
      "Step reward: -15.840501360316896, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -929.7955853850573\n",
      "Episode: 2, Step: 59\n",
      "Next Action: [ 0.571\n",
      "Step reward: -15.820775775620438, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -945.6163611606777\n",
      "Episode: 2, Step: 60\n",
      "Next Action: [ 0.608\n",
      "Step reward: -15.818880206420769, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.4352413670985\n",
      "Episode: 2, Step: 61\n",
      "Next Action: [ 0.609\n",
      "Step reward: -15.843806435140166, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.2790478022387\n",
      "Episode: 2, Step: 62\n",
      "Next Action: [ 0.580\n",
      "Step reward: -15.871047097571436, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.1500948998101\n",
      "Episode: 2, Step: 63\n",
      "Next Action: [ 0.423\n",
      "Step reward: -15.927236436373994, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.0773313361841\n",
      "Episode: 2, Step: 64\n",
      "Next Action: [ 0.359\n",
      "Step reward: -15.944663830102682, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.0219951662868\n",
      "Episode: 2, Step: 65\n",
      "Next Action: [ 0.510\n",
      "Step reward: -15.931233244469782, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1040.9532284107565\n",
      "Episode: 2, Step: 66\n",
      "Next Action: [ 0.422\n",
      "Step reward: -15.912185213317967, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1056.8654136240746\n",
      "Episode: 2, Step: 67\n",
      "Next Action: [ 0.310\n",
      "Step reward: -15.915506868307057, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1072.7809204923817\n",
      "Episode: 2, Step: 68\n",
      "Next Action: [ 0.640\n",
      "Step reward: -15.888230991206711, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1088.6691514835884\n",
      "Episode: 2, Step: 69\n",
      "Next Action: [ 0.965\n",
      "Step reward: -15.840883523387076, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1104.5100350069754\n",
      "Episode: 2, Step: 70\n",
      "Next Action: [ 0.971\n",
      "Step reward: -15.787098985301254, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1120.2971339922767\n",
      "Episode: 2, Step: 71\n",
      "Next Action: [ 0.917\n",
      "Step reward: -15.767263972146944, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1136.0643979644237\n",
      "Episode: 2, Step: 72\n",
      "Next Action: [ 1.369\n",
      "Step reward: -15.764197744595077, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1151.8285957090188\n",
      "Episode: 2, Step: 73\n",
      "Next Action: [ 1.775\n",
      "Step reward: -15.796275786230613, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1167.6248714952494\n",
      "Episode: 2, Step: 74\n",
      "Next Action: [ 1.667\n",
      "Step reward: -15.831039461532006, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1183.4559109567815\n",
      "Episode: 2, Step: 75\n",
      "Next Action: [ 1.682\n",
      "Step reward: -15.85740252112735, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1199.3133134779089\n",
      "Episode: 2, Step: 76\n",
      "Next Action: [ 1.484\n",
      "Step reward: -15.866786332215593, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1215.1800998101244\n",
      "Episode: 2, Step: 77\n",
      "Next Action: [ 1.541\n",
      "Step reward: -15.860716765485778, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1231.04081657561\n",
      "Episode: 2, Step: 78\n",
      "Next Action: [ 1.529\n",
      "Step reward: -15.860359007465503, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1246.9011755830757\n",
      "Episode: 2, Step: 79\n",
      "Next Action: [ 1.399\n",
      "Step reward: -15.860701856273328, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1262.761877439349\n",
      "Episode: 2, Step: 80\n",
      "Next Action: [ 1.063\n",
      "Step reward: -15.90057837610922, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1278.6624558154583\n",
      "Episode: 2, Step: 81\n",
      "Next Action: [ 1.192\n",
      "Step reward: -15.904652271004165, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1294.5671080864624\n",
      "Episode: 2, Step: 82\n",
      "Next Action: [ 1.260\n",
      "Step reward: -15.894592825921027, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1310.4617009123835\n",
      "Episode: 2, Step: 83\n",
      "Next Action: [ 1.340\n",
      "Step reward: -15.901008000047673, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1326.3627089124311\n",
      "Episode: 2, Step: 84\n",
      "Next Action: [ 1.402\n",
      "Step reward: -15.844771249979027, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1342.20748016241\n",
      "Episode: 2, Step: 85\n",
      "Next Action: [ 1.123\n",
      "Step reward: -15.814491395257726, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1358.0219715576677\n",
      "Episode: 2, Step: 86\n",
      "Next Action: [ 8.914\n",
      "Step reward: -15.83282664800216, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1373.8547982056698\n",
      "Episode: 2, Step: 87\n",
      "Next Action: [ 1.082\n",
      "Step reward: -15.816995989539356, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1389.6717941952093\n",
      "Episode: 2, Step: 88\n",
      "Next Action: [ 0.983\n",
      "Step reward: -15.7688667477392, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1405.4406609429484\n",
      "Episode: 2, Step: 89\n",
      "Next Action: [ 0.591\n",
      "Step reward: -15.775839745034135, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1421.2165006879825\n",
      "Episode: 2, Step: 90\n",
      "Next Action: [ 0.682\n",
      "Step reward: -15.815167784056802, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1437.0316684720392\n",
      "Episode: 2, Step: 91\n",
      "Next Action: [ 0.606\n",
      "Step reward: -15.827574828890896, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1452.85924330093\n",
      "Episode: 2, Step: 92\n",
      "Next Action: [ 0.432\n",
      "Step reward: -15.766227633198545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1468.6254709341285\n",
      "Episode: 2, Step: 93\n",
      "Next Action: [ 0.374\n",
      "Step reward: -15.73784932160799, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1484.3633202557364\n",
      "Episode: 2, Step: 94\n",
      "Next Action: [ 0.587\n",
      "Step reward: -15.794599657614185, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1500.1579199133507\n",
      "Episode: 2, Step: 95\n",
      "Next Action: [ 0.502\n",
      "Step reward: -15.794039501609388, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1515.9519594149601\n",
      "Episode: 2, Step: 96\n",
      "Next Action: [ 0.285\n",
      "Step reward: -15.77091800028147, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1531.7228774152416\n",
      "Episode: 2, Step: 97\n",
      "Next Action: [ 0.708\n",
      "Step reward: -15.762823861186444, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1547.485701276428\n",
      "Episode: 2, Step: 98\n",
      "Next Action: [ 0.508\n",
      "Step reward: -15.780912955655564, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1563.2666142320836\n",
      "Episode: 2, Step: 99\n",
      "Next Action: [ 0.769\n",
      "Step reward: -15.799297233476828, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1579.0659114655605\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 34.997039794921875\n",
      "Actor loss: 32.849666595458984\n",
      "Critic loss: 43.71762466430664\n",
      "Actor loss: 39.494834899902344\n",
      "Critic loss: 2.377255916595459\n",
      "Actor loss: 43.58856201171875\n",
      "Critic loss: 3.0149319171905518\n",
      "Actor loss: 43.75475311279297\n",
      "Critic loss: 4.052827835083008\n",
      "Actor loss: 48.190860748291016\n",
      "Critic loss: 30.003944396972656\n",
      "Actor loss: 41.8652458190918\n",
      "Critic loss: 7.230780601501465\n",
      "Actor loss: 36.16345977783203\n",
      "Critic loss: 9.449202537536621\n",
      "Actor loss: 51.990760803222656\n",
      "Critic loss: 24.4993839263916\n",
      "Actor loss: 53.65276336669922\n",
      "Critic loss: 130.39892578125\n",
      "Actor loss: 52.62853240966797\n",
      "Episode: 3\n",
      "Episode: 3, Step: 0\n",
      "Next Action: [ 0.701\n",
      "Step reward: -11.773364447262981, Next State: [ 8.\n",
      "Total episode reward: -11.773364447262981\n",
      "Episode: 3, Step: 1\n",
      "Next Action: [ 1.301\n",
      "Step reward: -14.725447901447687, Next State: [ 1.\n",
      "Total episode reward: -26.49881234871067\n",
      "Episode: 3, Step: 2\n",
      "Next Action: [ 1.107\n",
      "Step reward: -15.475992396152154, Next State: [ 1.\n",
      "Total episode reward: -41.97480474486282\n",
      "Episode: 3, Step: 3\n",
      "Next Action: [ 0.883\n",
      "Step reward: -15.704013402658783, Next State: [ 1.\n",
      "Total episode reward: -57.6788181475216\n",
      "Episode: 3, Step: 4\n",
      "Next Action: [ 0.878\n",
      "Step reward: -15.817588302377677, Next State: [ 1.\n",
      "Total episode reward: -73.49640644989928\n",
      "Episode: 3, Step: 5\n",
      "Next Action: [ 0.853\n",
      "Step reward: -15.888320755767522, Next State: [ 1.\n",
      "Total episode reward: -89.3847272056668\n",
      "Episode: 3, Step: 6\n",
      "Next Action: [ 1.094\n",
      "Step reward: -15.89898509159001, Next State: [ 1. \n",
      "Total episode reward: -105.28371229725681\n",
      "Episode: 3, Step: 7\n",
      "Next Action: [ 1.036\n",
      "Step reward: -15.846472748583265, Next State: [ 1.\n",
      "Total episode reward: -121.13018504584008\n",
      "Episode: 3, Step: 8\n",
      "Next Action: [ 1.080\n",
      "Step reward: -15.830523119336416, Next State: [ 1.\n",
      "Total episode reward: -136.9607081651765\n",
      "Episode: 3, Step: 9\n",
      "Next Action: [ 0.900\n",
      "Step reward: -15.875628269541878, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.83633643471836\n",
      "Episode: 3, Step: 10\n",
      "Next Action: [ 9.094\n",
      "Step reward: -15.851123245753804, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.68745968047216\n",
      "Episode: 3, Step: 11\n",
      "Next Action: [ 1.021\n",
      "Step reward: -15.841545307575966, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.52900498804811\n",
      "Episode: 3, Step: 12\n",
      "Next Action: [ 1.262\n",
      "Step reward: -15.870613317162427, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.39961830521054\n",
      "Episode: 3, Step: 13\n",
      "Next Action: [ 1.061\n",
      "Step reward: -15.882285423196231, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.28190372840677\n",
      "Episode: 3, Step: 14\n",
      "Next Action: [ 1.311\n",
      "Step reward: -15.887128215259272, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.16903194366606\n",
      "Episode: 3, Step: 15\n",
      "Next Action: [ 1.324\n",
      "Step reward: -15.903582459044614, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.07261440271066\n",
      "Episode: 3, Step: 16\n",
      "Next Action: [ 1.389\n",
      "Step reward: -15.883296320413406, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.9559107231241\n",
      "Episode: 3, Step: 17\n",
      "Next Action: [ 1.519\n",
      "Step reward: -15.86610548155427, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.82201620467833\n",
      "Episode: 3, Step: 18\n",
      "Next Action: [ 1.419\n",
      "Step reward: -15.81098963577663, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -295.633005840455\n",
      "Episode: 3, Step: 19\n",
      "Next Action: [ 1.299\n",
      "Step reward: -15.821060428123173, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -311.4540662685782\n",
      "Episode: 3, Step: 20\n",
      "Next Action: [ 1.161\n",
      "Step reward: -15.819080095919706, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -327.2731463644979\n",
      "Episode: 3, Step: 21\n",
      "Next Action: [ 1.473\n",
      "Step reward: -15.818620877610975, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -343.09176724210886\n",
      "Episode: 3, Step: 22\n",
      "Next Action: [ 1.531\n",
      "Step reward: -15.860023991570017, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.9517912336789\n",
      "Episode: 3, Step: 23\n",
      "Next Action: [ 1.457\n",
      "Step reward: -15.891530612393376, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.84332184607223\n",
      "Episode: 3, Step: 24\n",
      "Next Action: [ 1.597\n",
      "Step reward: -15.936742184395937, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.78006403046817\n",
      "Episode: 3, Step: 25\n",
      "Next Action: [ 1.791\n",
      "Step reward: -15.898590528729933, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -406.6786545591981\n",
      "Episode: 3, Step: 26\n",
      "Next Action: [ 1.910\n",
      "Step reward: -15.888255239234084, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -422.56690979843216\n",
      "Episode: 3, Step: 27\n",
      "Next Action: [ 2.082\n",
      "Step reward: -15.86513383795101, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -438.4320436363832\n",
      "Episode: 3, Step: 28\n",
      "Next Action: [ 1.694\n",
      "Step reward: -15.853393368894487, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -454.2854370052777\n",
      "Episode: 3, Step: 29\n",
      "Next Action: [ 1.318\n",
      "Step reward: -15.859169495146626, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -470.1446065004243\n",
      "Episode: 3, Step: 30\n",
      "Next Action: [ 1.330\n",
      "Step reward: -15.879267196507744, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -486.023873696932\n",
      "Episode: 3, Step: 31\n",
      "Next Action: [ 1.259\n",
      "Step reward: -15.906729576934294, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -501.9306032738663\n",
      "Episode: 3, Step: 32\n",
      "Next Action: [ 1.096\n",
      "Step reward: -15.896007788255988, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -517.8266110621223\n",
      "Episode: 3, Step: 33\n",
      "Next Action: [ 1.120\n",
      "Step reward: -15.884696652240217, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -533.7113077143625\n",
      "Episode: 3, Step: 34\n",
      "Next Action: [ 1.385\n",
      "Step reward: -15.896116733009517, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -549.6074244473721\n",
      "Episode: 3, Step: 35\n",
      "Next Action: [ 1.448\n",
      "Step reward: -15.934974913784954, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -565.542399361157\n",
      "Episode: 3, Step: 36\n",
      "Next Action: [ 1.138\n",
      "Step reward: -15.885182331196821, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -581.4275816923538\n",
      "Episode: 3, Step: 37\n",
      "Next Action: [ 1.200\n",
      "Step reward: -15.848676437071958, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -597.2762581294257\n",
      "Episode: 3, Step: 38\n",
      "Next Action: [ 0.978\n",
      "Step reward: -15.863078092340068, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -613.1393362217658\n",
      "Episode: 3, Step: 39\n",
      "Next Action: [ 0.551\n",
      "Step reward: -15.87373016260628, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -629.0130663843721\n",
      "Episode: 3, Step: 40\n",
      "Next Action: [ 0.384\n",
      "Step reward: -15.865956878464496, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -644.8790232628367\n",
      "Episode: 3, Step: 41\n",
      "Next Action: [ 0.372\n",
      "Step reward: -15.917320782863897, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -660.7963440457006\n",
      "Episode: 3, Step: 42\n",
      "Next Action: [ 5.359\n",
      "Step reward: -15.953329685591479, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -676.7496737312921\n",
      "Episode: 3, Step: 43\n",
      "Next Action: [ 0.765\n",
      "Step reward: -15.917581583272108, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -692.6672553145642\n",
      "Episode: 3, Step: 44\n",
      "Next Action: [ 0.632\n",
      "Step reward: -15.881168157692226, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -708.5484234722563\n",
      "Episode: 3, Step: 45\n",
      "Next Action: [ 0.536\n",
      "Step reward: -15.868649442014314, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -724.4170729142706\n",
      "Episode: 3, Step: 46\n",
      "Next Action: [ 0.903\n",
      "Step reward: -15.85772321720907, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -740.2747961314797\n",
      "Episode: 3, Step: 47\n",
      "Next Action: [ 0.757\n",
      "Step reward: -15.881877449963437, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -756.1566735814431\n",
      "Episode: 3, Step: 48\n",
      "Next Action: [ 0.724\n",
      "Step reward: -15.881596417296485, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -772.0382699987396\n",
      "Episode: 3, Step: 49\n",
      "Next Action: [ 0.591\n",
      "Step reward: -15.867645517307256, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -787.9059155160468\n",
      "Episode: 3, Step: 50\n",
      "Next Action: [ 0.417\n",
      "Step reward: -15.852558401523533, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -803.7584739175703\n",
      "Episode: 3, Step: 51\n",
      "Next Action: [ 0.167\n",
      "Step reward: -15.828624252137372, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -819.5870981697077\n",
      "Episode: 3, Step: 52\n",
      "Next Action: [ 0.195\n",
      "Step reward: -15.809364106257458, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -835.3964622759652\n",
      "Episode: 3, Step: 53\n",
      "Next Action: [ 0.014\n",
      "Step reward: -15.79130740827215, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -851.1877696842373\n",
      "Episode: 3, Step: 54\n",
      "Next Action: [ 1.535\n",
      "Step reward: -15.800808494249635, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -866.9885781784869\n",
      "Episode: 3, Step: 55\n",
      "Next Action: [-0.149\n",
      "Step reward: -15.861619571175558, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -882.8501977496625\n",
      "Episode: 3, Step: 56\n",
      "Next Action: [ 0.142\n",
      "Step reward: -15.899649947078258, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -898.7498476967407\n",
      "Episode: 3, Step: 57\n",
      "Next Action: [ 0.091\n",
      "Step reward: -15.861153841891097, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -914.6110015386319\n",
      "Episode: 3, Step: 58\n",
      "Next Action: [-0.065\n",
      "Step reward: -15.808825924778509, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -930.4198274634103\n",
      "Episode: 3, Step: 59\n",
      "Next Action: [-0.325\n",
      "Step reward: -15.742605149752816, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -946.1624326131631\n",
      "Episode: 3, Step: 60\n",
      "Next Action: [ 0.016\n",
      "Step reward: -15.727100832939774, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.8895334461029\n",
      "Episode: 3, Step: 61\n",
      "Next Action: [ 3.650\n",
      "Step reward: -15.747020475555534, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.6365539216584\n",
      "Episode: 3, Step: 62\n",
      "Next Action: [ 0.720\n",
      "Step reward: -15.778171882780569, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.4147258044391\n",
      "Episode: 3, Step: 63\n",
      "Next Action: [ 0.588\n",
      "Step reward: -15.823289331327471, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.2380151357665\n",
      "Episode: 3, Step: 64\n",
      "Next Action: [ 0.644\n",
      "Step reward: -15.804302393819698, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.0423175295862\n",
      "Episode: 3, Step: 65\n",
      "Next Action: [ 0.698\n",
      "Step reward: -15.791194764987392, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1040.8335122945737\n",
      "Episode: 3, Step: 66\n",
      "Next Action: [ 0.852\n",
      "Step reward: -15.774755464510672, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1056.6082677590844\n",
      "Episode: 3, Step: 67\n",
      "Next Action: [ 1.067\n",
      "Step reward: -15.785400622046673, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1072.393668381131\n",
      "Episode: 3, Step: 68\n",
      "Next Action: [ 1.117\n",
      "Step reward: -15.813140794701999, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1088.2068091758329\n",
      "Episode: 3, Step: 69\n",
      "Next Action: [ 0.537\n",
      "Step reward: -15.828077534061999, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1104.0348867098949\n",
      "Episode: 3, Step: 70\n",
      "Next Action: [ 0.650\n",
      "Step reward: -15.793181903255718, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1119.8280686131507\n",
      "Episode: 3, Step: 71\n",
      "Next Action: [ 0.883\n",
      "Step reward: -15.822994252585218, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1135.6510628657359\n",
      "Episode: 3, Step: 72\n",
      "Next Action: [ 0.641\n",
      "Step reward: -15.909269418020415, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1151.5603322837562\n",
      "Episode: 3, Step: 73\n",
      "Next Action: [ 7.986\n",
      "Step reward: -15.919488046705252, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1167.4798203304615\n",
      "Episode: 3, Step: 74\n",
      "Next Action: [ 0.854\n",
      "Step reward: -15.866109890549431, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1183.345930221011\n",
      "Episode: 3, Step: 75\n",
      "Next Action: [ 0.782\n",
      "Step reward: -15.848692869215922, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1199.194623090227\n",
      "Episode: 3, Step: 76\n",
      "Next Action: [ 1.105\n",
      "Step reward: -15.85699478496212, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1215.051617875189\n",
      "Episode: 3, Step: 77\n",
      "Next Action: [ 1.010\n",
      "Step reward: -15.878354747439996, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1230.929972622629\n",
      "Episode: 3, Step: 78\n",
      "Next Action: [ 1.074\n",
      "Step reward: -15.893268755679452, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1246.8232413783085\n",
      "Episode: 3, Step: 79\n",
      "Next Action: [ 0.895\n",
      "Step reward: -15.843482393757906, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1262.6667237720665\n",
      "Episode: 3, Step: 80\n",
      "Next Action: [ 7.764\n",
      "Step reward: -15.84183836578301, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1278.5085621378494\n",
      "Episode: 3, Step: 81\n",
      "Next Action: [ 0.542\n",
      "Step reward: -15.866263706146414, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1294.3748258439957\n",
      "Episode: 3, Step: 82\n",
      "Next Action: [ 0.694\n",
      "Step reward: -15.855246668258355, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1310.230072512254\n",
      "Episode: 3, Step: 83\n",
      "Next Action: [ 0.933\n",
      "Step reward: -15.870827225117255, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1326.1008997373713\n",
      "Episode: 3, Step: 84\n",
      "Next Action: [ 0.759\n",
      "Step reward: -15.85112690632685, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1341.9520266436982\n",
      "Episode: 3, Step: 85\n",
      "Next Action: [ 1.041\n",
      "Step reward: -15.887580092408554, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1357.8396067361068\n",
      "Episode: 3, Step: 86\n",
      "Next Action: [ 1.264\n",
      "Step reward: -15.915519025579503, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1373.7551257616863\n",
      "Episode: 3, Step: 87\n",
      "Next Action: [ 1.029\n",
      "Step reward: -15.91554892658719, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1389.6706746882735\n",
      "Episode: 3, Step: 88\n",
      "Next Action: [ 1.105\n",
      "Step reward: -15.88653033109326, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1405.5572050193668\n",
      "Episode: 3, Step: 89\n",
      "Next Action: [ 0.943\n",
      "Step reward: -15.863790455854732, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1421.4209954752216\n",
      "Episode: 3, Step: 90\n",
      "Next Action: [ 0.955\n",
      "Step reward: -15.843168873146988, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1437.2641643483687\n",
      "Episode: 3, Step: 91\n",
      "Next Action: [ 1.268\n",
      "Step reward: -15.824617607673959, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1453.0887819560426\n",
      "Episode: 3, Step: 92\n",
      "Next Action: [ 1.314\n",
      "Step reward: -15.854863337676342, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1468.9436452937189\n",
      "Episode: 3, Step: 93\n",
      "Next Action: [ 1.026\n",
      "Step reward: -15.882380002535802, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1484.8260252962548\n",
      "Episode: 3, Step: 94\n",
      "Next Action: [ 0.997\n",
      "Step reward: -15.902010995561783, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1500.7280362918166\n",
      "Episode: 3, Step: 95\n",
      "Next Action: [ 1.314\n",
      "Step reward: -15.889631225323285, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1516.6176675171398\n",
      "Episode: 3, Step: 96\n",
      "Next Action: [ 1.478\n",
      "Step reward: -15.910609148393483, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1532.5282766655332\n",
      "Episode: 3, Step: 97\n",
      "Next Action: [ 1.353\n",
      "Step reward: -15.942855201893057, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1548.4711318674263\n",
      "Episode: 3, Step: 98\n",
      "Next Action: [ 1.374\n",
      "Step reward: -15.892779699182892, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1564.3639115666092\n",
      "Episode: 3, Step: 99\n",
      "Next Action: [ 1.088\n",
      "Step reward: -15.832579285737905, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1580.196490852347\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 7.614552021026611\n",
      "Actor loss: 43.65788650512695\n",
      "Critic loss: 65.58140563964844\n",
      "Actor loss: 52.16817092895508\n",
      "Critic loss: 22.852832794189453\n",
      "Actor loss: 40.30323791503906\n",
      "Critic loss: 55.73109436035156\n",
      "Actor loss: 41.790828704833984\n",
      "Critic loss: 3.785038948059082\n",
      "Actor loss: 26.669879913330078\n",
      "Critic loss: 4.493443012237549\n",
      "Actor loss: 49.99766159057617\n",
      "Critic loss: 54.793941497802734\n",
      "Actor loss: 23.047861099243164\n",
      "Critic loss: 25.317672729492188\n",
      "Actor loss: 34.682373046875\n",
      "Critic loss: 6.966654300689697\n",
      "Actor loss: 26.75070571899414\n",
      "Critic loss: 5.698638439178467\n",
      "Actor loss: 24.48348045349121\n",
      "Episode: 4\n",
      "Episode: 4, Step: 0\n",
      "Next Action: [ 0.634\n",
      "Step reward: -11.274250859521375, Next State: [ 0.\n",
      "Total episode reward: -11.274250859521375\n",
      "Episode: 4, Step: 1\n",
      "Next Action: [ 0.677\n",
      "Step reward: -14.499956567415026, Next State: [ 8.\n",
      "Total episode reward: -25.7742074269364\n",
      "Episode: 4, Step: 2\n",
      "Next Action: [ 0.734\n",
      "Step reward: -15.133107926815699, Next State: [ 1.\n",
      "Total episode reward: -40.9073153537521\n",
      "Episode: 4, Step: 3\n",
      "Next Action: [ 0.609\n",
      "Step reward: -15.3972148024058, Next State: [ 1.  \n",
      "Total episode reward: -56.304530156157895\n",
      "Episode: 4, Step: 4\n",
      "Next Action: [ 0.856\n",
      "Step reward: -15.634427491301032, Next State: [ 1.\n",
      "Total episode reward: -71.93895764745892\n",
      "Episode: 4, Step: 5\n",
      "Next Action: [ 1.119\n",
      "Step reward: -15.734718635579975, Next State: [ 1.\n",
      "Total episode reward: -87.6736762830389\n",
      "Episode: 4, Step: 6\n",
      "Next Action: [ 0.856\n",
      "Step reward: -15.782660865199944, Next State: [ 1.\n",
      "Total episode reward: -103.45633714823884\n",
      "Episode: 4, Step: 7\n",
      "Next Action: [ 0.593\n",
      "Step reward: -15.803514969462217, Next State: [ 1.\n",
      "Total episode reward: -119.25985211770106\n",
      "Episode: 4, Step: 8\n",
      "Next Action: [ 0.230\n",
      "Step reward: -15.83205431284831, Next State: [ 1. \n",
      "Total episode reward: -135.09190643054936\n",
      "Episode: 4, Step: 9\n",
      "Next Action: [ 0.474\n",
      "Step reward: -15.874106086832862, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -150.9660125173822\n",
      "Episode: 4, Step: 10\n",
      "Next Action: [ 0.665\n",
      "Step reward: -15.876441950367894, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -166.84245446775012\n",
      "Episode: 4, Step: 11\n",
      "Next Action: [ 1.047\n",
      "Step reward: -15.851612709199287, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -182.6940671769494\n",
      "Episode: 4, Step: 12\n",
      "Next Action: [ 1.088\n",
      "Step reward: -15.794528323792099, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -198.4885955007415\n",
      "Episode: 4, Step: 13\n",
      "Next Action: [ 1.575\n",
      "Step reward: -15.82488047410143, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.31347597484293\n",
      "Episode: 4, Step: 14\n",
      "Next Action: [ 1.767\n",
      "Step reward: -15.854786116325526, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.16826209116846\n",
      "Episode: 4, Step: 15\n",
      "Next Action: [ 1.821\n",
      "Step reward: -15.875979378689305, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.04424146985775\n",
      "Episode: 4, Step: 16\n",
      "Next Action: [ 1.545\n",
      "Step reward: -15.898827348485002, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -261.94306881834274\n",
      "Episode: 4, Step: 17\n",
      "Next Action: [ 1.381\n",
      "Step reward: -15.873005131369732, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -277.81607394971246\n",
      "Episode: 4, Step: 18\n",
      "Next Action: [ 1.646\n",
      "Step reward: -15.859014735036602, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -293.6750886847491\n",
      "Episode: 4, Step: 19\n",
      "Next Action: [ 1.849\n",
      "Step reward: -15.794339300962644, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -309.4694279857117\n",
      "Episode: 4, Step: 20\n",
      "Next Action: [ 1.693\n",
      "Step reward: -15.782562509682554, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -325.2519904953943\n",
      "Episode: 4, Step: 21\n",
      "Next Action: [ 1.575\n",
      "Step reward: -15.816604693829868, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.06859518922414\n",
      "Episode: 4, Step: 22\n",
      "Next Action: [ 1.414\n",
      "Step reward: -15.83391094539195, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -356.9025061346161\n",
      "Episode: 4, Step: 23\n",
      "Next Action: [ 1.257\n",
      "Step reward: -15.834190529407342, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -372.73669666402344\n",
      "Episode: 4, Step: 24\n",
      "Next Action: [ 0.651\n",
      "Step reward: -15.840710076643683, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -388.57740674066713\n",
      "Episode: 4, Step: 25\n",
      "Next Action: [ 0.236\n",
      "Step reward: -15.82739247529099, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -404.4047992159581\n",
      "Episode: 4, Step: 26\n",
      "Next Action: [ 3.334\n",
      "Step reward: -15.796458614617958, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -420.2012578305761\n",
      "Episode: 4, Step: 27\n",
      "Next Action: [ 0.571\n",
      "Step reward: -15.75335011442005, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -435.95460794499616\n",
      "Episode: 4, Step: 28\n",
      "Next Action: [ 0.570\n",
      "Step reward: -15.750077978528497, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -451.70468592352466\n",
      "Episode: 4, Step: 29\n",
      "Next Action: [ 0.592\n",
      "Step reward: -15.77687336331285, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -467.4815592868375\n",
      "Episode: 4, Step: 30\n",
      "Next Action: [ 0.719\n",
      "Step reward: -15.79024580469814, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -483.27180509153567\n",
      "Episode: 4, Step: 31\n",
      "Next Action: [ 0.383\n",
      "Step reward: -15.854368365810398, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -499.12617345734606\n",
      "Episode: 4, Step: 32\n",
      "Next Action: [ 0.474\n",
      "Step reward: -15.897730400701159, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -515.0239038580472\n",
      "Episode: 4, Step: 33\n",
      "Next Action: [ 0.239\n",
      "Step reward: -15.923553661513242, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -530.9474575195604\n",
      "Episode: 4, Step: 34\n",
      "Next Action: [ 0.207\n",
      "Step reward: -15.941516861285054, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -546.8889743808455\n",
      "Episode: 4, Step: 35\n",
      "Next Action: [ 0.345\n",
      "Step reward: -15.914992102820086, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -562.8039664836656\n",
      "Episode: 4, Step: 36\n",
      "Next Action: [ 0.188\n",
      "Step reward: -15.883555064136495, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -578.6875215478021\n",
      "Episode: 4, Step: 37\n",
      "Next Action: [ 0.020\n",
      "Step reward: -15.878174652120562, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -594.5656961999227\n",
      "Episode: 4, Step: 38\n",
      "Next Action: [-0.072\n",
      "Step reward: -15.903738465977908, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -610.4694346659006\n",
      "Episode: 4, Step: 39\n",
      "Next Action: [ 0.093\n",
      "Step reward: -15.91626712399356, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -626.3857017898941\n",
      "Episode: 4, Step: 40\n",
      "Next Action: [ 0.354\n",
      "Step reward: -15.917870599018022, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -642.3035723889121\n",
      "Episode: 4, Step: 41\n",
      "Next Action: [ 0.458\n",
      "Step reward: -15.917604704394748, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -658.2211770933069\n",
      "Episode: 4, Step: 42\n",
      "Next Action: [ 0.280\n",
      "Step reward: -15.897018691805256, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -674.1181957851121\n",
      "Episode: 4, Step: 43\n",
      "Next Action: [ 0.264\n",
      "Step reward: -15.885970154710881, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.004165939823\n",
      "Episode: 4, Step: 44\n",
      "Next Action: [ 0.475\n",
      "Step reward: -15.873294124185291, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -705.8774600640082\n",
      "Episode: 4, Step: 45\n",
      "Next Action: [ 0.724\n",
      "Step reward: -15.910102269063065, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -721.7875623330713\n",
      "Episode: 4, Step: 46\n",
      "Next Action: [ 0.853\n",
      "Step reward: -15.94231927592663, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -737.7298816089979\n",
      "Episode: 4, Step: 47\n",
      "Next Action: [ 1.143\n",
      "Step reward: -15.935503074029146, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -753.665384683027\n",
      "Episode: 4, Step: 48\n",
      "Next Action: [ 0.891\n",
      "Step reward: -15.929005508778133, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -769.5943901918051\n",
      "Episode: 4, Step: 49\n",
      "Next Action: [ 0.965\n",
      "Step reward: -15.921399379470467, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -785.5157895712756\n",
      "Episode: 4, Step: 50\n",
      "Next Action: [ 0.607\n",
      "Step reward: -15.922446296922013, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -801.4382358681976\n",
      "Episode: 4, Step: 51\n",
      "Next Action: [ 0.789\n",
      "Step reward: -15.942900970512717, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.3811368387103\n",
      "Episode: 4, Step: 52\n",
      "Next Action: [ 0.884\n",
      "Step reward: -15.928058504655235, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -833.3091953433656\n",
      "Episode: 4, Step: 53\n",
      "Next Action: [ 0.885\n",
      "Step reward: -15.923226473423707, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -849.2324218167893\n",
      "Episode: 4, Step: 54\n",
      "Next Action: [ 0.854\n",
      "Step reward: -15.901301773279645, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -865.1337235900689\n",
      "Episode: 4, Step: 55\n",
      "Next Action: [ 1.303\n",
      "Step reward: -15.908234629862173, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -881.0419582199311\n",
      "Episode: 4, Step: 56\n",
      "Next Action: [ 1.273\n",
      "Step reward: -15.939416855091649, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -896.9813750750227\n",
      "Episode: 4, Step: 57\n",
      "Next Action: [ 1.410\n",
      "Step reward: -15.943337390890857, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -912.9247124659136\n",
      "Episode: 4, Step: 58\n",
      "Next Action: [ 0.921\n",
      "Step reward: -15.927343869006464, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -928.85205633492\n",
      "Episode: 4, Step: 59\n",
      "Next Action: [ 1.037\n",
      "Step reward: -15.947317091977073, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -944.7993734268971\n",
      "Episode: 4, Step: 60\n",
      "Next Action: [ 1.083\n",
      "Step reward: -15.960335868572649, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -960.7597092954697\n",
      "Episode: 4, Step: 61\n",
      "Next Action: [ 1.061\n",
      "Step reward: -15.938711050265232, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -976.698420345735\n",
      "Episode: 4, Step: 62\n",
      "Next Action: [ 1.056\n",
      "Step reward: -15.898333761751998, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -992.5967541074871\n",
      "Episode: 4, Step: 63\n",
      "Next Action: [ 0.984\n",
      "Step reward: -15.853998008649777, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1008.4507521161369\n",
      "Episode: 4, Step: 64\n",
      "Next Action: [ 0.939\n",
      "Step reward: -15.869298179458024, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1024.320050295595\n",
      "Episode: 4, Step: 65\n",
      "Next Action: [ 1.184\n",
      "Step reward: -15.935678546877169, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1040.255728842472\n",
      "Episode: 4, Step: 66\n",
      "Next Action: [ 1.084\n",
      "Step reward: -15.958909720046297, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1056.2146385625183\n",
      "Episode: 4, Step: 67\n",
      "Next Action: [ 0.919\n",
      "Step reward: -15.940007962725943, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1072.1546465252443\n",
      "Episode: 4, Step: 68\n",
      "Next Action: [ 1.233\n",
      "Step reward: -15.926669247905679, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1088.08131577315\n",
      "Episode: 4, Step: 69\n",
      "Next Action: [ 1.373\n",
      "Step reward: -15.916886394957023, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1103.998202168107\n",
      "Episode: 4, Step: 70\n",
      "Next Action: [ 1.439\n",
      "Step reward: -15.899072181011318, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1119.897274349118\n",
      "Episode: 4, Step: 71\n",
      "Next Action: [ 1.628\n",
      "Step reward: -15.911239088610094, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1135.8085134377282\n",
      "Episode: 4, Step: 72\n",
      "Next Action: [ 1.465\n",
      "Step reward: -15.895357048753954, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1151.7038704864822\n",
      "Episode: 4, Step: 73\n",
      "Next Action: [ 1.011\n",
      "Step reward: -15.877247112158246, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1167.5811175986405\n",
      "Episode: 4, Step: 74\n",
      "Next Action: [ 1.418\n",
      "Step reward: -15.903461958505886, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1183.4845795571464\n",
      "Episode: 4, Step: 75\n",
      "Next Action: [ 1.401\n",
      "Step reward: -15.896732286783585, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1199.38131184393\n",
      "Episode: 4, Step: 76\n",
      "Next Action: [ 1.640\n",
      "Step reward: -15.889223653251099, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1215.270535497181\n",
      "Episode: 4, Step: 77\n",
      "Next Action: [ 1.954\n",
      "Step reward: -15.89275463283767, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1231.1632901300188\n",
      "Episode: 4, Step: 78\n",
      "Next Action: [ 2.092\n",
      "Step reward: -15.90540515398894, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1247.0686952840078\n",
      "Episode: 4, Step: 79\n",
      "Next Action: [ 1.879\n",
      "Step reward: -15.9479203183407, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1263.0166156023486\n",
      "Episode: 4, Step: 80\n",
      "Next Action: [ 1.910\n",
      "Step reward: -15.939807069460798, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1278.9564226718094\n",
      "Episode: 4, Step: 81\n",
      "Next Action: [ 1.866\n",
      "Step reward: -15.94696004180392, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1294.9033827136134\n",
      "Episode: 4, Step: 82\n",
      "Next Action: [ 1.796\n",
      "Step reward: -15.952610498400935, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1310.8559932120143\n",
      "Episode: 4, Step: 83\n",
      "Next Action: [ 1.447\n",
      "Step reward: -15.927991510441462, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1326.7839847224557\n",
      "Episode: 4, Step: 84\n",
      "Next Action: [ 1.162\n",
      "Step reward: -15.886843267197868, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1342.6708279896536\n",
      "Episode: 4, Step: 85\n",
      "Next Action: [ 1.421\n",
      "Step reward: -15.854696565642147, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1358.5255245552958\n",
      "Episode: 4, Step: 86\n",
      "Next Action: [ 1.543\n",
      "Step reward: -15.837579185571977, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1374.3631037408677\n",
      "Episode: 4, Step: 87\n",
      "Next Action: [ 1.676\n",
      "Step reward: -15.854255214257593, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1390.2173589551253\n",
      "Episode: 4, Step: 88\n",
      "Next Action: [ 1.454\n",
      "Step reward: -15.880101270679534, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1406.0974602258048\n",
      "Episode: 4, Step: 89\n",
      "Next Action: [ 1.201\n",
      "Step reward: -15.881307547860674, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1421.9787677736654\n",
      "Episode: 4, Step: 90\n",
      "Next Action: [ 1.325\n",
      "Step reward: -15.871436317674464, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1437.8502040913397\n",
      "Episode: 4, Step: 91\n",
      "Next Action: [ 1.565\n",
      "Step reward: -15.839840397651283, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1453.690044488991\n",
      "Episode: 4, Step: 92\n",
      "Next Action: [ 1.621\n",
      "Step reward: -15.828060565062273, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1469.5181050540532\n",
      "Episode: 4, Step: 93\n",
      "Next Action: [ 1.671\n",
      "Step reward: -15.858089148302213, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1485.3761942023555\n",
      "Episode: 4, Step: 94\n",
      "Next Action: [ 1.295\n",
      "Step reward: -15.861473738804644, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1501.23766794116\n",
      "Episode: 4, Step: 95\n",
      "Next Action: [ 1.311\n",
      "Step reward: -15.860800672190459, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1517.0984686133504\n",
      "Episode: 4, Step: 96\n",
      "Next Action: [ 1.192\n",
      "Step reward: -15.874049561686183, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1532.9725181750366\n",
      "Episode: 4, Step: 97\n",
      "Next Action: [ 0.874\n",
      "Step reward: -15.86706830095103, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1548.8395864759877\n",
      "Episode: 4, Step: 98\n",
      "Next Action: [ 0.943\n",
      "Step reward: -15.87975799104217, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1564.71934446703\n",
      "Episode: 4, Step: 99\n",
      "Next Action: [ 0.889\n",
      "Step reward: -15.890503510997831, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1580.6098479780278\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 5.704939842224121\n",
      "Actor loss: 46.61428451538086\n",
      "Critic loss: 49.54756546020508\n",
      "Actor loss: 24.878204345703125\n",
      "Critic loss: 17.249025344848633\n",
      "Actor loss: 26.2421875\n",
      "Critic loss: 41.103309631347656\n",
      "Actor loss: 26.38089942932129\n",
      "Critic loss: 3.01434588432312\n",
      "Actor loss: 46.91777801513672\n",
      "Critic loss: 36.77244567871094\n",
      "Actor loss: 32.77562713623047\n",
      "Critic loss: 5.081103324890137\n",
      "Actor loss: 32.77616500854492\n",
      "Critic loss: 5.062090873718262\n",
      "Actor loss: 32.52507400512695\n",
      "Critic loss: 71.64230346679688\n",
      "Actor loss: 22.577375411987305\n",
      "Critic loss: 2.834865093231201\n",
      "Actor loss: 36.035343170166016\n",
      "Episode: 5\n",
      "Episode: 5, Step: 0\n",
      "Next Action: [ 0.406\n",
      "Step reward: -12.051053990102904, Next State: [ 0.\n",
      "Total episode reward: -12.051053990102904\n",
      "Episode: 5, Step: 1\n",
      "Next Action: [ 0.874\n",
      "Step reward: -14.760301603820526, Next State: [ 1.\n",
      "Total episode reward: -26.81135559392343\n",
      "Episode: 5, Step: 2\n",
      "Next Action: [ 1.210\n",
      "Step reward: -15.377176010699452, Next State: [ 1.\n",
      "Total episode reward: -42.18853160462288\n",
      "Episode: 5, Step: 3\n",
      "Next Action: [ 1.043\n",
      "Step reward: -15.672350235445068, Next State: [ 1.\n",
      "Total episode reward: -57.86088184006795\n",
      "Episode: 5, Step: 4\n",
      "Next Action: [ 1.367\n",
      "Step reward: -15.848842179096286, Next State: [ 1.\n",
      "Total episode reward: -73.70972401916423\n",
      "Episode: 5, Step: 5\n",
      "Next Action: [ 1.277\n",
      "Step reward: -15.943774116998961, Next State: [ 1.\n",
      "Total episode reward: -89.6534981361632\n",
      "Episode: 5, Step: 6\n",
      "Next Action: [ 0.819\n",
      "Step reward: -15.946217955504805, Next State: [ 1.\n",
      "Total episode reward: -105.599716091668\n",
      "Episode: 5, Step: 7\n",
      "Next Action: [ 0.872\n",
      "Step reward: -15.92280317092939, Next State: [ 1. \n",
      "Total episode reward: -121.52251926259738\n",
      "Episode: 5, Step: 8\n",
      "Next Action: [ 0.869\n",
      "Step reward: -15.948381474328775, Next State: [ 1.\n",
      "Total episode reward: -137.47090073692615\n",
      "Episode: 5, Step: 9\n",
      "Next Action: [ 1.303\n",
      "Step reward: -15.92253382166775, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.3934345585939\n",
      "Episode: 5, Step: 10\n",
      "Next Action: [ 1.154\n",
      "Step reward: -15.910935542413982, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.3043701010079\n",
      "Episode: 5, Step: 11\n",
      "Next Action: [ 1.118\n",
      "Step reward: -15.928688749842228, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.2330588508501\n",
      "Episode: 5, Step: 12\n",
      "Next Action: [ 1.384\n",
      "Step reward: -15.89935392676769, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.13241277761782\n",
      "Episode: 5, Step: 13\n",
      "Next Action: [ 1.215\n",
      "Step reward: -15.861521727579209, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.99393450519702\n",
      "Episode: 5, Step: 14\n",
      "Next Action: [ 1.443\n",
      "Step reward: -15.88263457891069, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.8765690841077\n",
      "Episode: 5, Step: 15\n",
      "Next Action: [ 1.148\n",
      "Step reward: -15.886264637324734, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.76283372143243\n",
      "Episode: 5, Step: 16\n",
      "Next Action: [ 9.615\n",
      "Step reward: -15.8795700015579, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.6424037229903\n",
      "Episode: 5, Step: 17\n",
      "Next Action: [ 0.847\n",
      "Step reward: -15.902546441252255, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.54495016424255\n",
      "Episode: 5, Step: 18\n",
      "Next Action: [ 0.877\n",
      "Step reward: -15.933654859855253, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.4786050240978\n",
      "Episode: 5, Step: 19\n",
      "Next Action: [ 0.864\n",
      "Step reward: -15.960107161148379, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.4387121852462\n",
      "Episode: 5, Step: 20\n",
      "Next Action: [ 1.106\n",
      "Step reward: -15.950162967279553, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.3888751525257\n",
      "Episode: 5, Step: 21\n",
      "Next Action: [ 7.552\n",
      "Step reward: -15.964508444047105, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.3533835965728\n",
      "Episode: 5, Step: 22\n",
      "Next Action: [ 0.524\n",
      "Step reward: -15.956136049950045, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.30951964652286\n",
      "Episode: 5, Step: 23\n",
      "Next Action: [ 0.148\n",
      "Step reward: -15.92405388595586, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.2335735324787\n",
      "Episode: 5, Step: 24\n",
      "Next Action: [-0.076\n",
      "Step reward: -15.861778641870892, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.0953521743496\n",
      "Episode: 5, Step: 25\n",
      "Next Action: [-0.031\n",
      "Step reward: -15.869772979905074, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.9651251542547\n",
      "Episode: 5, Step: 26\n",
      "Next Action: [ 0.113\n",
      "Step reward: -15.911534318284714, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.8766594725394\n",
      "Episode: 5, Step: 27\n",
      "Next Action: [ 0.117\n",
      "Step reward: -15.932858660162317, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.8095181327017\n",
      "Episode: 5, Step: 28\n",
      "Next Action: [ 0.160\n",
      "Step reward: -15.915380670259683, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.72489880296143\n",
      "Episode: 5, Step: 29\n",
      "Next Action: [ 0.557\n",
      "Step reward: -15.90690351995705, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.6318023229185\n",
      "Episode: 5, Step: 30\n",
      "Next Action: [ 0.640\n",
      "Step reward: -15.885911487060545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.517713809979\n",
      "Episode: 5, Step: 31\n",
      "Next Action: [ 0.439\n",
      "Step reward: -15.857899635616404, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.3756134455954\n",
      "Episode: 5, Step: 32\n",
      "Next Action: [ 6.902\n",
      "Step reward: -15.82479971549507, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.2004131610904\n",
      "Episode: 5, Step: 33\n",
      "Next Action: [ 5.435\n",
      "Step reward: -15.803098929586161, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.0035120906766\n",
      "Episode: 5, Step: 34\n",
      "Next Action: [ 0.817\n",
      "Step reward: -15.859494673432286, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -550.863006764109\n",
      "Episode: 5, Step: 35\n",
      "Next Action: [ 0.785\n",
      "Step reward: -15.877082141344886, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -566.7400889054538\n",
      "Episode: 5, Step: 36\n",
      "Next Action: [ 1.038\n",
      "Step reward: -15.895582728401667, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -582.6356716338555\n",
      "Episode: 5, Step: 37\n",
      "Next Action: [ 8.013\n",
      "Step reward: -15.894220564553207, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -598.5298921984087\n",
      "Episode: 5, Step: 38\n",
      "Next Action: [ 1.024\n",
      "Step reward: -15.809096410047726, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -614.3389886084564\n",
      "Episode: 5, Step: 39\n",
      "Next Action: [ 1.265\n",
      "Step reward: -15.772432444298799, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -630.1114210527552\n",
      "Episode: 5, Step: 40\n",
      "Next Action: [ 1.333\n",
      "Step reward: -15.789607447478465, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -645.9010285002337\n",
      "Episode: 5, Step: 41\n",
      "Next Action: [ 1.150\n",
      "Step reward: -15.795909102348041, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -661.6969376025817\n",
      "Episode: 5, Step: 42\n",
      "Next Action: [ 9.045\n",
      "Step reward: -15.833933670630708, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -677.5308712732124\n",
      "Episode: 5, Step: 43\n",
      "Next Action: [ 0.947\n",
      "Step reward: -15.868379263879543, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -693.3992505370919\n",
      "Episode: 5, Step: 44\n",
      "Next Action: [ 0.756\n",
      "Step reward: -15.871689957977528, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -709.2709404950695\n",
      "Episode: 5, Step: 45\n",
      "Next Action: [ 0.796\n",
      "Step reward: -15.854942727031634, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -725.125883222101\n",
      "Episode: 5, Step: 46\n",
      "Next Action: [ 0.701\n",
      "Step reward: -15.833929364789086, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -740.9598125868902\n",
      "Episode: 5, Step: 47\n",
      "Next Action: [ 1.144\n",
      "Step reward: -15.833126090424871, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -756.7929386773151\n",
      "Episode: 5, Step: 48\n",
      "Next Action: [ 1.141\n",
      "Step reward: -15.824922559135535, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -772.6178612364506\n",
      "Episode: 5, Step: 49\n",
      "Next Action: [ 9.223\n",
      "Step reward: -15.841760047049137, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -788.4596212834997\n",
      "Episode: 5, Step: 50\n",
      "Next Action: [ 0.894\n",
      "Step reward: -15.874959248276731, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -804.3345805317764\n",
      "Episode: 5, Step: 51\n",
      "Next Action: [ 1.154\n",
      "Step reward: -15.887931035156539, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -820.2225115669329\n",
      "Episode: 5, Step: 52\n",
      "Next Action: [ 1.549\n",
      "Step reward: -15.876309258803506, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -836.0988208257364\n",
      "Episode: 5, Step: 53\n",
      "Next Action: [ 1.323\n",
      "Step reward: -15.862317231455494, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -851.9611380571919\n",
      "Episode: 5, Step: 54\n",
      "Next Action: [ 1.461\n",
      "Step reward: -15.859454051089312, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -867.8205921082812\n",
      "Episode: 5, Step: 55\n",
      "Next Action: [ 1.499\n",
      "Step reward: -15.859911647355581, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -883.6805037556368\n",
      "Episode: 5, Step: 56\n",
      "Next Action: [ 1.640\n",
      "Step reward: -15.871085909308041, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -899.5515896649448\n",
      "Episode: 5, Step: 57\n",
      "Next Action: [ 1.867\n",
      "Step reward: -15.8692630970175, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -915.4208527619624\n",
      "Episode: 5, Step: 58\n",
      "Next Action: [ 1.724\n",
      "Step reward: -15.873878181445855, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -931.2947309434082\n",
      "Episode: 5, Step: 59\n",
      "Next Action: [ 1.578\n",
      "Step reward: -15.859850302827471, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -947.1545812462357\n",
      "Episode: 5, Step: 60\n",
      "Next Action: [ 1.686\n",
      "Step reward: -15.847001711778521, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -963.0015829580142\n",
      "Episode: 5, Step: 61\n",
      "Next Action: [ 1.763\n",
      "Step reward: -15.844345993025103, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -978.8459289510392\n",
      "Episode: 5, Step: 62\n",
      "Next Action: [ 1.550\n",
      "Step reward: -15.830707853113624, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -994.6766368041528\n",
      "Episode: 5, Step: 63\n",
      "Next Action: [ 1.415\n",
      "Step reward: -15.871664302576429, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1010.5483011067292\n",
      "Episode: 5, Step: 64\n",
      "Next Action: [ 1.616\n",
      "Step reward: -15.938177905706356, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1026.4864790124357\n",
      "Episode: 5, Step: 65\n",
      "Next Action: [ 1.405\n",
      "Step reward: -15.916747147134952, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1042.4032261595705\n",
      "Episode: 5, Step: 66\n",
      "Next Action: [ 1.724\n",
      "Step reward: -15.910445102277928, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1058.3136712618484\n",
      "Episode: 5, Step: 67\n",
      "Next Action: [ 1.716\n",
      "Step reward: -15.909781766334346, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1074.2234530281828\n",
      "Episode: 5, Step: 68\n",
      "Next Action: [ 1.428\n",
      "Step reward: -15.90439148632303, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1090.1278445145058\n",
      "Episode: 5, Step: 69\n",
      "Next Action: [ 1.117\n",
      "Step reward: -15.913139245313744, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1106.0409837598195\n",
      "Episode: 5, Step: 70\n",
      "Next Action: [ 1.121\n",
      "Step reward: -15.920294945919096, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1121.9612787057386\n",
      "Episode: 5, Step: 71\n",
      "Next Action: [ 1.128\n",
      "Step reward: -15.896506532206706, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1137.8577852379453\n",
      "Episode: 5, Step: 72\n",
      "Next Action: [ 1.192\n",
      "Step reward: -15.894369726934684, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1153.75215496488\n",
      "Episode: 5, Step: 73\n",
      "Next Action: [ 0.950\n",
      "Step reward: -15.874297901072206, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1169.6264528659522\n",
      "Episode: 5, Step: 74\n",
      "Next Action: [ 1.022\n",
      "Step reward: -15.880490856267494, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1185.5069437222196\n",
      "Episode: 5, Step: 75\n",
      "Next Action: [ 1.107\n",
      "Step reward: -15.870204192196853, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1201.3771479144164\n",
      "Episode: 5, Step: 76\n",
      "Next Action: [ 1.131\n",
      "Step reward: -15.844173508448156, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1217.2213214228645\n",
      "Episode: 5, Step: 77\n",
      "Next Action: [ 1.344\n",
      "Step reward: -15.841128855367478, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1233.062450278232\n",
      "Episode: 5, Step: 78\n",
      "Next Action: [ 1.478\n",
      "Step reward: -15.866335786621386, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1248.9287860648535\n",
      "Episode: 5, Step: 79\n",
      "Next Action: [ 1.312\n",
      "Step reward: -15.887611133821075, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1264.8163971986746\n",
      "Episode: 5, Step: 80\n",
      "Next Action: [ 1.082\n",
      "Step reward: -15.880450890947804, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1280.6968480896223\n",
      "Episode: 5, Step: 81\n",
      "Next Action: [ 0.926\n",
      "Step reward: -15.87148506358395, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1296.5683331532064\n",
      "Episode: 5, Step: 82\n",
      "Next Action: [ 0.681\n",
      "Step reward: -15.878148820139362, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1312.4464819733457\n",
      "Episode: 5, Step: 83\n",
      "Next Action: [ 0.874\n",
      "Step reward: -15.8500155470023, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1328.296497520348\n",
      "Episode: 5, Step: 84\n",
      "Next Action: [ 0.817\n",
      "Step reward: -15.861067675164128, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1344.1575651955122\n",
      "Episode: 5, Step: 85\n",
      "Next Action: [ 0.725\n",
      "Step reward: -15.923328149235388, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1360.0808933447477\n",
      "Episode: 5, Step: 86\n",
      "Next Action: [ 9.156\n",
      "Step reward: -15.90557896119385, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1375.9864723059416\n",
      "Episode: 5, Step: 87\n",
      "Next Action: [ 0.960\n",
      "Step reward: -15.878299313838923, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1391.8647716197804\n",
      "Episode: 5, Step: 88\n",
      "Next Action: [ 0.920\n",
      "Step reward: -15.839555470316311, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1407.7043270900967\n",
      "Episode: 5, Step: 89\n",
      "Next Action: [ 1.251\n",
      "Step reward: -15.816782974467479, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1423.5211100645643\n",
      "Episode: 5, Step: 90\n",
      "Next Action: [ 1.034\n",
      "Step reward: -15.859726854108093, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1439.3808369186725\n",
      "Episode: 5, Step: 91\n",
      "Next Action: [ 1.166\n",
      "Step reward: -15.87468962811498, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1455.2555265467875\n",
      "Episode: 5, Step: 92\n",
      "Next Action: [ 1.212\n",
      "Step reward: -15.873287884821165, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1471.1288144316086\n",
      "Episode: 5, Step: 93\n",
      "Next Action: [ 1.065\n",
      "Step reward: -15.864739869113835, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1486.9935543007225\n",
      "Episode: 5, Step: 94\n",
      "Next Action: [ 1.200\n",
      "Step reward: -15.83551676573573, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1502.8290710664583\n",
      "Episode: 5, Step: 95\n",
      "Next Action: [ 1.099\n",
      "Step reward: -15.828119867351152, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1518.6571909338095\n",
      "Episode: 5, Step: 96\n",
      "Next Action: [ 1.288\n",
      "Step reward: -15.836381849880574, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1534.49357278369\n",
      "Episode: 5, Step: 97\n",
      "Next Action: [ 1.578\n",
      "Step reward: -15.79525404979837, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1550.2888268334884\n",
      "Episode: 5, Step: 98\n",
      "Next Action: [ 1.372\n",
      "Step reward: -15.776232710423184, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1566.0650595439115\n",
      "Episode: 5, Step: 99\n",
      "Next Action: [ 1.240\n",
      "Step reward: -15.778787674804478, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1581.843847218716\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 94.67642974853516\n",
      "Actor loss: 20.54399299621582\n",
      "Critic loss: 27.524240493774414\n",
      "Actor loss: 64.81758880615234\n",
      "Critic loss: 18.923538208007812\n",
      "Actor loss: 62.398536682128906\n",
      "Critic loss: 14.32575511932373\n",
      "Actor loss: 58.795921325683594\n",
      "Critic loss: 10.091535568237305\n",
      "Actor loss: 38.73766326904297\n",
      "Critic loss: 11.589731216430664\n",
      "Actor loss: 37.44031524658203\n",
      "Critic loss: 16.455074310302734\n",
      "Actor loss: 41.39409637451172\n",
      "Critic loss: 3.171696901321411\n",
      "Actor loss: 38.2538948059082\n",
      "Critic loss: 25.14196014404297\n",
      "Actor loss: 35.67561721801758\n",
      "Critic loss: 5.403747081756592\n",
      "Actor loss: 39.0831298828125\n",
      "Episode: 6\n",
      "Episode: 6, Step: 0\n",
      "Next Action: [ 1.091\n",
      "Step reward: -11.327680390974038, Next State: [ 9.\n",
      "Total episode reward: -11.327680390974038\n",
      "Episode: 6, Step: 1\n",
      "Next Action: [ 1.380\n",
      "Step reward: -14.494423422656755, Next State: [ 1.\n",
      "Total episode reward: -25.822103813630793\n",
      "Episode: 6, Step: 2\n",
      "Next Action: [ 1.261\n",
      "Step reward: -15.510707994968957, Next State: [ 1.\n",
      "Total episode reward: -41.33281180859975\n",
      "Episode: 6, Step: 3\n",
      "Next Action: [ 1.184\n",
      "Step reward: -15.764295846225675, Next State: [ 1.\n",
      "Total episode reward: -57.09710765482542\n",
      "Episode: 6, Step: 4\n",
      "Next Action: [ 1.111\n",
      "Step reward: -15.833485175334324, Next State: [ 1.\n",
      "Total episode reward: -72.93059283015975\n",
      "Episode: 6, Step: 5\n",
      "Next Action: [ 1.164\n",
      "Step reward: -15.877983257000981, Next State: [ 1.\n",
      "Total episode reward: -88.80857608716073\n",
      "Episode: 6, Step: 6\n",
      "Next Action: [ 1.037\n",
      "Step reward: -15.885557731762285, Next State: [ 1.\n",
      "Total episode reward: -104.69413381892302\n",
      "Episode: 6, Step: 7\n",
      "Next Action: [ 1.025\n",
      "Step reward: -15.883790260522872, Next State: [ 1.\n",
      "Total episode reward: -120.57792407944589\n",
      "Episode: 6, Step: 8\n",
      "Next Action: [ 0.844\n",
      "Step reward: -15.920984296018894, Next State: [ 1.\n",
      "Total episode reward: -136.4989083754648\n",
      "Episode: 6, Step: 9\n",
      "Next Action: [ 8.916\n",
      "Step reward: -15.91015091081458, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.4090592862794\n",
      "Episode: 6, Step: 10\n",
      "Next Action: [ 0.845\n",
      "Step reward: -15.896802521672214, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.3058618079516\n",
      "Episode: 6, Step: 11\n",
      "Next Action: [ 1.178\n",
      "Step reward: -15.888978906935366, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.19484071488696\n",
      "Episode: 6, Step: 12\n",
      "Next Action: [ 1.331\n",
      "Step reward: -15.902446273007827, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.0972869878948\n",
      "Episode: 6, Step: 13\n",
      "Next Action: [ 1.139\n",
      "Step reward: -15.90633339642348, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.00362038431828\n",
      "Episode: 6, Step: 14\n",
      "Next Action: [ 1.282\n",
      "Step reward: -15.936716070859974, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.94033645517825\n",
      "Episode: 6, Step: 15\n",
      "Next Action: [ 1.298\n",
      "Step reward: -15.947767229306017, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.88810368448426\n",
      "Episode: 6, Step: 16\n",
      "Next Action: [ 1.214\n",
      "Step reward: -15.926288500318794, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.81439218480307\n",
      "Episode: 6, Step: 17\n",
      "Next Action: [ 0.918\n",
      "Step reward: -15.920437658299488, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.73482984310255\n",
      "Episode: 6, Step: 18\n",
      "Next Action: [ 0.883\n",
      "Step reward: -15.932069458611705, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -295.66689930171424\n",
      "Episode: 6, Step: 19\n",
      "Next Action: [ 1.054\n",
      "Step reward: -15.916184200448717, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -311.58308350216294\n",
      "Episode: 6, Step: 20\n",
      "Next Action: [ 1.113\n",
      "Step reward: -15.93650342437037, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -327.5195869265333\n",
      "Episode: 6, Step: 21\n",
      "Next Action: [ 1.258\n",
      "Step reward: -15.921052910997219, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -343.4406398375305\n",
      "Episode: 6, Step: 22\n",
      "Next Action: [ 1.039\n",
      "Step reward: -15.902485113890243, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -359.34312495142075\n",
      "Episode: 6, Step: 23\n",
      "Next Action: [ 0.765\n",
      "Step reward: -15.86356134486578, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.20668629628653\n",
      "Episode: 6, Step: 24\n",
      "Next Action: [ 1.087\n",
      "Step reward: -15.8738988983764, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.08058519466294\n",
      "Episode: 6, Step: 25\n",
      "Next Action: [ 0.970\n",
      "Step reward: -15.85844487108167, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -406.9390300657446\n",
      "Episode: 6, Step: 26\n",
      "Next Action: [ 0.534\n",
      "Step reward: -15.879909745528515, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -422.8189398112731\n",
      "Episode: 6, Step: 27\n",
      "Next Action: [ 6.081\n",
      "Step reward: -15.877779084941254, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -438.69671889621435\n",
      "Episode: 6, Step: 28\n",
      "Next Action: [ 0.339\n",
      "Step reward: -15.898890550093833, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -454.5956094463082\n",
      "Episode: 6, Step: 29\n",
      "Next Action: [ 0.462\n",
      "Step reward: -15.903907145427862, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -470.49951659173604\n",
      "Episode: 6, Step: 30\n",
      "Next Action: [ 0.353\n",
      "Step reward: -15.909964512986432, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -486.4094811047225\n",
      "Episode: 6, Step: 31\n",
      "Next Action: [ 0.688\n",
      "Step reward: -15.900663938856395, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -502.31014504357887\n",
      "Episode: 6, Step: 32\n",
      "Next Action: [ 0.817\n",
      "Step reward: -15.907635328256058, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -518.2177803718349\n",
      "Episode: 6, Step: 33\n",
      "Next Action: [ 0.456\n",
      "Step reward: -15.931494421780075, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -534.149274793615\n",
      "Episode: 6, Step: 34\n",
      "Next Action: [ 2.060\n",
      "Step reward: -15.927209303770464, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -550.0764840973854\n",
      "Episode: 6, Step: 35\n",
      "Next Action: [ 0.608\n",
      "Step reward: -15.908436638377987, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -565.9849207357635\n",
      "Episode: 6, Step: 36\n",
      "Next Action: [ 0.275\n",
      "Step reward: -15.891590431917956, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -581.8765111676814\n",
      "Episode: 6, Step: 37\n",
      "Next Action: [ 0.133\n",
      "Step reward: -15.890342970591986, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -597.7668541382734\n",
      "Episode: 6, Step: 38\n",
      "Next Action: [ 0.360\n",
      "Step reward: -15.889282733147727, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -613.6561368714212\n",
      "Episode: 6, Step: 39\n",
      "Next Action: [ 0.384\n",
      "Step reward: -15.871340071686118, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -629.5274769431072\n",
      "Episode: 6, Step: 40\n",
      "Next Action: [ 0.486\n",
      "Step reward: -15.870659245523948, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -645.3981361886312\n",
      "Episode: 6, Step: 41\n",
      "Next Action: [ 0.785\n",
      "Step reward: -15.88701741476064, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -661.2851536033918\n",
      "Episode: 6, Step: 42\n",
      "Next Action: [ 0.916\n",
      "Step reward: -15.93265622720855, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -677.2178098306003\n",
      "Episode: 6, Step: 43\n",
      "Next Action: [ 1.269\n",
      "Step reward: -15.938830260452441, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -693.1566400910527\n",
      "Episode: 6, Step: 44\n",
      "Next Action: [ 0.945\n",
      "Step reward: -15.923481915813966, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -709.0801220068668\n",
      "Episode: 6, Step: 45\n",
      "Next Action: [ 1.128\n",
      "Step reward: -15.91572325610212, Next State: [ 1.0\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -724.9958452629688\n",
      "Episode: 6, Step: 46\n",
      "Next Action: [ 1.120\n",
      "Step reward: -15.907536632428464, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -740.9033818953973\n",
      "Episode: 6, Step: 47\n",
      "Next Action: [ 1.036\n",
      "Step reward: -15.915809067897879, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -756.8191909632951\n",
      "Episode: 6, Step: 48\n",
      "Next Action: [ 0.707\n",
      "Step reward: -15.930745691728227, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -772.7499366550234\n",
      "Episode: 6, Step: 49\n",
      "Next Action: [ 0.726\n",
      "Step reward: -15.93660829365442, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -788.6865449486778\n",
      "Episode: 6, Step: 50\n",
      "Next Action: [ 0.798\n",
      "Step reward: -15.923117939275386, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -804.6096628879532\n",
      "Episode: 6, Step: 51\n",
      "Next Action: [ 0.669\n",
      "Step reward: -15.91837115113619, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -820.5280340390893\n",
      "Episode: 6, Step: 52\n",
      "Next Action: [ 0.500\n",
      "Step reward: -15.875746456260483, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -836.4037804953498\n",
      "Episode: 6, Step: 53\n",
      "Next Action: [ 0.433\n",
      "Step reward: -15.876655334651874, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -852.2804358300017\n",
      "Episode: 6, Step: 54\n",
      "Next Action: [ 0.655\n",
      "Step reward: -15.882870470590968, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -868.1633063005927\n",
      "Episode: 6, Step: 55\n",
      "Next Action: [ 0.747\n",
      "Step reward: -15.901382828796285, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -884.064689129389\n",
      "Episode: 6, Step: 56\n",
      "Next Action: [ 0.807\n",
      "Step reward: -15.852495176157989, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -899.9171843055469\n",
      "Episode: 6, Step: 57\n",
      "Next Action: [ 1.149\n",
      "Step reward: -15.812204270556464, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -915.7293885761034\n",
      "Episode: 6, Step: 58\n",
      "Next Action: [ 1.021\n",
      "Step reward: -15.802321226174461, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -931.5317098022779\n",
      "Episode: 6, Step: 59\n",
      "Next Action: [ 0.690\n",
      "Step reward: -15.842821149847897, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -947.3745309521257\n",
      "Episode: 6, Step: 60\n",
      "Next Action: [ 0.724\n",
      "Step reward: -15.861556385416915, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -963.2360873375426\n",
      "Episode: 6, Step: 61\n",
      "Next Action: [ 0.973\n",
      "Step reward: -15.864713748247498, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -979.1008010857901\n",
      "Episode: 6, Step: 62\n",
      "Next Action: [ 0.904\n",
      "Step reward: -15.92273990205256, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -995.0235409878426\n",
      "Episode: 6, Step: 63\n",
      "Next Action: [ 0.991\n",
      "Step reward: -15.92381806283047, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1010.9473590506731\n",
      "Episode: 6, Step: 64\n",
      "Next Action: [ 1.250\n",
      "Step reward: -15.918739761595058, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1026.866098812268\n",
      "Episode: 6, Step: 65\n",
      "Next Action: [ 1.175\n",
      "Step reward: -15.918275589330303, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1042.7843744015984\n",
      "Episode: 6, Step: 66\n",
      "Next Action: [ 1.270\n",
      "Step reward: -15.887400094700345, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1058.6717744962987\n",
      "Episode: 6, Step: 67\n",
      "Next Action: [ 1.157\n",
      "Step reward: -15.899818384706784, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1074.5715928810055\n",
      "Episode: 6, Step: 68\n",
      "Next Action: [ 0.916\n",
      "Step reward: -15.903782876044682, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1090.4753757570502\n",
      "Episode: 6, Step: 69\n",
      "Next Action: [ 0.782\n",
      "Step reward: -15.935856536164355, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1106.4112322932147\n",
      "Episode: 6, Step: 70\n",
      "Next Action: [ 1.134\n",
      "Step reward: -15.931820349692968, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1122.3430526429077\n",
      "Episode: 6, Step: 71\n",
      "Next Action: [ 1.045\n",
      "Step reward: -15.927184199858516, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1138.2702368427663\n",
      "Episode: 6, Step: 72\n",
      "Next Action: [ 1.450\n",
      "Step reward: -15.917211215714087, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1154.1874480584804\n",
      "Episode: 6, Step: 73\n",
      "Next Action: [ 1.194\n",
      "Step reward: -15.91774793328937, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1170.1051959917697\n",
      "Episode: 6, Step: 74\n",
      "Next Action: [ 1.059\n",
      "Step reward: -15.953901515334824, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1186.0590975071045\n",
      "Episode: 6, Step: 75\n",
      "Next Action: [ 1.214\n",
      "Step reward: -15.94818035689762, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1202.0072778640022\n",
      "Episode: 6, Step: 76\n",
      "Next Action: [ 1.337\n",
      "Step reward: -15.912571868938931, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1217.9198497329412\n",
      "Episode: 6, Step: 77\n",
      "Next Action: [ 1.507\n",
      "Step reward: -15.896088533817286, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1233.8159382667584\n",
      "Episode: 6, Step: 78\n",
      "Next Action: [ 1.433\n",
      "Step reward: -15.903679210038106, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1249.7196174767964\n",
      "Episode: 6, Step: 79\n",
      "Next Action: [ 1.629\n",
      "Step reward: -15.924763722606883, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1265.6443811994034\n",
      "Episode: 6, Step: 80\n",
      "Next Action: [ 1.473\n",
      "Step reward: -15.936195402450009, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1281.5805766018534\n",
      "Episode: 6, Step: 81\n",
      "Next Action: [ 1.230\n",
      "Step reward: -15.949735257627037, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1297.5303118594804\n",
      "Episode: 6, Step: 82\n",
      "Next Action: [ 1.089\n",
      "Step reward: -15.943443525477392, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1313.4737553849577\n",
      "Episode: 6, Step: 83\n",
      "Next Action: [ 1.130\n",
      "Step reward: -15.949902239689973, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1329.4236576246476\n",
      "Episode: 6, Step: 84\n",
      "Next Action: [ 1.265\n",
      "Step reward: -15.965370078008043, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1345.3890277026555\n",
      "Episode: 6, Step: 85\n",
      "Next Action: [ 1.312\n",
      "Step reward: -15.940164178054705, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1361.3291918807101\n",
      "Episode: 6, Step: 86\n",
      "Next Action: [ 1.090\n",
      "Step reward: -15.934757319875951, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1377.263949200586\n",
      "Episode: 6, Step: 87\n",
      "Next Action: [ 1.148\n",
      "Step reward: -15.916752917162398, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1393.1807021177483\n",
      "Episode: 6, Step: 88\n",
      "Next Action: [ 0.720\n",
      "Step reward: -15.885630775933002, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1409.0663328936812\n",
      "Episode: 6, Step: 89\n",
      "Next Action: [ 0.751\n",
      "Step reward: -15.847613146105175, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1424.9139460397864\n",
      "Episode: 6, Step: 90\n",
      "Next Action: [ 0.547\n",
      "Step reward: -15.799794720852793, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1440.7137407606392\n",
      "Episode: 6, Step: 91\n",
      "Next Action: [ 0.692\n",
      "Step reward: -15.835466255867033, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1456.5492070165062\n",
      "Episode: 6, Step: 92\n",
      "Next Action: [ 0.880\n",
      "Step reward: -15.844502130805271, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1472.3937091473115\n",
      "Episode: 6, Step: 93\n",
      "Next Action: [ 1.168\n",
      "Step reward: -15.833636288280545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1488.227345435592\n",
      "Episode: 6, Step: 94\n",
      "Next Action: [ 1.286\n",
      "Step reward: -15.804195247117052, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1504.0315406827092\n",
      "Episode: 6, Step: 95\n",
      "Next Action: [ 1.425\n",
      "Step reward: -15.849982316552554, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1519.8815229992617\n",
      "Episode: 6, Step: 96\n",
      "Next Action: [ 1.342\n",
      "Step reward: -15.888925479146662, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1535.7704484784083\n",
      "Episode: 6, Step: 97\n",
      "Next Action: [ 1.190\n",
      "Step reward: -15.891344061541561, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1551.6617925399498\n",
      "Episode: 6, Step: 98\n",
      "Next Action: [ 0.955\n",
      "Step reward: -15.906665515375959, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1567.568458055326\n",
      "Episode: 6, Step: 99\n",
      "Next Action: [ 1.068\n",
      "Step reward: -15.934498444221575, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1583.5029564995475\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 14.096664428710938\n",
      "Actor loss: 55.17329025268555\n",
      "Critic loss: 5.760342597961426\n",
      "Actor loss: 46.20309066772461\n",
      "Critic loss: 3.3320651054382324\n",
      "Actor loss: 39.25970458984375\n",
      "Critic loss: 41.378196716308594\n",
      "Actor loss: 41.10630416870117\n",
      "Critic loss: 8.154449462890625\n",
      "Actor loss: 44.461936950683594\n",
      "Critic loss: 5.917619705200195\n",
      "Actor loss: 42.69898223876953\n",
      "Critic loss: 226.45521545410156\n",
      "Actor loss: 40.96491241455078\n",
      "Critic loss: 19.20751190185547\n",
      "Actor loss: 36.72356414794922\n",
      "Critic loss: 63.65633773803711\n",
      "Actor loss: 43.666831970214844\n",
      "Critic loss: 6.949351787567139\n",
      "Actor loss: 38.133052825927734\n",
      "Episode: 7\n",
      "Episode: 7, Step: 0\n",
      "Next Action: [ 0.539\n",
      "Step reward: -11.335195316810692, Next State: [ 0.\n",
      "Total episode reward: -11.335195316810692\n",
      "Episode: 7, Step: 1\n",
      "Next Action: [ 6.640\n",
      "Step reward: -14.108974694661796, Next State: [ 8.\n",
      "Total episode reward: -25.444170011472487\n",
      "Episode: 7, Step: 2\n",
      "Next Action: [ 0.940\n",
      "Step reward: -14.951146941304929, Next State: [ 1.\n",
      "Total episode reward: -40.395316952777414\n",
      "Episode: 7, Step: 3\n",
      "Next Action: [ 0.939\n",
      "Step reward: -15.237900839467938, Next State: [ 1.\n",
      "Total episode reward: -55.633217792245354\n",
      "Episode: 7, Step: 4\n",
      "Next Action: [ 0.693\n",
      "Step reward: -15.514751813898833, Next State: [ 1.\n",
      "Total episode reward: -71.14796960614419\n",
      "Episode: 7, Step: 5\n",
      "Next Action: [ 1.051\n",
      "Step reward: -15.678767897036622, Next State: [ 1.\n",
      "Total episode reward: -86.82673750318081\n",
      "Episode: 7, Step: 6\n",
      "Next Action: [ 1.027\n",
      "Step reward: -15.730073133855903, Next State: [ 1.\n",
      "Total episode reward: -102.5568106370367\n",
      "Episode: 7, Step: 7\n",
      "Next Action: [ 0.921\n",
      "Step reward: -15.806347363472145, Next State: [ 1.\n",
      "Total episode reward: -118.36315800050885\n",
      "Episode: 7, Step: 8\n",
      "Next Action: [ 1.078\n",
      "Step reward: -15.868081696038024, Next State: [ 1.\n",
      "Total episode reward: -134.23123969654688\n",
      "Episode: 7, Step: 9\n",
      "Next Action: [ 0.799\n",
      "Step reward: -15.816093938650523, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -150.0473336351974\n",
      "Episode: 7, Step: 10\n",
      "Next Action: [ 1.362\n",
      "Step reward: -15.817614312763896, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -165.8649479479613\n",
      "Episode: 7, Step: 11\n",
      "Next Action: [ 1.413\n",
      "Step reward: -15.864532527598916, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -181.72948047556022\n",
      "Episode: 7, Step: 12\n",
      "Next Action: [ 1.604\n",
      "Step reward: -15.841816018591148, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -197.57129649415137\n",
      "Episode: 7, Step: 13\n",
      "Next Action: [ 1.469\n",
      "Step reward: -15.834240566024224, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -213.4055370601756\n",
      "Episode: 7, Step: 14\n",
      "Next Action: [ 1.355\n",
      "Step reward: -15.856734310717705, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -229.2622713708933\n",
      "Episode: 7, Step: 15\n",
      "Next Action: [ 1.511\n",
      "Step reward: -15.790154555697315, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -245.05242592659062\n",
      "Episode: 7, Step: 16\n",
      "Next Action: [ 1.527\n",
      "Step reward: -15.710566370277807, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -260.76299229686845\n",
      "Episode: 7, Step: 17\n",
      "Next Action: [ 1.434\n",
      "Step reward: -15.677749743182117, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -276.44074204005057\n",
      "Episode: 7, Step: 18\n",
      "Next Action: [ 1.277\n",
      "Step reward: -15.690811949028026, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -292.1315539890786\n",
      "Episode: 7, Step: 19\n",
      "Next Action: [ 1.336\n",
      "Step reward: -15.693447059820462, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -307.8250010488991\n",
      "Episode: 7, Step: 20\n",
      "Next Action: [ 1.330\n",
      "Step reward: -15.732102274103642, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -323.5571033230027\n",
      "Episode: 7, Step: 21\n",
      "Next Action: [ 1.418\n",
      "Step reward: -15.779826339025703, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -339.3369296620284\n",
      "Episode: 7, Step: 22\n",
      "Next Action: [ 1.063\n",
      "Step reward: -15.785848148898804, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -355.12277781092723\n",
      "Episode: 7, Step: 23\n",
      "Next Action: [ 8.633\n",
      "Step reward: -15.797663647269117, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -370.92044145819636\n",
      "Episode: 7, Step: 24\n",
      "Next Action: [ 1.049\n",
      "Step reward: -15.76071946327661, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -386.68116092147295\n",
      "Episode: 7, Step: 25\n",
      "Next Action: [ 1.449\n",
      "Step reward: -15.792798884267913, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -402.4739598057409\n",
      "Episode: 7, Step: 26\n",
      "Next Action: [ 1.086\n",
      "Step reward: -15.773454519264115, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -418.247414325005\n",
      "Episode: 7, Step: 27\n",
      "Next Action: [ 1.067\n",
      "Step reward: -15.739868037895876, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -433.9872823629009\n",
      "Episode: 7, Step: 28\n",
      "Next Action: [ 1.138\n",
      "Step reward: -15.745210738453341, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -449.73249310135424\n",
      "Episode: 7, Step: 29\n",
      "Next Action: [ 1.273\n",
      "Step reward: -15.784204695105336, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -465.5166977964596\n",
      "Episode: 7, Step: 30\n",
      "Next Action: [ 1.564\n",
      "Step reward: -15.770385704830685, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -481.2870835012903\n",
      "Episode: 7, Step: 31\n",
      "Next Action: [ 1.728\n",
      "Step reward: -15.700762284490791, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -496.9878457857811\n",
      "Episode: 7, Step: 32\n",
      "Next Action: [ 1.593\n",
      "Step reward: -15.678250628812362, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -512.6660964145934\n",
      "Episode: 7, Step: 33\n",
      "Next Action: [ 1.265\n",
      "Step reward: -15.709742070462482, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -528.375838485056\n",
      "Episode: 7, Step: 34\n",
      "Next Action: [ 1.482\n",
      "Step reward: -15.75280151720459, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -544.1286400022606\n",
      "Episode: 7, Step: 35\n",
      "Next Action: [ 1.479\n",
      "Step reward: -15.775639331494492, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -559.9042793337551\n",
      "Episode: 7, Step: 36\n",
      "Next Action: [ 1.090\n",
      "Step reward: -15.783575157174383, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -575.6878544909295\n",
      "Episode: 7, Step: 37\n",
      "Next Action: [ 1.006\n",
      "Step reward: -15.793297128793728, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -591.4811516197232\n",
      "Episode: 7, Step: 38\n",
      "Next Action: [ 0.699\n",
      "Step reward: -15.780126408269524, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -607.2612780279927\n",
      "Episode: 7, Step: 39\n",
      "Next Action: [ 0.619\n",
      "Step reward: -15.790709831057468, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -623.0519878590502\n",
      "Episode: 7, Step: 40\n",
      "Next Action: [ 0.586\n",
      "Step reward: -15.803231185349107, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -638.8552190443993\n",
      "Episode: 7, Step: 41\n",
      "Next Action: [ 0.655\n",
      "Step reward: -15.782983015560674, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -654.63820205996\n",
      "Episode: 7, Step: 42\n",
      "Next Action: [ 0.883\n",
      "Step reward: -15.794543578804522, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -670.4327456387646\n",
      "Episode: 7, Step: 43\n",
      "Next Action: [ 1.146\n",
      "Step reward: -15.820470342789603, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -686.2532159815541\n",
      "Episode: 7, Step: 44\n",
      "Next Action: [ 0.803\n",
      "Step reward: -15.827913612021506, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -702.0811295935756\n",
      "Episode: 7, Step: 45\n",
      "Next Action: [ 1.041\n",
      "Step reward: -15.781665249193239, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -717.8627948427688\n",
      "Episode: 7, Step: 46\n",
      "Next Action: [ 0.597\n",
      "Step reward: -15.768520338004668, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -733.6313151807735\n",
      "Episode: 7, Step: 47\n",
      "Next Action: [ 0.694\n",
      "Step reward: -15.762103917345932, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -749.3934190981195\n",
      "Episode: 7, Step: 48\n",
      "Next Action: [ 1.024\n",
      "Step reward: -15.765225629447915, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -765.1586447275674\n",
      "Episode: 7, Step: 49\n",
      "Next Action: [ 1.061\n",
      "Step reward: -15.768430507639914, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -780.9270752352073\n",
      "Episode: 7, Step: 50\n",
      "Next Action: [ 0.962\n",
      "Step reward: -15.796987315978173, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -796.7240625511855\n",
      "Episode: 7, Step: 51\n",
      "Next Action: [ 1.147\n",
      "Step reward: -15.759362517826647, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -812.4834250690121\n",
      "Episode: 7, Step: 52\n",
      "Next Action: [ 1.260\n",
      "Step reward: -15.714587624818524, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -828.1980126938306\n",
      "Episode: 7, Step: 53\n",
      "Next Action: [ 0.979\n",
      "Step reward: -15.705782526990385, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -843.9037952208209\n",
      "Episode: 7, Step: 54\n",
      "Next Action: [ 1.027\n",
      "Step reward: -15.741630775986437, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -859.6454259968074\n",
      "Episode: 7, Step: 55\n",
      "Next Action: [ 1.072\n",
      "Step reward: -15.796627724235478, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -875.4420537210428\n",
      "Episode: 7, Step: 56\n",
      "Next Action: [ 0.550\n",
      "Step reward: -15.751576861248664, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -891.1936305822915\n",
      "Episode: 7, Step: 57\n",
      "Next Action: [ 0.414\n",
      "Step reward: -15.754174484837016, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -906.9478050671285\n",
      "Episode: 7, Step: 58\n",
      "Next Action: [ 0.392\n",
      "Step reward: -15.682158597457603, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -922.6299636645861\n",
      "Episode: 7, Step: 59\n",
      "Next Action: [ 0.102\n",
      "Step reward: -15.612014382752445, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -938.2419780473385\n",
      "Episode: 7, Step: 60\n",
      "Next Action: [ 0.318\n",
      "Step reward: -15.601060149604647, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.8430381969432\n",
      "Episode: 7, Step: 61\n",
      "Next Action: [ 0.231\n",
      "Step reward: -15.659827118794032, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.5028653157373\n",
      "Episode: 7, Step: 62\n",
      "Next Action: [ 0.231\n",
      "Step reward: -15.667357458943233, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.1702227746805\n",
      "Episode: 7, Step: 63\n",
      "Next Action: [ 0.614\n",
      "Step reward: -15.670471162047239, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.8406939367277\n",
      "Episode: 7, Step: 64\n",
      "Next Action: [ 0.579\n",
      "Step reward: -15.662812093802156, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.5035060305298\n",
      "Episode: 7, Step: 65\n",
      "Next Action: [ 0.443\n",
      "Step reward: -15.7052052362909, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.2087112668207\n",
      "Episode: 7, Step: 66\n",
      "Next Action: [ 0.753\n",
      "Step reward: -15.773537675378654, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.9822489421995\n",
      "Episode: 7, Step: 67\n",
      "Next Action: [ 0.915\n",
      "Step reward: -15.811289748031825, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.7935386902313\n",
      "Episode: 7, Step: 68\n",
      "Next Action: [ 0.896\n",
      "Step reward: -15.802091440568052, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.5956301307995\n",
      "Episode: 7, Step: 69\n",
      "Next Action: [ 0.741\n",
      "Step reward: -15.791183655065637, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.3868137858651\n",
      "Episode: 7, Step: 70\n",
      "Next Action: [ 0.957\n",
      "Step reward: -15.750047912284417, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.1368616981495\n",
      "Episode: 7, Step: 71\n",
      "Next Action: [ 6.651\n",
      "Step reward: -15.728018665526054, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.8648803636754\n",
      "Episode: 7, Step: 72\n",
      "Next Action: [ 0.193\n",
      "Step reward: -15.72327996485973, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.5881603285352\n",
      "Episode: 7, Step: 73\n",
      "Next Action: [ 0.382\n",
      "Step reward: -15.730321476734884, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.31848180527\n",
      "Episode: 7, Step: 74\n",
      "Next Action: [ 0.352\n",
      "Step reward: -15.727273863263926, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.045755668534\n",
      "Episode: 7, Step: 75\n",
      "Next Action: [ 6.718\n",
      "Step reward: -15.677911030496542, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1189.7236666990304\n",
      "Episode: 7, Step: 76\n",
      "Next Action: [ 0.688\n",
      "Step reward: -15.702080793853185, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1205.4257474928836\n",
      "Episode: 7, Step: 77\n",
      "Next Action: [ 0.777\n",
      "Step reward: -15.773094904866507, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1221.19884239775\n",
      "Episode: 7, Step: 78\n",
      "Next Action: [ 1.023\n",
      "Step reward: -15.802041879233267, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1237.0008842769832\n",
      "Episode: 7, Step: 79\n",
      "Next Action: [ 0.805\n",
      "Step reward: -15.794145180908608, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1252.7950294578918\n",
      "Episode: 7, Step: 80\n",
      "Next Action: [ 0.872\n",
      "Step reward: -15.767918097328252, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1268.56294755522\n",
      "Episode: 7, Step: 81\n",
      "Next Action: [ 0.608\n",
      "Step reward: -15.714230774490414, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1284.2771783297105\n",
      "Episode: 7, Step: 82\n",
      "Next Action: [ 0.165\n",
      "Step reward: -15.730067101150706, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1300.0072454308613\n",
      "Episode: 7, Step: 83\n",
      "Next Action: [ 0.078\n",
      "Step reward: -15.719280905421789, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1315.726526336283\n",
      "Episode: 7, Step: 84\n",
      "Next Action: [-0.086\n",
      "Step reward: -15.681234388733218, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1331.4077607250163\n",
      "Episode: 7, Step: 85\n",
      "Next Action: [-0.153\n",
      "Step reward: -15.653010098432478, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1347.0607708234488\n",
      "Episode: 7, Step: 86\n",
      "Next Action: [-0.003\n",
      "Step reward: -15.672521501098421, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1362.7332923245472\n",
      "Episode: 7, Step: 87\n",
      "Next Action: [ 0.045\n",
      "Step reward: -15.732835516705961, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1378.4661278412532\n",
      "Episode: 7, Step: 88\n",
      "Next Action: [ 3.569\n",
      "Step reward: -15.781312546460434, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1394.2474403877136\n",
      "Episode: 7, Step: 89\n",
      "Next Action: [ 0.228\n",
      "Step reward: -15.724019987393023, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1409.9714603751065\n",
      "Episode: 7, Step: 90\n",
      "Next Action: [ 0.471\n",
      "Step reward: -15.694367325569013, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1425.6658277006754\n",
      "Episode: 7, Step: 91\n",
      "Next Action: [ 0.647\n",
      "Step reward: -15.728975537361775, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1441.3948032380372\n",
      "Episode: 7, Step: 92\n",
      "Next Action: [ 0.675\n",
      "Step reward: -15.725668416165167, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1457.1204716542024\n",
      "Episode: 7, Step: 93\n",
      "Next Action: [ 0.922\n",
      "Step reward: -15.677569469102117, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1472.7980411233045\n",
      "Episode: 7, Step: 94\n",
      "Next Action: [ 0.593\n",
      "Step reward: -15.716337212381024, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1488.5143783356855\n",
      "Episode: 7, Step: 95\n",
      "Next Action: [ 0.478\n",
      "Step reward: -15.740413764709444, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1504.254792100395\n",
      "Episode: 7, Step: 96\n",
      "Next Action: [ 8.453\n",
      "Step reward: -15.750397341507446, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1520.0051894419025\n",
      "Episode: 7, Step: 97\n",
      "Next Action: [ 1.108\n",
      "Step reward: -15.77226312021729, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1535.7774525621198\n",
      "Episode: 7, Step: 98\n",
      "Next Action: [ 0.940\n",
      "Step reward: -15.77539960976136, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1551.552852171881\n",
      "Episode: 7, Step: 99\n",
      "Next Action: [ 0.855\n",
      "Step reward: -15.754242200857561, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1567.3070943727387\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 14.994731903076172\n",
      "Actor loss: 48.713890075683594\n",
      "Critic loss: 4.066653728485107\n",
      "Actor loss: 23.93661117553711\n",
      "Critic loss: 105.07174682617188\n",
      "Actor loss: 16.295734405517578\n",
      "Critic loss: 29.380876541137695\n",
      "Actor loss: 20.64539909362793\n",
      "Critic loss: 46.138675689697266\n",
      "Actor loss: 29.474315643310547\n",
      "Critic loss: 15.46623706817627\n",
      "Actor loss: 32.31711959838867\n",
      "Critic loss: 51.38433837890625\n",
      "Actor loss: 24.750995635986328\n",
      "Critic loss: 71.51071166992188\n",
      "Actor loss: 20.371631622314453\n",
      "Critic loss: 11.966954231262207\n",
      "Actor loss: 25.461015701293945\n",
      "Critic loss: 70.10807037353516\n",
      "Actor loss: 21.916345596313477\n",
      "Episode: 8\n",
      "Episode: 8, Step: 0\n",
      "Next Action: [ 0.330\n",
      "Step reward: -11.493671942079736, Next State: [ 0.\n",
      "Total episode reward: -11.493671942079736\n",
      "Episode: 8, Step: 1\n",
      "Next Action: [ 7.545\n",
      "Step reward: -14.248957785039783, Next State: [ 1.\n",
      "Total episode reward: -25.74262972711952\n",
      "Episode: 8, Step: 2\n",
      "Next Action: [ 0.837\n",
      "Step reward: -14.988546210106787, Next State: [ 1.\n",
      "Total episode reward: -40.73117593722631\n",
      "Episode: 8, Step: 3\n",
      "Next Action: [ 0.790\n",
      "Step reward: -15.44417215795058, Next State: [ 1. \n",
      "Total episode reward: -56.17534809517689\n",
      "Episode: 8, Step: 4\n",
      "Next Action: [ 0.682\n",
      "Step reward: -15.617018859504118, Next State: [ 1.\n",
      "Total episode reward: -71.79236695468101\n",
      "Episode: 8, Step: 5\n",
      "Next Action: [ 0.685\n",
      "Step reward: -15.612928039538037, Next State: [ 1.\n",
      "Total episode reward: -87.40529499421905\n",
      "Episode: 8, Step: 6\n",
      "Next Action: [ 0.617\n",
      "Step reward: -15.660415513376208, Next State: [ 1.\n",
      "Total episode reward: -103.06571050759526\n",
      "Episode: 8, Step: 7\n",
      "Next Action: [ 0.539\n",
      "Step reward: -15.693744275462976, Next State: [ 1.\n",
      "Total episode reward: -118.75945478305823\n",
      "Episode: 8, Step: 8\n",
      "Next Action: [ 0.893\n",
      "Step reward: -15.756375644931033, Next State: [ 1.\n",
      "Total episode reward: -134.51583042798927\n",
      "Episode: 8, Step: 9\n",
      "Next Action: [ 0.452\n",
      "Step reward: -15.776783184374038, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -150.2926136123633\n",
      "Episode: 8, Step: 10\n",
      "Next Action: [ 0.652\n",
      "Step reward: -15.755226797592103, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -166.04784040995543\n",
      "Episode: 8, Step: 11\n",
      "Next Action: [ 0.388\n",
      "Step reward: -15.76348047326082, Next State: [ 1.0\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -181.81132088321624\n",
      "Episode: 8, Step: 12\n",
      "Next Action: [ 0.102\n",
      "Step reward: -15.761005842786863, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -197.5723267260031\n",
      "Episode: 8, Step: 13\n",
      "Next Action: [ 0.272\n",
      "Step reward: -15.77698388275184, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -213.34931060875493\n",
      "Episode: 8, Step: 14\n",
      "Next Action: [-0.037\n",
      "Step reward: -15.839467589026345, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -229.1887781977813\n",
      "Episode: 8, Step: 15\n",
      "Next Action: [ 0.147\n",
      "Step reward: -15.865506529352519, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -245.05428472713382\n",
      "Episode: 8, Step: 16\n",
      "Next Action: [ 0.194\n",
      "Step reward: -15.889887116667696, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -260.94417184380154\n",
      "Episode: 8, Step: 17\n",
      "Next Action: [ 0.275\n",
      "Step reward: -15.861413064633467, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -276.805584908435\n",
      "Episode: 8, Step: 18\n",
      "Next Action: [ 1.628\n",
      "Step reward: -15.850288869023505, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -292.6558737774585\n",
      "Episode: 8, Step: 19\n",
      "Next Action: [ 0.299\n",
      "Step reward: -15.791483792863719, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -308.44735757032225\n",
      "Episode: 8, Step: 20\n",
      "Next Action: [ 0.819\n",
      "Step reward: -15.771314628639152, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -324.2186721989614\n",
      "Episode: 8, Step: 21\n",
      "Next Action: [ 0.735\n",
      "Step reward: -15.771788142117554, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -339.990460341079\n",
      "Episode: 8, Step: 22\n",
      "Next Action: [ 7.814\n",
      "Step reward: -15.753443374674818, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -355.7439037157538\n",
      "Episode: 8, Step: 23\n",
      "Next Action: [ 0.619\n",
      "Step reward: -15.764584491233926, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -371.5084882069877\n",
      "Episode: 8, Step: 24\n",
      "Next Action: [ 0.429\n",
      "Step reward: -15.821888205466486, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -387.3303764124542\n",
      "Episode: 8, Step: 25\n",
      "Next Action: [ 0.414\n",
      "Step reward: -15.833204088392145, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -403.16358050084636\n",
      "Episode: 8, Step: 26\n",
      "Next Action: [ 0.170\n",
      "Step reward: -15.849753989468358, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -419.0133344903147\n",
      "Episode: 8, Step: 27\n",
      "Next Action: [ 0.122\n",
      "Step reward: -15.834002822614458, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -434.8473373129292\n",
      "Episode: 8, Step: 28\n",
      "Next Action: [ 0.267\n",
      "Step reward: -15.841231527394562, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -450.68856884032374\n",
      "Episode: 8, Step: 29\n",
      "Next Action: [ 4.203\n",
      "Step reward: -15.881931057630592, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -466.57049989795433\n",
      "Episode: 8, Step: 30\n",
      "Next Action: [ 0.510\n",
      "Step reward: -15.884920753281094, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -482.45542065123544\n",
      "Episode: 8, Step: 31\n",
      "Next Action: [ 0.186\n",
      "Step reward: -15.926282498654059, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -498.3817031498895\n",
      "Episode: 8, Step: 32\n",
      "Next Action: [ 0.683\n",
      "Step reward: -15.918546297052343, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -514.3002494469418\n",
      "Episode: 8, Step: 33\n",
      "Next Action: [ 0.801\n",
      "Step reward: -15.855877619190634, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -530.1561270661325\n",
      "Episode: 8, Step: 34\n",
      "Next Action: [ 0.807\n",
      "Step reward: -15.812651089924119, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -545.9687781560566\n",
      "Episode: 8, Step: 35\n",
      "Next Action: [ 0.727\n",
      "Step reward: -15.824531136475466, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -561.7933092925321\n",
      "Episode: 8, Step: 36\n",
      "Next Action: [ 0.586\n",
      "Step reward: -15.823341308089477, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -577.6166506006215\n",
      "Episode: 8, Step: 37\n",
      "Next Action: [ 5.233\n",
      "Step reward: -15.833774197724493, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -593.4504247983459\n",
      "Episode: 8, Step: 38\n",
      "Next Action: [ 4.775\n",
      "Step reward: -15.804572654248659, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -609.2549974525946\n",
      "Episode: 8, Step: 39\n",
      "Next Action: [ 0.253\n",
      "Step reward: -15.770127593460792, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -625.0251250460553\n",
      "Episode: 8, Step: 40\n",
      "Next Action: [ 0.336\n",
      "Step reward: -15.72364787097523, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -640.7487729170306\n",
      "Episode: 8, Step: 41\n",
      "Next Action: [ 0.577\n",
      "Step reward: -15.73231728473138, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -656.481090201762\n",
      "Episode: 8, Step: 42\n",
      "Next Action: [ 0.876\n",
      "Step reward: -15.745279980860207, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -672.2263701826222\n",
      "Episode: 8, Step: 43\n",
      "Next Action: [ 0.643\n",
      "Step reward: -15.752415677855094, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -687.9787858604773\n",
      "Episode: 8, Step: 44\n",
      "Next Action: [ 0.646\n",
      "Step reward: -15.725542620390922, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -703.7043284808682\n",
      "Episode: 8, Step: 45\n",
      "Next Action: [ 0.809\n",
      "Step reward: -15.73771391935491, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -719.442042400223\n",
      "Episode: 8, Step: 46\n",
      "Next Action: [ 0.601\n",
      "Step reward: -15.705900018829233, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -735.1479424190522\n",
      "Episode: 8, Step: 47\n",
      "Next Action: [ 0.159\n",
      "Step reward: -15.708357370931669, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -750.856299789984\n",
      "Episode: 8, Step: 48\n",
      "Next Action: [ 5.462\n",
      "Step reward: -15.71176133055319, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -766.5680611205371\n",
      "Episode: 8, Step: 49\n",
      "Next Action: [ 1.214\n",
      "Step reward: -15.7369756152211, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -782.3050367357582\n",
      "Episode: 8, Step: 50\n",
      "Next Action: [ 1.134\n",
      "Step reward: -15.738489022663876, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -798.0435257584221\n",
      "Episode: 8, Step: 51\n",
      "Next Action: [ 1.340\n",
      "Step reward: -15.750877313697064, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -813.7944030721192\n",
      "Episode: 8, Step: 52\n",
      "Next Action: [ 1.271\n",
      "Step reward: -15.76755380142547, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -829.5619568735447\n",
      "Episode: 8, Step: 53\n",
      "Next Action: [ 0.937\n",
      "Step reward: -15.784355554305172, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -845.3463124278499\n",
      "Episode: 8, Step: 54\n",
      "Next Action: [ 1.136\n",
      "Step reward: -15.82645189177969, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -861.1727643196296\n",
      "Episode: 8, Step: 55\n",
      "Next Action: [ 1.270\n",
      "Step reward: -15.877295655483724, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -877.0500599751133\n",
      "Episode: 8, Step: 56\n",
      "Next Action: [ 1.071\n",
      "Step reward: -15.850864251689078, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -892.9009242268024\n",
      "Episode: 8, Step: 57\n",
      "Next Action: [ 1.174\n",
      "Step reward: -15.83083760643926, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -908.7317618332416\n",
      "Episode: 8, Step: 58\n",
      "Next Action: [ 0.977\n",
      "Step reward: -15.824570969866699, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -924.5563328031084\n",
      "Episode: 8, Step: 59\n",
      "Next Action: [ 8.521\n",
      "Step reward: -15.828888671689802, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -940.3852214747982\n",
      "Episode: 8, Step: 60\n",
      "Next Action: [ 0.876\n",
      "Step reward: -15.800078396537947, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -956.1852998713362\n",
      "Episode: 8, Step: 61\n",
      "Next Action: [ 0.836\n",
      "Step reward: -15.809901798222642, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -971.9952016695588\n",
      "Episode: 8, Step: 62\n",
      "Next Action: [ 1.014\n",
      "Step reward: -15.844847545863093, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -987.8400492154219\n",
      "Episode: 8, Step: 63\n",
      "Next Action: [ 0.753\n",
      "Step reward: -15.8270297120585, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1003.6670789274804\n",
      "Episode: 8, Step: 64\n",
      "Next Action: [ 0.913\n",
      "Step reward: -15.8058594140128, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1019.4729383414932\n",
      "Episode: 8, Step: 65\n",
      "Next Action: [ 0.726\n",
      "Step reward: -15.781972819347684, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1035.254911160841\n",
      "Episode: 8, Step: 66\n",
      "Next Action: [ 0.952\n",
      "Step reward: -15.770357938419071, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1051.02526909926\n",
      "Episode: 8, Step: 67\n",
      "Next Action: [ 1.407\n",
      "Step reward: -15.789445864310094, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1066.81471496357\n",
      "Episode: 8, Step: 68\n",
      "Next Action: [ 1.791\n",
      "Step reward: -15.81500977706545, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1082.6297247406353\n",
      "Episode: 8, Step: 69\n",
      "Next Action: [ 1.501\n",
      "Step reward: -15.787451023929904, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1098.4171757645652\n",
      "Episode: 8, Step: 70\n",
      "Next Action: [ 1.499\n",
      "Step reward: -15.797203673235193, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1114.2143794378003\n",
      "Episode: 8, Step: 71\n",
      "Next Action: [ 1.416\n",
      "Step reward: -15.83760571033665, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1130.051985148137\n",
      "Episode: 8, Step: 72\n",
      "Next Action: [ 1.361\n",
      "Step reward: -15.834964344423451, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.8869494925605\n",
      "Episode: 8, Step: 73\n",
      "Next Action: [ 1.033\n",
      "Step reward: -15.860127109110723, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.747076601671\n",
      "Episode: 8, Step: 74\n",
      "Next Action: [ 1.394\n",
      "Step reward: -15.85085317299874, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.5979297746699\n",
      "Episode: 8, Step: 75\n",
      "Next Action: [ 1.209\n",
      "Step reward: -15.820740793508525, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.4186705681784\n",
      "Episode: 8, Step: 76\n",
      "Next Action: [ 1.194\n",
      "Step reward: -15.828112790381693, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.2467833585602\n",
      "Episode: 8, Step: 77\n",
      "Next Action: [ 1.228\n",
      "Step reward: -15.818375244985347, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.0651586035456\n",
      "Episode: 8, Step: 78\n",
      "Next Action: [ 9.836\n",
      "Step reward: -15.806724641948396, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.871883245494\n",
      "Episode: 8, Step: 79\n",
      "Next Action: [ 8.205\n",
      "Step reward: -15.797764859862431, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.6696481053564\n",
      "Episode: 8, Step: 80\n",
      "Next Action: [ 0.609\n",
      "Step reward: -15.782774941055315, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.4524230464117\n",
      "Episode: 8, Step: 81\n",
      "Next Action: [ 0.475\n",
      "Step reward: -15.790108880304818, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.2425319267165\n",
      "Episode: 8, Step: 82\n",
      "Next Action: [ 0.097\n",
      "Step reward: -15.752696584588362, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.995228511305\n",
      "Episode: 8, Step: 83\n",
      "Next Action: [ 0.179\n",
      "Step reward: -15.748138386371608, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.7433668976767\n",
      "Episode: 8, Step: 84\n",
      "Next Action: [ 3.478\n",
      "Step reward: -15.807589896701149, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.5509567943777\n",
      "Episode: 8, Step: 85\n",
      "Next Action: [ 0.120\n",
      "Step reward: -15.8062223615241, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.3571791559018\n",
      "Episode: 8, Step: 86\n",
      "Next Action: [ 0.507\n",
      "Step reward: -15.793318116593545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.1504972724954\n",
      "Episode: 8, Step: 87\n",
      "Next Action: [ 0.459\n",
      "Step reward: -15.792607347510907, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.9431046200064\n",
      "Episode: 8, Step: 88\n",
      "Next Action: [ 0.331\n",
      "Step reward: -15.772689781007438, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.7157944010137\n",
      "Episode: 8, Step: 89\n",
      "Next Action: [ 0.564\n",
      "Step reward: -15.770437882601529, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.4862322836152\n",
      "Episode: 8, Step: 90\n",
      "Next Action: [ 0.796\n",
      "Step reward: -15.790737796240089, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.2769700798553\n",
      "Episode: 8, Step: 91\n",
      "Next Action: [ 0.812\n",
      "Step reward: -15.843581915156996, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.1205519950124\n",
      "Episode: 8, Step: 92\n",
      "Next Action: [ 0.772\n",
      "Step reward: -15.8792497720095, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1461.9998017670218\n",
      "Episode: 8, Step: 93\n",
      "Next Action: [ 0.757\n",
      "Step reward: -15.862434844810739, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1477.8622366118325\n",
      "Episode: 8, Step: 94\n",
      "Next Action: [ 0.972\n",
      "Step reward: -15.857130238794795, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.7193668506272\n",
      "Episode: 8, Step: 95\n",
      "Next Action: [ 1.348\n",
      "Step reward: -15.816007232424106, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.5353740830512\n",
      "Episode: 8, Step: 96\n",
      "Next Action: [ 0.747\n",
      "Step reward: -15.736597245471282, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.2719713285226\n",
      "Episode: 8, Step: 97\n",
      "Next Action: [ 0.871\n",
      "Step reward: -15.687109475264341, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1540.959080803787\n",
      "Episode: 8, Step: 98\n",
      "Next Action: [ 0.924\n",
      "Step reward: -15.69173743816855, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1556.6508182419554\n",
      "Episode: 8, Step: 99\n",
      "Next Action: [ 0.811\n",
      "Step reward: -15.691100232435875, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1572.3419184743914\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 13.94919204711914\n",
      "Actor loss: 47.101383209228516\n",
      "Critic loss: 31.75732421875\n",
      "Actor loss: 28.070613861083984\n",
      "Critic loss: 22.10262107849121\n",
      "Actor loss: 30.37154769897461\n",
      "Critic loss: 7.810779571533203\n",
      "Actor loss: 51.71147537231445\n",
      "Critic loss: 11.258853912353516\n",
      "Actor loss: 33.36375045776367\n",
      "Critic loss: 7.4514946937561035\n",
      "Actor loss: 65.17684173583984\n",
      "Critic loss: 45.56515121459961\n",
      "Actor loss: 55.41008758544922\n",
      "Critic loss: 46.77226638793945\n",
      "Actor loss: 51.98305130004883\n",
      "Critic loss: 13.163018226623535\n",
      "Actor loss: 41.829307556152344\n",
      "Critic loss: 3.8769400119781494\n",
      "Actor loss: 49.46342849731445\n",
      "Episode: 9\n",
      "Episode: 9, Step: 0\n",
      "Next Action: [ 0.185\n",
      "Step reward: -11.8861328596626, Next State: [ 0.86\n",
      "Total episode reward: -11.8861328596626\n",
      "Episode: 9, Step: 1\n",
      "Next Action: [ 0.718\n",
      "Step reward: -14.424122219680932, Next State: [ 1.\n",
      "Total episode reward: -26.310255079343534\n",
      "Episode: 9, Step: 2\n",
      "Next Action: [ 1.018\n",
      "Step reward: -15.227267097356455, Next State: [ 1.\n",
      "Total episode reward: -41.53752217669999\n",
      "Episode: 9, Step: 3\n",
      "Next Action: [ 1.037\n",
      "Step reward: -15.445841341484522, Next State: [ 1.\n",
      "Total episode reward: -56.98336351818451\n",
      "Episode: 9, Step: 4\n",
      "Next Action: [ 0.588\n",
      "Step reward: -15.551580934485427, Next State: [ 1.\n",
      "Total episode reward: -72.53494445266993\n",
      "Episode: 9, Step: 5\n",
      "Next Action: [ 6.913\n",
      "Step reward: -15.640532579339839, Next State: [ 1.\n",
      "Total episode reward: -88.17547703200977\n",
      "Episode: 9, Step: 6\n",
      "Next Action: [ 0.624\n",
      "Step reward: -15.746099496675207, Next State: [ 1.\n",
      "Total episode reward: -103.92157652868497\n",
      "Episode: 9, Step: 7\n",
      "Next Action: [ 0.882\n",
      "Step reward: -15.80154532826906, Next State: [ 1. \n",
      "Total episode reward: -119.72312185695402\n",
      "Episode: 9, Step: 8\n",
      "Next Action: [ 1.063\n",
      "Step reward: -15.804155369312975, Next State: [ 1.\n",
      "Total episode reward: -135.527277226267\n",
      "Episode: 9, Step: 9\n",
      "Next Action: [ 1.388\n",
      "Step reward: -15.803305343604592, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.33058256987158\n",
      "Episode: 9, Step: 10\n",
      "Next Action: [ 1.629\n",
      "Step reward: -15.84325482310393, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.1738373929755\n",
      "Episode: 9, Step: 11\n",
      "Next Action: [ 1.765\n",
      "Step reward: -15.880976707829495, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.054814100805\n",
      "Episode: 9, Step: 12\n",
      "Next Action: [ 1.620\n",
      "Step reward: -15.86218254040945, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -198.91699664121444\n",
      "Episode: 9, Step: 13\n",
      "Next Action: [ 1.682\n",
      "Step reward: -15.838340761400866, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.7553374026153\n",
      "Episode: 9, Step: 14\n",
      "Next Action: [ 1.471\n",
      "Step reward: -15.828423921958883, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.5837613245742\n",
      "Episode: 9, Step: 15\n",
      "Next Action: [ 1.497\n",
      "Step reward: -15.83453654969928, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.4182978742735\n",
      "Episode: 9, Step: 16\n",
      "Next Action: [ 1.530\n",
      "Step reward: -15.82854993390109, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.2468478081746\n",
      "Episode: 9, Step: 17\n",
      "Next Action: [ 1.629\n",
      "Step reward: -15.807489386641933, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.0543371948165\n",
      "Episode: 9, Step: 18\n",
      "Next Action: [ 1.596\n",
      "Step reward: -15.82515558212517, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -293.8794927769417\n",
      "Episode: 9, Step: 19\n",
      "Next Action: [ 1.441\n",
      "Step reward: -15.848824570409256, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -309.7283173473509\n",
      "Episode: 9, Step: 20\n",
      "Next Action: [ 1.552\n",
      "Step reward: -15.874293831432304, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -325.6026111787832\n",
      "Episode: 9, Step: 21\n",
      "Next Action: [ 1.372\n",
      "Step reward: -15.871228582086111, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.47383976086934\n",
      "Episode: 9, Step: 22\n",
      "Next Action: [ 1.166\n",
      "Step reward: -15.905320814576449, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -357.3791605754458\n",
      "Episode: 9, Step: 23\n",
      "Next Action: [ 0.771\n",
      "Step reward: -15.926480215391855, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -373.30564079083763\n",
      "Episode: 9, Step: 24\n",
      "Next Action: [ 0.762\n",
      "Step reward: -15.904889107959262, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.2105298987969\n",
      "Episode: 9, Step: 25\n",
      "Next Action: [ 0.502\n",
      "Step reward: -15.903592668430726, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.11412256722764\n",
      "Episode: 9, Step: 26\n",
      "Next Action: [ 0.525\n",
      "Step reward: -15.904971704283625, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.0190942715113\n",
      "Episode: 9, Step: 27\n",
      "Next Action: [-0.022\n",
      "Step reward: -15.940115519143133, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -436.9592097906544\n",
      "Episode: 9, Step: 28\n",
      "Next Action: [ 0.128\n",
      "Step reward: -15.9889520675669, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -452.94816185822134\n",
      "Episode: 9, Step: 29\n",
      "Next Action: [-0.100\n",
      "Step reward: -15.98724973418666, Next State: [ 0.8\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -468.935411592408\n",
      "Episode: 9, Step: 30\n",
      "Next Action: [-0.334\n",
      "Step reward: -15.91622950450456, Next State: [ 0.5\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -484.85164109691254\n",
      "Episode: 9, Step: 31\n",
      "Next Action: [-0.545\n",
      "Step reward: -15.866993648042858, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -500.7186347449554\n",
      "Episode: 9, Step: 32\n",
      "Next Action: [-0.523\n",
      "Step reward: -15.849534515592664, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.5681692605481\n",
      "Episode: 9, Step: 33\n",
      "Next Action: [-0.212\n",
      "Step reward: -15.847324534870296, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.4154937954183\n",
      "Episode: 9, Step: 34\n",
      "Next Action: [-0.414\n",
      "Step reward: -15.878799797421824, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.2942935928402\n",
      "Episode: 9, Step: 35\n",
      "Next Action: [-0.341\n",
      "Step reward: -15.875633331492123, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.1699269243323\n",
      "Episode: 9, Step: 36\n",
      "Next Action: [-2.604\n",
      "Step reward: -15.89512100404013, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.0650479283725\n",
      "Episode: 9, Step: 37\n",
      "Next Action: [ 0.176\n",
      "Step reward: -15.922326782085866, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -595.9873747104583\n",
      "Episode: 9, Step: 38\n",
      "Next Action: [ 0.413\n",
      "Step reward: -15.899609219690811, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -611.8869839301491\n",
      "Episode: 9, Step: 39\n",
      "Next Action: [ 0.156\n",
      "Step reward: -15.924836354878314, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -627.8118202850274\n",
      "Episode: 9, Step: 40\n",
      "Next Action: [ 0.157\n",
      "Step reward: -15.91068776943443, Next State: [-0.0\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -643.7225080544617\n",
      "Episode: 9, Step: 41\n",
      "Next Action: [ 0.071\n",
      "Step reward: -15.805433629804156, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -659.5279416842659\n",
      "Episode: 9, Step: 42\n",
      "Next Action: [-0.205\n",
      "Step reward: -15.756681190455389, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -675.2846228747213\n",
      "Episode: 9, Step: 43\n",
      "Next Action: [-0.020\n",
      "Step reward: -15.7477384302446, Next State: [-0.25\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -691.0323613049659\n",
      "Episode: 9, Step: 44\n",
      "Next Action: [ 0.120\n",
      "Step reward: -15.809364768091612, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.8417260730575\n",
      "Episode: 9, Step: 45\n",
      "Next Action: [ 0.137\n",
      "Step reward: -15.838080797495278, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -722.6798068705529\n",
      "Episode: 9, Step: 46\n",
      "Next Action: [ 0.239\n",
      "Step reward: -15.838393511451393, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -738.5182003820042\n",
      "Episode: 9, Step: 47\n",
      "Next Action: [ 0.128\n",
      "Step reward: -15.841836764304078, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -754.3600371463083\n",
      "Episode: 9, Step: 48\n",
      "Next Action: [ 0.386\n",
      "Step reward: -15.877190290067956, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.2372274363762\n",
      "Episode: 9, Step: 49\n",
      "Next Action: [ 7.363\n",
      "Step reward: -15.892984383690727, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -786.130211820067\n",
      "Episode: 9, Step: 50\n",
      "Next Action: [ 0.896\n",
      "Step reward: -15.840420825451494, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -801.9706326455184\n",
      "Episode: 9, Step: 51\n",
      "Next Action: [ 1.277\n",
      "Step reward: -15.775545424783779, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.7461780703022\n",
      "Episode: 9, Step: 52\n",
      "Next Action: [ 1.149\n",
      "Step reward: -15.774097131876264, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -833.5202752021785\n",
      "Episode: 9, Step: 53\n",
      "Next Action: [ 1.074\n",
      "Step reward: -15.823855088873906, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -849.3441302910524\n",
      "Episode: 9, Step: 54\n",
      "Next Action: [ 1.051\n",
      "Step reward: -15.853259682576432, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -865.1973899736288\n",
      "Episode: 9, Step: 55\n",
      "Next Action: [ 0.803\n",
      "Step reward: -15.87042109618007, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -881.0678110698088\n",
      "Episode: 9, Step: 56\n",
      "Next Action: [ 0.404\n",
      "Step reward: -15.861670051543344, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -896.9294811213522\n",
      "Episode: 9, Step: 57\n",
      "Next Action: [ 0.142\n",
      "Step reward: -15.841764065364375, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -912.7712451867166\n",
      "Episode: 9, Step: 58\n",
      "Next Action: [ 1.232\n",
      "Step reward: -15.842191023630708, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -928.6134362103472\n",
      "Episode: 9, Step: 59\n",
      "Next Action: [ 0.293\n",
      "Step reward: -15.832994053195339, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -944.4464302635425\n",
      "Episode: 9, Step: 60\n",
      "Next Action: [ 0.061\n",
      "Step reward: -15.83846865488903, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -960.2848989184316\n",
      "Episode: 9, Step: 61\n",
      "Next Action: [ 0.090\n",
      "Step reward: -15.861110727519725, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -976.1460096459514\n",
      "Episode: 9, Step: 62\n",
      "Next Action: [ 0.403\n",
      "Step reward: -15.825337961618157, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -991.9713476075696\n",
      "Episode: 9, Step: 63\n",
      "Next Action: [ 0.184\n",
      "Step reward: -15.796548394131266, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1007.7678960017008\n",
      "Episode: 9, Step: 64\n",
      "Next Action: [ 0.376\n",
      "Step reward: -15.759302111684836, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1023.5271981133857\n",
      "Episode: 9, Step: 65\n",
      "Next Action: [ 0.645\n",
      "Step reward: -15.773773719878296, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1039.300971833264\n",
      "Episode: 9, Step: 66\n",
      "Next Action: [ 0.726\n",
      "Step reward: -15.807821077914108, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1055.108792911178\n",
      "Episode: 9, Step: 67\n",
      "Next Action: [ 0.861\n",
      "Step reward: -15.834395093800612, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1070.9431880049785\n",
      "Episode: 9, Step: 68\n",
      "Next Action: [ 1.104\n",
      "Step reward: -15.845775544873202, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1086.7889635498518\n",
      "Episode: 9, Step: 69\n",
      "Next Action: [ 1.281\n",
      "Step reward: -15.840959690378297, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1102.62992324023\n",
      "Episode: 9, Step: 70\n",
      "Next Action: [ 1.333\n",
      "Step reward: -15.803911563994545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1118.4338348042245\n",
      "Episode: 9, Step: 71\n",
      "Next Action: [ 0.995\n",
      "Step reward: -15.797581545555708, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1134.2314163497801\n",
      "Episode: 9, Step: 72\n",
      "Next Action: [ 1.225\n",
      "Step reward: -15.792590752333583, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1150.0240071021137\n",
      "Episode: 9, Step: 73\n",
      "Next Action: [ 0.960\n",
      "Step reward: -15.807430946244077, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1165.8314380483578\n",
      "Episode: 9, Step: 74\n",
      "Next Action: [ 0.981\n",
      "Step reward: -15.827366947170558, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1181.6588049955283\n",
      "Episode: 9, Step: 75\n",
      "Next Action: [ 0.661\n",
      "Step reward: -15.842541184143249, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1197.5013461796716\n",
      "Episode: 9, Step: 76\n",
      "Next Action: [ 0.547\n",
      "Step reward: -15.862564068866611, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1213.363910248538\n",
      "Episode: 9, Step: 77\n",
      "Next Action: [ 0.838\n",
      "Step reward: -15.874254635497996, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1229.238164884036\n",
      "Episode: 9, Step: 78\n",
      "Next Action: [ 1.099\n",
      "Step reward: -15.837306707189452, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1245.0754715912256\n",
      "Episode: 9, Step: 79\n",
      "Next Action: [ 0.912\n",
      "Step reward: -15.805000680829636, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1260.8804722720552\n",
      "Episode: 9, Step: 80\n",
      "Next Action: [ 0.653\n",
      "Step reward: -15.803798678636529, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1276.6842709506916\n",
      "Episode: 9, Step: 81\n",
      "Next Action: [ 0.435\n",
      "Step reward: -15.787007359890922, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1292.4712783105824\n",
      "Episode: 9, Step: 82\n",
      "Next Action: [ 0.193\n",
      "Step reward: -15.808951468708552, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1308.280229779291\n",
      "Episode: 9, Step: 83\n",
      "Next Action: [ 0.250\n",
      "Step reward: -15.81067408237725, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1324.0909038616683\n",
      "Episode: 9, Step: 84\n",
      "Next Action: [ 0.308\n",
      "Step reward: -15.798614900346946, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1339.8895187620153\n",
      "Episode: 9, Step: 85\n",
      "Next Action: [ 0.396\n",
      "Step reward: -15.782021452704585, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1355.67154021472\n",
      "Episode: 9, Step: 86\n",
      "Next Action: [ 0.260\n",
      "Step reward: -15.758665175713459, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1371.4302053904335\n",
      "Episode: 9, Step: 87\n",
      "Next Action: [ 0.242\n",
      "Step reward: -15.767862995283796, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1387.1980683857173\n",
      "Episode: 9, Step: 88\n",
      "Next Action: [ 0.213\n",
      "Step reward: -15.805793281290708, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1403.003861667008\n",
      "Episode: 9, Step: 89\n",
      "Next Action: [ 0.714\n",
      "Step reward: -15.84988122973595, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1418.853742896744\n",
      "Episode: 9, Step: 90\n",
      "Next Action: [ 6.715\n",
      "Step reward: -15.856735040296236, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1434.7104779370402\n",
      "Episode: 9, Step: 91\n",
      "Next Action: [ 0.398\n",
      "Step reward: -15.85088354580037, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1450.5613614828405\n",
      "Episode: 9, Step: 92\n",
      "Next Action: [ 0.441\n",
      "Step reward: -15.8523411699037, Next State: [ 1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1466.4137026527442\n",
      "Episode: 9, Step: 93\n",
      "Next Action: [ 0.754\n",
      "Step reward: -15.836466231052007, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1482.2501688837963\n",
      "Episode: 9, Step: 94\n",
      "Next Action: [ 0.491\n",
      "Step reward: -15.806598575767545, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1498.0567674595638\n",
      "Episode: 9, Step: 95\n",
      "Next Action: [ 9.832\n",
      "Step reward: -15.779886903721879, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1513.8366543632858\n",
      "Episode: 9, Step: 96\n",
      "Next Action: [ 0.987\n",
      "Step reward: -15.79946887953883, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1529.6361232428246\n",
      "Episode: 9, Step: 97\n",
      "Next Action: [ 0.950\n",
      "Step reward: -15.826299576901944, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1545.4624228197265\n",
      "Episode: 9, Step: 98\n",
      "Next Action: [ 0.883\n",
      "Step reward: -15.837772296450394, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1561.3001951161768\n",
      "Episode: 9, Step: 99\n",
      "Next Action: [ 1.336\n",
      "Step reward: -15.839534593368304, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1577.1397297095452\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.757685661315918\n",
      "Actor loss: 37.340763092041016\n",
      "Critic loss: 12.963990211486816\n",
      "Actor loss: 57.67998504638672\n",
      "Critic loss: 29.788745880126953\n",
      "Actor loss: 36.83087158203125\n",
      "Critic loss: 24.063899993896484\n",
      "Actor loss: 31.299678802490234\n",
      "Critic loss: 13.404463768005371\n",
      "Actor loss: 39.83685302734375\n",
      "Critic loss: 9.232010841369629\n",
      "Actor loss: 44.02730178833008\n",
      "Critic loss: 26.74138641357422\n",
      "Actor loss: 42.18144989013672\n",
      "Critic loss: 8.300555229187012\n",
      "Actor loss: 47.78946304321289\n",
      "Critic loss: 14.194757461547852\n",
      "Actor loss: 47.01908493041992\n",
      "Critic loss: 28.186323165893555\n",
      "Actor loss: 51.44085693359375\n",
      "Episode: 10\n",
      "Episode: 10, Step: 0\n",
      "Next Action: [ 0.424\n",
      "Step reward: -12.398417450515826, Next State: [-0.\n",
      "Total episode reward: -12.398417450515826\n",
      "Episode: 10, Step: 1\n",
      "Next Action: [ 0.316\n",
      "Step reward: -14.8636589652648, Next State: [-0.05\n",
      "Total episode reward: -27.262076415780626\n",
      "Episode: 10, Step: 2\n",
      "Next Action: [ 0.677\n",
      "Step reward: -15.597152306637172, Next State: [ 6.\n",
      "Total episode reward: -42.8592287224178\n",
      "Episode: 10, Step: 3\n",
      "Next Action: [ 0.305\n",
      "Step reward: -15.856843265214845, Next State: [ 0.\n",
      "Total episode reward: -58.716071987632645\n",
      "Episode: 10, Step: 4\n",
      "Next Action: [ 0.134\n",
      "Step reward: -15.883463911557937, Next State: [ 1.\n",
      "Total episode reward: -74.59953589919058\n",
      "Episode: 10, Step: 5\n",
      "Next Action: [-0.007\n",
      "Step reward: -15.82906480842032, Next State: [ 0.9\n",
      "Total episode reward: -90.42860070761091\n",
      "Episode: 10, Step: 6\n",
      "Next Action: [ 0.135\n",
      "Step reward: -15.818067346983046, Next State: [ 1.\n",
      "Total episode reward: -106.24666805459395\n",
      "Episode: 10, Step: 7\n",
      "Next Action: [-4.257\n",
      "Step reward: -15.838844862361203, Next State: [ 0.\n",
      "Total episode reward: -122.08551291695515\n",
      "Episode: 10, Step: 8\n",
      "Next Action: [-0.532\n",
      "Step reward: -15.823600385763529, Next State: [ 0.\n",
      "Total episode reward: -137.90911330271868\n",
      "Episode: 10, Step: 9\n",
      "Next Action: [-0.205\n",
      "Step reward: -15.800480605990959, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.70959390870965\n",
      "Episode: 10, Step: 10\n",
      "Next Action: [ 0.004\n",
      "Step reward: -15.796371969192334, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.50596587790199\n",
      "Episode: 10, Step: 11\n",
      "Next Action: [-0.152\n",
      "Step reward: -15.815127993367136, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.3210938712691\n",
      "Episode: 10, Step: 12\n",
      "Next Action: [-0.097\n",
      "Step reward: -15.81655304041498, Next State: [-0.4\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.1376469116841\n",
      "Episode: 10, Step: 13\n",
      "Next Action: [-0.189\n",
      "Step reward: -15.815951082069756, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.95359799375385\n",
      "Episode: 10, Step: 14\n",
      "Next Action: [-2.551\n",
      "Step reward: -15.795671022390284, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.74926901614413\n",
      "Episode: 10, Step: 15\n",
      "Next Action: [-0.282\n",
      "Step reward: -15.827620023099703, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.57688903924384\n",
      "Episode: 10, Step: 16\n",
      "Next Action: [-0.247\n",
      "Step reward: -15.861449617812836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.43833865705665\n",
      "Episode: 10, Step: 17\n",
      "Next Action: [-0.496\n",
      "Step reward: -15.884823066982806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.32316172403944\n",
      "Episode: 10, Step: 18\n",
      "Next Action: [-0.570\n",
      "Step reward: -15.909900874910859, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.2330625989503\n",
      "Episode: 10, Step: 19\n",
      "Next Action: [-0.243\n",
      "Step reward: -15.92953614678211, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.1625987457324\n",
      "Episode: 10, Step: 20\n",
      "Next Action: [-0.258\n",
      "Step reward: -15.938120988584624, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.100719734317\n",
      "Episode: 10, Step: 21\n",
      "Next Action: [-0.676\n",
      "Step reward: -15.949873248709617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.0505929830266\n",
      "Episode: 10, Step: 22\n",
      "Next Action: [-0.434\n",
      "Step reward: -15.941600084038438, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -359.99219306706505\n",
      "Episode: 10, Step: 23\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.935463213317108, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.92765628038217\n",
      "Episode: 10, Step: 24\n",
      "Next Action: [-1.120\n",
      "Step reward: -15.932370363030074, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.86002664341225\n",
      "Episode: 10, Step: 25\n",
      "Next Action: [-0.979\n",
      "Step reward: -15.915397050520614, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.77542369393285\n",
      "Episode: 10, Step: 26\n",
      "Next Action: [-1.194\n",
      "Step reward: -15.922420875757906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.69784456969074\n",
      "Episode: 10, Step: 27\n",
      "Next Action: [-1.088\n",
      "Step reward: -15.927460656776617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.6253052264674\n",
      "Episode: 10, Step: 28\n",
      "Next Action: [-0.781\n",
      "Step reward: -15.953200012385386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.57850523885276\n",
      "Episode: 10, Step: 29\n",
      "Next Action: [-0.757\n",
      "Step reward: -15.954668758231392, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.53317399708413\n",
      "Episode: 10, Step: 30\n",
      "Next Action: [-0.571\n",
      "Step reward: -15.95728900037694, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.49046299746107\n",
      "Episode: 10, Step: 31\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.910835975638566, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.40129897309964\n",
      "Episode: 10, Step: 32\n",
      "Next Action: [-0.438\n",
      "Step reward: -15.894571258775262, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.2958702318749\n",
      "Episode: 10, Step: 33\n",
      "Next Action: [-0.551\n",
      "Step reward: -15.909480959986379, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.2053511918613\n",
      "Episode: 10, Step: 34\n",
      "Next Action: [-0.401\n",
      "Step reward: -15.910671984386303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.1160231762476\n",
      "Episode: 10, Step: 35\n",
      "Next Action: [-0.323\n",
      "Step reward: -15.913025641280154, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -567.0290488175277\n",
      "Episode: 10, Step: 36\n",
      "Next Action: [-0.531\n",
      "Step reward: -15.866054515814762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -582.8951033333425\n",
      "Episode: 10, Step: 37\n",
      "Next Action: [-0.523\n",
      "Step reward: -15.876274508376808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -598.7713778417193\n",
      "Episode: 10, Step: 38\n",
      "Next Action: [-0.814\n",
      "Step reward: -15.906458639504583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -614.6778364812238\n",
      "Episode: 10, Step: 39\n",
      "Next Action: [-0.562\n",
      "Step reward: -15.92200411163924, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -630.5998405928631\n",
      "Episode: 10, Step: 40\n",
      "Next Action: [-0.659\n",
      "Step reward: -15.902436507063332, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -646.5022770999265\n",
      "Episode: 10, Step: 41\n",
      "Next Action: [-0.562\n",
      "Step reward: -15.893819582040601, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -662.396096681967\n",
      "Episode: 10, Step: 42\n",
      "Next Action: [-0.358\n",
      "Step reward: -15.900661316636148, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -678.2967579986032\n",
      "Episode: 10, Step: 43\n",
      "Next Action: [-0.294\n",
      "Step reward: -15.939781313659115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -694.2365393122623\n",
      "Episode: 10, Step: 44\n",
      "Next Action: [-0.627\n",
      "Step reward: -15.930705241976218, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -710.1672445542384\n",
      "Episode: 10, Step: 45\n",
      "Next Action: [-0.814\n",
      "Step reward: -15.922208602610223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -726.0894531568487\n",
      "Episode: 10, Step: 46\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.942121913188371, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -742.031575070037\n",
      "Episode: 10, Step: 47\n",
      "Next Action: [-0.174\n",
      "Step reward: -15.942363532517847, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -757.9739386025549\n",
      "Episode: 10, Step: 48\n",
      "Next Action: [-0.366\n",
      "Step reward: -15.921739228814756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -773.8956778313697\n",
      "Episode: 10, Step: 49\n",
      "Next Action: [-0.308\n",
      "Step reward: -15.86896871012634, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -789.764646541496\n",
      "Episode: 10, Step: 50\n",
      "Next Action: [-0.325\n",
      "Step reward: -15.879855084284962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -805.6445016257809\n",
      "Episode: 10, Step: 51\n",
      "Next Action: [-0.449\n",
      "Step reward: -15.874104457527391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -821.5186060833083\n",
      "Episode: 10, Step: 52\n",
      "Next Action: [-0.264\n",
      "Step reward: -15.87505310833445, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -837.3936591916428\n",
      "Episode: 10, Step: 53\n",
      "Next Action: [ 0.012\n",
      "Step reward: -15.866474614562346, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -853.2601338062051\n",
      "Episode: 10, Step: 54\n",
      "Next Action: [-0.033\n",
      "Step reward: -15.887982680102576, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -869.1481164863077\n",
      "Episode: 10, Step: 55\n",
      "Next Action: [-0.183\n",
      "Step reward: -15.92906180408837, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -885.0771782903961\n",
      "Episode: 10, Step: 56\n",
      "Next Action: [-0.514\n",
      "Step reward: -15.955933686191257, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -901.0331119765873\n",
      "Episode: 10, Step: 57\n",
      "Next Action: [-0.366\n",
      "Step reward: -15.963902249024704, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -916.997014225612\n",
      "Episode: 10, Step: 58\n",
      "Next Action: [-0.114\n",
      "Step reward: -15.94028987685528, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -932.9373041024674\n",
      "Episode: 10, Step: 59\n",
      "Next Action: [-0.279\n",
      "Step reward: -15.927673413180674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -948.864977515648\n",
      "Episode: 10, Step: 60\n",
      "Next Action: [-0.376\n",
      "Step reward: -15.902902936916076, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -964.7678804525641\n",
      "Episode: 10, Step: 61\n",
      "Next Action: [-0.571\n",
      "Step reward: -15.925096412834014, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -980.6929768653981\n",
      "Episode: 10, Step: 62\n",
      "Next Action: [-0.470\n",
      "Step reward: -15.965876340893558, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -996.6588532062917\n",
      "Episode: 10, Step: 63\n",
      "Next Action: [-0.565\n",
      "Step reward: -15.96907434424701, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1012.6279275505387\n",
      "Episode: 10, Step: 64\n",
      "Next Action: [-0.963\n",
      "Step reward: -15.940617397499532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1028.5685449480384\n",
      "Episode: 10, Step: 65\n",
      "Next Action: [-1.172\n",
      "Step reward: -15.911428007108256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1044.4799729551466\n",
      "Episode: 10, Step: 66\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.947035923378209, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1060.427008878525\n",
      "Episode: 10, Step: 67\n",
      "Next Action: [-0.554\n",
      "Step reward: -15.95459340848507, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1076.38160228701\n",
      "Episode: 10, Step: 68\n",
      "Next Action: [-0.592\n",
      "Step reward: -15.960791693749202, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1092.3423939807592\n",
      "Episode: 10, Step: 69\n",
      "Next Action: [-0.464\n",
      "Step reward: -15.97072499773083, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1108.3131189784901\n",
      "Episode: 10, Step: 70\n",
      "Next Action: [-0.411\n",
      "Step reward: -15.944533149470764, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1124.257652127961\n",
      "Episode: 10, Step: 71\n",
      "Next Action: [-0.230\n",
      "Step reward: -15.944064108266518, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1140.2017162362274\n",
      "Episode: 10, Step: 72\n",
      "Next Action: [-0.220\n",
      "Step reward: -15.951302854264805, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1156.1530190904923\n",
      "Episode: 10, Step: 73\n",
      "Next Action: [-0.043\n",
      "Step reward: -15.947432999004256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1172.1004520894965\n",
      "Episode: 10, Step: 74\n",
      "Next Action: [ 0.018\n",
      "Step reward: -15.905314367429396, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1188.0057664569258\n",
      "Episode: 10, Step: 75\n",
      "Next Action: [-0.172\n",
      "Step reward: -15.891409350741391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1203.8971758076673\n",
      "Episode: 10, Step: 76\n",
      "Next Action: [-0.393\n",
      "Step reward: -15.863029985230085, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1219.7602057928973\n",
      "Episode: 10, Step: 77\n",
      "Next Action: [-0.046\n",
      "Step reward: -15.892824584762522, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1235.65303037766\n",
      "Episode: 10, Step: 78\n",
      "Next Action: [-2.073\n",
      "Step reward: -15.904825988334965, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1251.557856365995\n",
      "Episode: 10, Step: 79\n",
      "Next Action: [ 0.202\n",
      "Step reward: -15.903517820865812, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1267.4613741868607\n",
      "Episode: 10, Step: 80\n",
      "Next Action: [ 3.148\n",
      "Step reward: -15.913982697810294, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1283.375356884671\n",
      "Episode: 10, Step: 81\n",
      "Next Action: [ 0.387\n",
      "Step reward: -15.905340485938606, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1299.2806973706095\n",
      "Episode: 10, Step: 82\n",
      "Next Action: [ 4.997\n",
      "Step reward: -15.926920221494708, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1315.2076175921043\n",
      "Episode: 10, Step: 83\n",
      "Next Action: [ 0.513\n",
      "Step reward: -15.957404439535871, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1331.16502203164\n",
      "Episode: 10, Step: 84\n",
      "Next Action: [ 0.504\n",
      "Step reward: -15.96356983062071, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1347.1285918622607\n",
      "Episode: 10, Step: 85\n",
      "Next Action: [ 0.818\n",
      "Step reward: -15.956966978770526, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1363.0855588410311\n",
      "Episode: 10, Step: 86\n",
      "Next Action: [ 0.433\n",
      "Step reward: -15.944261966786858, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1379.029820807818\n",
      "Episode: 10, Step: 87\n",
      "Next Action: [ 0.247\n",
      "Step reward: -15.954598118523101, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1394.984418926341\n",
      "Episode: 10, Step: 88\n",
      "Next Action: [ 0.170\n",
      "Step reward: -15.946903366233192, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1410.9313222925744\n",
      "Episode: 10, Step: 89\n",
      "Next Action: [ 0.042\n",
      "Step reward: -15.929172373302423, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1426.8604946658768\n",
      "Episode: 10, Step: 90\n",
      "Next Action: [-0.085\n",
      "Step reward: -15.915034376627997, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1442.7755290425048\n",
      "Episode: 10, Step: 91\n",
      "Next Action: [-0.428\n",
      "Step reward: -15.868774251799481, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1458.6443032943043\n",
      "Episode: 10, Step: 92\n",
      "Next Action: [-0.307\n",
      "Step reward: -15.826840451964788, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1474.471143746269\n",
      "Episode: 10, Step: 93\n",
      "Next Action: [-0.371\n",
      "Step reward: -15.823542189536244, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1490.2946859358053\n",
      "Episode: 10, Step: 94\n",
      "Next Action: [-0.422\n",
      "Step reward: -15.855205160400505, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1506.149891096206\n",
      "Episode: 10, Step: 95\n",
      "Next Action: [-0.084\n",
      "Step reward: -15.834234560277372, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1521.9841256564832\n",
      "Episode: 10, Step: 96\n",
      "Next Action: [-2.316\n",
      "Step reward: -15.846713968980655, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1537.8308396254638\n",
      "Episode: 10, Step: 97\n",
      "Next Action: [-0.018\n",
      "Step reward: -15.860542222581046, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1553.6913818480448\n",
      "Episode: 10, Step: 98\n",
      "Next Action: [-0.202\n",
      "Step reward: -15.915861476907482, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1569.6072433249524\n",
      "Episode: 10, Step: 99\n",
      "Next Action: [-0.636\n",
      "Step reward: -15.927356141024138, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1585.5345994659765\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 16.035175323486328\n",
      "Actor loss: 41.9759635925293\n",
      "Critic loss: 107.11264038085938\n",
      "Actor loss: 21.208911895751953\n",
      "Critic loss: 3.4472711086273193\n",
      "Actor loss: 43.71200942993164\n",
      "Critic loss: 38.34407424926758\n",
      "Actor loss: 45.748069763183594\n",
      "Critic loss: 5.374762535095215\n",
      "Actor loss: 44.13669967651367\n",
      "Critic loss: 52.269866943359375\n",
      "Actor loss: 54.896339416503906\n",
      "Critic loss: 5.132572650909424\n",
      "Actor loss: 34.25591278076172\n",
      "Critic loss: 84.10345458984375\n",
      "Actor loss: 21.256879806518555\n",
      "Critic loss: 86.96610260009766\n",
      "Actor loss: 20.070688247680664\n",
      "Critic loss: 8.652504920959473\n",
      "Actor loss: 41.28942108154297\n",
      "Episode: 11\n",
      "Episode: 11, Step: 0\n",
      "Next Action: [-0.800\n",
      "Step reward: -11.512287804336614, Next State: [-1.\n",
      "Total episode reward: -11.512287804336614\n",
      "Episode: 11, Step: 1\n",
      "Next Action: [-1.056\n",
      "Step reward: -14.579132007624159, Next State: [-1.\n",
      "Total episode reward: -26.091419811960773\n",
      "Episode: 11, Step: 2\n",
      "Next Action: [-1.275\n",
      "Step reward: -15.466686266509354, Next State: [-1.\n",
      "Total episode reward: -41.558106078470125\n",
      "Episode: 11, Step: 3\n",
      "Next Action: [-1.568\n",
      "Step reward: -15.706552590103767, Next State: [-1.\n",
      "Total episode reward: -57.264658668573894\n",
      "Episode: 11, Step: 4\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.805380480681967, Next State: [-1.\n",
      "Total episode reward: -73.07003914925586\n",
      "Episode: 11, Step: 5\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.80720224725885, Next State: [-1. \n",
      "Total episode reward: -88.8772413965147\n",
      "Episode: 11, Step: 6\n",
      "Next Action: [-0.853\n",
      "Step reward: -15.847418669843556, Next State: [-1.\n",
      "Total episode reward: -104.72466006635827\n",
      "Episode: 11, Step: 7\n",
      "Next Action: [-1.116\n",
      "Step reward: -15.877684816920514, Next State: [-1.\n",
      "Total episode reward: -120.60234488327879\n",
      "Episode: 11, Step: 8\n",
      "Next Action: [-0.967\n",
      "Step reward: -15.851389950167714, Next State: [-1.\n",
      "Total episode reward: -136.4537348334465\n",
      "Episode: 11, Step: 9\n",
      "Next Action: [-0.866\n",
      "Step reward: -15.855014275369799, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.3087491088163\n",
      "Episode: 11, Step: 10\n",
      "Next Action: [-0.772\n",
      "Step reward: -15.855410302159598, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.1641594109759\n",
      "Episode: 11, Step: 11\n",
      "Next Action: [-0.418\n",
      "Step reward: -15.88495083582501, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.04911024680092\n",
      "Episode: 11, Step: 12\n",
      "Next Action: [-0.217\n",
      "Step reward: -15.86930595955112, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.91841620635205\n",
      "Episode: 11, Step: 13\n",
      "Next Action: [-0.519\n",
      "Step reward: -15.879036117978847, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -215.7974523243309\n",
      "Episode: 11, Step: 14\n",
      "Next Action: [-0.047\n",
      "Step reward: -15.858632836528832, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.65608516085973\n",
      "Episode: 11, Step: 15\n",
      "Next Action: [-0.414\n",
      "Step reward: -15.854419266023282, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.51050442688302\n",
      "Episode: 11, Step: 16\n",
      "Next Action: [-0.525\n",
      "Step reward: -15.86880043088924, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.37930485777224\n",
      "Episode: 11, Step: 17\n",
      "Next Action: [-0.183\n",
      "Step reward: -15.854962031717278, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.2342668894895\n",
      "Episode: 11, Step: 18\n",
      "Next Action: [-0.322\n",
      "Step reward: -15.864915015142888, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -295.0991819046324\n",
      "Episode: 11, Step: 19\n",
      "Next Action: [-0.350\n",
      "Step reward: -15.885214473448134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.98439637808053\n",
      "Episode: 11, Step: 20\n",
      "Next Action: [-0.481\n",
      "Step reward: -15.85176015094303, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.8361565290236\n",
      "Episode: 11, Step: 21\n",
      "Next Action: [-0.469\n",
      "Step reward: -15.835049416462345, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -342.67120594548595\n",
      "Episode: 11, Step: 22\n",
      "Next Action: [-0.645\n",
      "Step reward: -15.841098471358961, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.5123044168449\n",
      "Episode: 11, Step: 23\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.824287870673555, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.3365922875185\n",
      "Episode: 11, Step: 24\n",
      "Next Action: [-0.861\n",
      "Step reward: -15.781785522838351, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.11837781035683\n",
      "Episode: 11, Step: 25\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.72653801338747, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.8449158237443\n",
      "Episode: 11, Step: 26\n",
      "Next Action: [-0.710\n",
      "Step reward: -15.706258534923869, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.55117435866816\n",
      "Episode: 11, Step: 27\n",
      "Next Action: [-0.769\n",
      "Step reward: -15.739920635336338, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.2910949940045\n",
      "Episode: 11, Step: 28\n",
      "Next Action: [-0.742\n",
      "Step reward: -15.783194619040502, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.074289613045\n",
      "Episode: 11, Step: 29\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.79211787016906, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -468.86640748321406\n",
      "Episode: 11, Step: 30\n",
      "Next Action: [-9.185\n",
      "Step reward: -15.818924508880167, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -484.68533199209423\n",
      "Episode: 11, Step: 31\n",
      "Next Action: [-8.683\n",
      "Step reward: -15.815961338180141, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -500.5012933302744\n",
      "Episode: 11, Step: 32\n",
      "Next Action: [-0.711\n",
      "Step reward: -15.788147491432298, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.2894408217066\n",
      "Episode: 11, Step: 33\n",
      "Next Action: [-8.967\n",
      "Step reward: -15.756628213036468, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.0460690347431\n",
      "Episode: 11, Step: 34\n",
      "Next Action: [-1.223\n",
      "Step reward: -15.773741651716668, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -547.8198106864598\n",
      "Episode: 11, Step: 35\n",
      "Next Action: [-1.371\n",
      "Step reward: -15.83487193431924, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -563.6546826207791\n",
      "Episode: 11, Step: 36\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.851671155064691, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -579.5063537758438\n",
      "Episode: 11, Step: 37\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.871951289828331, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -595.3783050656721\n",
      "Episode: 11, Step: 38\n",
      "Next Action: [-1.202\n",
      "Step reward: -15.842679370866774, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -611.2209844365389\n",
      "Episode: 11, Step: 39\n",
      "Next Action: [-1.503\n",
      "Step reward: -15.82854371448354, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -627.0495281510224\n",
      "Episode: 11, Step: 40\n",
      "Next Action: [-0.882\n",
      "Step reward: -15.822922828978426, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -642.8724509800008\n",
      "Episode: 11, Step: 41\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.810663504708955, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -658.6831144847098\n",
      "Episode: 11, Step: 42\n",
      "Next Action: [-0.609\n",
      "Step reward: -15.849330189160854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -674.5324446738706\n",
      "Episode: 11, Step: 43\n",
      "Next Action: [-0.847\n",
      "Step reward: -15.889191370917695, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.4216360447883\n",
      "Episode: 11, Step: 44\n",
      "Next Action: [-7.786\n",
      "Step reward: -15.902798710248755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.324434755037\n",
      "Episode: 11, Step: 45\n",
      "Next Action: [-0.610\n",
      "Step reward: -15.919884457410134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -722.2443192124472\n",
      "Episode: 11, Step: 46\n",
      "Next Action: [-0.565\n",
      "Step reward: -15.928093331587222, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -738.1724125440344\n",
      "Episode: 11, Step: 47\n",
      "Next Action: [-0.494\n",
      "Step reward: -15.931107728728492, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -754.1035202727629\n",
      "Episode: 11, Step: 48\n",
      "Next Action: [-0.752\n",
      "Step reward: -15.913612191952607, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.0171324647155\n",
      "Episode: 11, Step: 49\n",
      "Next Action: [-0.981\n",
      "Step reward: -15.900685551753897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -785.9178180164694\n",
      "Episode: 11, Step: 50\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.904035189824548, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -801.8218532062939\n",
      "Episode: 11, Step: 51\n",
      "Next Action: [-0.704\n",
      "Step reward: -15.93702000240254, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.7588732086964\n",
      "Episode: 11, Step: 52\n",
      "Next Action: [-0.515\n",
      "Step reward: -15.91894599162429, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -833.6778192003206\n",
      "Episode: 11, Step: 53\n",
      "Next Action: [-0.465\n",
      "Step reward: -15.904482949936625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -849.5823021502573\n",
      "Episode: 11, Step: 54\n",
      "Next Action: [-0.733\n",
      "Step reward: -15.891331990930832, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -865.4736341411881\n",
      "Episode: 11, Step: 55\n",
      "Next Action: [-0.577\n",
      "Step reward: -15.849904831326425, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -881.3235389725145\n",
      "Episode: 11, Step: 56\n",
      "Next Action: [-0.509\n",
      "Step reward: -15.857621254608086, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -897.1811602271225\n",
      "Episode: 11, Step: 57\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.859428043039802, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -913.0405882701623\n",
      "Episode: 11, Step: 58\n",
      "Next Action: [-0.532\n",
      "Step reward: -15.876950079548244, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -928.9175383497105\n",
      "Episode: 11, Step: 59\n",
      "Next Action: [-0.230\n",
      "Step reward: -15.874922855060161, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -944.7924612047707\n",
      "Episode: 11, Step: 60\n",
      "Next Action: [-0.412\n",
      "Step reward: -15.893597723517587, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -960.6860589282883\n",
      "Episode: 11, Step: 61\n",
      "Next Action: [-6.737\n",
      "Step reward: -15.896578974793563, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -976.5826379030818\n",
      "Episode: 11, Step: 62\n",
      "Next Action: [-1.018\n",
      "Step reward: -15.89212578463409, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -992.474763687716\n",
      "Episode: 11, Step: 63\n",
      "Next Action: [-1.313\n",
      "Step reward: -15.84738349731668, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1008.3221471850326\n",
      "Episode: 11, Step: 64\n",
      "Next Action: [-1.006\n",
      "Step reward: -15.830836493842735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1024.1529836788752\n",
      "Episode: 11, Step: 65\n",
      "Next Action: [-1.243\n",
      "Step reward: -15.836857204628041, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1039.9898408835033\n",
      "Episode: 11, Step: 66\n",
      "Next Action: [-1.469\n",
      "Step reward: -15.799131281139069, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1055.7889721646425\n",
      "Episode: 11, Step: 67\n",
      "Next Action: [-1.476\n",
      "Step reward: -15.796241972699747, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1071.5852141373423\n",
      "Episode: 11, Step: 68\n",
      "Next Action: [-1.197\n",
      "Step reward: -15.807474942980036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1087.3926890803223\n",
      "Episode: 11, Step: 69\n",
      "Next Action: [-1.203\n",
      "Step reward: -15.82717908390422, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1103.2198681642265\n",
      "Episode: 11, Step: 70\n",
      "Next Action: [-1.486\n",
      "Step reward: -15.813107224898557, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1119.032975389125\n",
      "Episode: 11, Step: 71\n",
      "Next Action: [-1.470\n",
      "Step reward: -15.78506304714808, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1134.8180384362731\n",
      "Episode: 11, Step: 72\n",
      "Next Action: [-1.516\n",
      "Step reward: -15.76572131718776, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1150.583759753461\n",
      "Episode: 11, Step: 73\n",
      "Next Action: [-1.518\n",
      "Step reward: -15.762050948136409, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1166.3458107015974\n",
      "Episode: 11, Step: 74\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.812698926884414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1182.1585096284819\n",
      "Episode: 11, Step: 75\n",
      "Next Action: [-1.418\n",
      "Step reward: -15.81994089089042, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1197.9784505193722\n",
      "Episode: 11, Step: 76\n",
      "Next Action: [-1.056\n",
      "Step reward: -15.840936269618417, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1213.8193867889906\n",
      "Episode: 11, Step: 77\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.788573465062985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1229.6079602540535\n",
      "Episode: 11, Step: 78\n",
      "Next Action: [-0.895\n",
      "Step reward: -15.812620703684471, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1245.420580957738\n",
      "Episode: 11, Step: 79\n",
      "Next Action: [-1.115\n",
      "Step reward: -15.8154145046105, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1261.2359954623485\n",
      "Episode: 11, Step: 80\n",
      "Next Action: [-0.868\n",
      "Step reward: -15.833284571329921, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1277.0692800336783\n",
      "Episode: 11, Step: 81\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.846087982533358, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1292.9153680162117\n",
      "Episode: 11, Step: 82\n",
      "Next Action: [-0.702\n",
      "Step reward: -15.882386703444707, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1308.7977547196565\n",
      "Episode: 11, Step: 83\n",
      "Next Action: [-0.534\n",
      "Step reward: -15.928039309960665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1324.7257940296172\n",
      "Episode: 11, Step: 84\n",
      "Next Action: [-0.468\n",
      "Step reward: -15.921233933012969, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1340.64702796263\n",
      "Episode: 11, Step: 85\n",
      "Next Action: [-0.559\n",
      "Step reward: -15.896054881325988, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1356.543082843956\n",
      "Episode: 11, Step: 86\n",
      "Next Action: [-0.714\n",
      "Step reward: -15.896937677442033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1372.440020521398\n",
      "Episode: 11, Step: 87\n",
      "Next Action: [-9.549\n",
      "Step reward: -15.91838255267079, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1388.358403074069\n",
      "Episode: 11, Step: 88\n",
      "Next Action: [-0.918\n",
      "Step reward: -15.937233844424913, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1404.295636918494\n",
      "Episode: 11, Step: 89\n",
      "Next Action: [-1.076\n",
      "Step reward: -15.929377875220009, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1420.225014793714\n",
      "Episode: 11, Step: 90\n",
      "Next Action: [-1.119\n",
      "Step reward: -15.915699483882628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1436.1407142775968\n",
      "Episode: 11, Step: 91\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.914223529496859, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1452.0549378070937\n",
      "Episode: 11, Step: 92\n",
      "Next Action: [-6.901\n",
      "Step reward: -15.888786279748954, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1467.9437240868426\n",
      "Episode: 11, Step: 93\n",
      "Next Action: [-0.544\n",
      "Step reward: -15.907942171218853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1483.8516662580614\n",
      "Episode: 11, Step: 94\n",
      "Next Action: [-0.431\n",
      "Step reward: -15.92950576381499, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1499.7811720218765\n",
      "Episode: 11, Step: 95\n",
      "Next Action: [-0.340\n",
      "Step reward: -15.926084902549126, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1515.7072569244256\n",
      "Episode: 11, Step: 96\n",
      "Next Action: [-0.604\n",
      "Step reward: -15.942707361777245, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1531.6499642862027\n",
      "Episode: 11, Step: 97\n",
      "Next Action: [-0.817\n",
      "Step reward: -15.904226423667701, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1547.5541907098705\n",
      "Episode: 11, Step: 98\n",
      "Next Action: [-0.591\n",
      "Step reward: -15.880708083936698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1563.4348987938072\n",
      "Episode: 11, Step: 99\n",
      "Next Action: [-6.655\n",
      "Step reward: -15.890033267419955, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1579.3249320612272\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 47.25831604003906\n",
      "Actor loss: 29.0347957611084\n",
      "Critic loss: 26.704076766967773\n",
      "Actor loss: 35.234107971191406\n",
      "Critic loss: 9.95235824584961\n",
      "Actor loss: 37.41379165649414\n",
      "Critic loss: 6.405934810638428\n",
      "Actor loss: 34.155555725097656\n",
      "Critic loss: 9.075231552124023\n",
      "Actor loss: 32.04684066772461\n",
      "Critic loss: 54.135982513427734\n",
      "Actor loss: 60.36415481567383\n",
      "Critic loss: 20.57939338684082\n",
      "Actor loss: 36.974098205566406\n",
      "Critic loss: 21.206235885620117\n",
      "Actor loss: 60.463134765625\n",
      "Critic loss: 12.02235221862793\n",
      "Actor loss: 55.88031768798828\n",
      "Critic loss: 8.209168434143066\n",
      "Actor loss: 51.496116638183594\n",
      "Episode: 12\n",
      "Episode: 12, Step: 0\n",
      "Next Action: [-0.512\n",
      "Step reward: -11.631022636422628, Next State: [-1.\n",
      "Total episode reward: -11.631022636422628\n",
      "Episode: 12, Step: 1\n",
      "Next Action: [-0.584\n",
      "Step reward: -14.394986331519046, Next State: [-1.\n",
      "Total episode reward: -26.026008967941674\n",
      "Episode: 12, Step: 2\n",
      "Next Action: [-0.791\n",
      "Step reward: -15.38107191663869, Next State: [-1. \n",
      "Total episode reward: -41.40708088458037\n",
      "Episode: 12, Step: 3\n",
      "Next Action: [-0.748\n",
      "Step reward: -15.65188716241252, Next State: [-1. \n",
      "Total episode reward: -57.05896804699289\n",
      "Episode: 12, Step: 4\n",
      "Next Action: [-9.620\n",
      "Step reward: -15.682388945444904, Next State: [-1.\n",
      "Total episode reward: -72.74135699243779\n",
      "Episode: 12, Step: 5\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.736844300438763, Next State: [-1.\n",
      "Total episode reward: -88.47820129287655\n",
      "Episode: 12, Step: 6\n",
      "Next Action: [-1.310\n",
      "Step reward: -15.761466020616847, Next State: [-1.\n",
      "Total episode reward: -104.2396673134934\n",
      "Episode: 12, Step: 7\n",
      "Next Action: [-1.199\n",
      "Step reward: -15.836289721232435, Next State: [-1.\n",
      "Total episode reward: -120.07595703472583\n",
      "Episode: 12, Step: 8\n",
      "Next Action: [-1.219\n",
      "Step reward: -15.876791598645417, Next State: [-1.\n",
      "Total episode reward: -135.95274863337124\n",
      "Episode: 12, Step: 9\n",
      "Next Action: [-1.230\n",
      "Step reward: -15.855444386683354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.8081930200546\n",
      "Episode: 12, Step: 10\n",
      "Next Action: [-1.333\n",
      "Step reward: -15.858881785571166, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.66707480562576\n",
      "Episode: 12, Step: 11\n",
      "Next Action: [-1.334\n",
      "Step reward: -15.843643870854255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.51071867648002\n",
      "Episode: 12, Step: 12\n",
      "Next Action: [-1.197\n",
      "Step reward: -15.839923804011939, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.35064248049196\n",
      "Episode: 12, Step: 13\n",
      "Next Action: [-1.339\n",
      "Step reward: -15.825930761668952, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -215.1765732421609\n",
      "Episode: 12, Step: 14\n",
      "Next Action: [-1.370\n",
      "Step reward: -15.835049749854946, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.01162299201587\n",
      "Episode: 12, Step: 15\n",
      "Next Action: [-1.316\n",
      "Step reward: -15.88630660284709, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.89792959486297\n",
      "Episode: 12, Step: 16\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.894361919641293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.79229151450426\n",
      "Episode: 12, Step: 17\n",
      "Next Action: [-1.093\n",
      "Step reward: -15.89553955752013, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.6878310720244\n",
      "Episode: 12, Step: 18\n",
      "Next Action: [-1.181\n",
      "Step reward: -15.870111501797643, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.557942573822\n",
      "Episode: 12, Step: 19\n",
      "Next Action: [-1.552\n",
      "Step reward: -15.87020132246529, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.4281438962873\n",
      "Episode: 12, Step: 20\n",
      "Next Action: [-1.447\n",
      "Step reward: -15.902473690766666, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.33061758705395\n",
      "Episode: 12, Step: 21\n",
      "Next Action: [-1.680\n",
      "Step reward: -15.905374720285232, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -342.2359923073392\n",
      "Episode: 12, Step: 22\n",
      "Next Action: [-1.491\n",
      "Step reward: -15.90847168460071, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.1444639919399\n",
      "Episode: 12, Step: 23\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.861835437619867, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.00629942955976\n",
      "Episode: 12, Step: 24\n",
      "Next Action: [-1.317\n",
      "Step reward: -15.852361788706192, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.858661218266\n",
      "Episode: 12, Step: 25\n",
      "Next Action: [-1.230\n",
      "Step reward: -15.868482807290345, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.7271440255563\n",
      "Episode: 12, Step: 26\n",
      "Next Action: [-1.328\n",
      "Step reward: -15.887402342915289, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.61454636847157\n",
      "Episode: 12, Step: 27\n",
      "Next Action: [-1.417\n",
      "Step reward: -15.881774937983748, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.4963213064553\n",
      "Episode: 12, Step: 28\n",
      "Next Action: [-1.466\n",
      "Step reward: -15.895735308758976, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.39205661521424\n",
      "Episode: 12, Step: 29\n",
      "Next Action: [-1.521\n",
      "Step reward: -15.88140193358402, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -469.27345854879826\n",
      "Episode: 12, Step: 30\n",
      "Next Action: [-1.794\n",
      "Step reward: -15.863817834107543, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -485.1372763829058\n",
      "Episode: 12, Step: 31\n",
      "Next Action: [-1.962\n",
      "Step reward: -15.857854544950765, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -500.99513092785656\n",
      "Episode: 12, Step: 32\n",
      "Next Action: [-1.451\n",
      "Step reward: -15.83306439867378, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.8281953265304\n",
      "Episode: 12, Step: 33\n",
      "Next Action: [-1.140\n",
      "Step reward: -15.81182329433344, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.6400186208638\n",
      "Episode: 12, Step: 34\n",
      "Next Action: [-1.048\n",
      "Step reward: -15.800318398852328, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.4403370197161\n",
      "Episode: 12, Step: 35\n",
      "Next Action: [-1.539\n",
      "Step reward: -15.782248722239489, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.2225857419556\n",
      "Episode: 12, Step: 36\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.784182174999243, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.0067679169549\n",
      "Episode: 12, Step: 37\n",
      "Next Action: [-1.396\n",
      "Step reward: -15.842847304212555, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -595.8496152211675\n",
      "Episode: 12, Step: 38\n",
      "Next Action: [-1.236\n",
      "Step reward: -15.847751651154, Next State: [-1.   \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -611.6973668723215\n",
      "Episode: 12, Step: 39\n",
      "Next Action: [-0.945\n",
      "Step reward: -15.85317618864745, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -627.550543060969\n",
      "Episode: 12, Step: 40\n",
      "Next Action: [-1.074\n",
      "Step reward: -15.833416326173655, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -643.3839593871426\n",
      "Episode: 12, Step: 41\n",
      "Next Action: [-1.293\n",
      "Step reward: -15.809152549437476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -659.1931119365801\n",
      "Episode: 12, Step: 42\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.831152608806423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -675.0242645453865\n",
      "Episode: 12, Step: 43\n",
      "Next Action: [-1.134\n",
      "Step reward: -15.847943191845005, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.8722077372315\n",
      "Episode: 12, Step: 44\n",
      "Next Action: [-1.451\n",
      "Step reward: -15.859714248975026, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.7319219862065\n",
      "Episode: 12, Step: 45\n",
      "Next Action: [-1.526\n",
      "Step reward: -15.83550570505323, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -722.5674276912598\n",
      "Episode: 12, Step: 46\n",
      "Next Action: [-1.535\n",
      "Step reward: -15.849773245099204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -738.417200936359\n",
      "Episode: 12, Step: 47\n",
      "Next Action: [-1.595\n",
      "Step reward: -15.840436286534667, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -754.2576372228937\n",
      "Episode: 12, Step: 48\n",
      "Next Action: [-1.676\n",
      "Step reward: -15.847454888237957, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.1050921111316\n",
      "Episode: 12, Step: 49\n",
      "Next Action: [-1.727\n",
      "Step reward: -15.861431739825795, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -785.9665238509574\n",
      "Episode: 12, Step: 50\n",
      "Next Action: [-1.799\n",
      "Step reward: -15.889649179858745, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -801.8561730308162\n",
      "Episode: 12, Step: 51\n",
      "Next Action: [-1.441\n",
      "Step reward: -15.931098120725727, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.787271151542\n",
      "Episode: 12, Step: 52\n",
      "Next Action: [-1.445\n",
      "Step reward: -15.913737634321036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -833.701008785863\n",
      "Episode: 12, Step: 53\n",
      "Next Action: [-1.466\n",
      "Step reward: -15.88277186041271, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -849.5837806462757\n",
      "Episode: 12, Step: 54\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.917922524183945, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -865.5017031704597\n",
      "Episode: 12, Step: 55\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.946826977036054, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -881.4485301474957\n",
      "Episode: 12, Step: 56\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.93839437002198, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -897.3869245175177\n",
      "Episode: 12, Step: 57\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.959254081770357, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -913.3461785992881\n",
      "Episode: 12, Step: 58\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.939639017440348, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -929.2858176167284\n",
      "Episode: 12, Step: 59\n",
      "Next Action: [-1.120\n",
      "Step reward: -15.914848969278523, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -945.200666586007\n",
      "Episode: 12, Step: 60\n",
      "Next Action: [-1.235\n",
      "Step reward: -15.918254711909935, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.1189212979169\n",
      "Episode: 12, Step: 61\n",
      "Next Action: [-0.950\n",
      "Step reward: -15.963321162378314, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.0822424602952\n",
      "Episode: 12, Step: 62\n",
      "Next Action: [-0.720\n",
      "Step reward: -15.962833154166665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.0450756144619\n",
      "Episode: 12, Step: 63\n",
      "Next Action: [-0.808\n",
      "Step reward: -15.940696861669533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1008.9857724761314\n",
      "Episode: 12, Step: 64\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.936279842981993, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1024.9220523191134\n",
      "Episode: 12, Step: 65\n",
      "Next Action: [-0.874\n",
      "Step reward: -15.884974291265427, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1040.807026610379\n",
      "Episode: 12, Step: 66\n",
      "Next Action: [-0.619\n",
      "Step reward: -15.86202859275882, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1056.6690552031378\n",
      "Episode: 12, Step: 67\n",
      "Next Action: [-0.673\n",
      "Step reward: -15.84988566732025, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1072.518940870458\n",
      "Episode: 12, Step: 68\n",
      "Next Action: [-1.153\n",
      "Step reward: -15.833093057778086, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1088.352033928236\n",
      "Episode: 12, Step: 69\n",
      "Next Action: [-0.933\n",
      "Step reward: -15.82162199127817, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1104.1736559195142\n",
      "Episode: 12, Step: 70\n",
      "Next Action: [-1.188\n",
      "Step reward: -15.842303684827156, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1120.0159596043413\n",
      "Episode: 12, Step: 71\n",
      "Next Action: [-9.416\n",
      "Step reward: -15.86038662375228, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1135.8763462280936\n",
      "Episode: 12, Step: 72\n",
      "Next Action: [-1.044\n",
      "Step reward: -15.871991271121177, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1151.7483374992148\n",
      "Episode: 12, Step: 73\n",
      "Next Action: [-1.008\n",
      "Step reward: -15.878867602013036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1167.627205101228\n",
      "Episode: 12, Step: 74\n",
      "Next Action: [-8.597\n",
      "Step reward: -15.945544736864964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1183.572749838093\n",
      "Episode: 12, Step: 75\n",
      "Next Action: [-0.817\n",
      "Step reward: -15.964645087600967, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1199.537394925694\n",
      "Episode: 12, Step: 76\n",
      "Next Action: [-0.700\n",
      "Step reward: -15.965115796393517, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1215.5025107220874\n",
      "Episode: 12, Step: 77\n",
      "Next Action: [-1.052\n",
      "Step reward: -15.965823118179404, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1231.468333840267\n",
      "Episode: 12, Step: 78\n",
      "Next Action: [-1.073\n",
      "Step reward: -15.969004556458644, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1247.4373383967256\n",
      "Episode: 12, Step: 79\n",
      "Next Action: [-6.825\n",
      "Step reward: -15.946923830233604, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1263.3842622269592\n",
      "Episode: 12, Step: 80\n",
      "Next Action: [-0.573\n",
      "Step reward: -15.89610079445749, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1279.2803630214166\n",
      "Episode: 12, Step: 81\n",
      "Next Action: [-0.616\n",
      "Step reward: -15.895589723141818, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1295.1759527445583\n",
      "Episode: 12, Step: 82\n",
      "Next Action: [-0.808\n",
      "Step reward: -15.903741093335459, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1311.0796938378937\n",
      "Episode: 12, Step: 83\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.891469111187325, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1326.971162949081\n",
      "Episode: 12, Step: 84\n",
      "Next Action: [-1.342\n",
      "Step reward: -15.868499605167743, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1342.8396625542487\n",
      "Episode: 12, Step: 85\n",
      "Next Action: [-1.100\n",
      "Step reward: -15.855077565343018, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1358.6947401195916\n",
      "Episode: 12, Step: 86\n",
      "Next Action: [-1.299\n",
      "Step reward: -15.833993851773936, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1374.5287339713657\n",
      "Episode: 12, Step: 87\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.794412655187328, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1390.323146626553\n",
      "Episode: 12, Step: 88\n",
      "Next Action: [-1.457\n",
      "Step reward: -15.803936106817375, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1406.1270827333703\n",
      "Episode: 12, Step: 89\n",
      "Next Action: [-1.302\n",
      "Step reward: -15.841724413134141, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1421.9688071465046\n",
      "Episode: 12, Step: 90\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.836977307944414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1437.805784454449\n",
      "Episode: 12, Step: 91\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.830226195044174, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1453.636010649493\n",
      "Episode: 12, Step: 92\n",
      "Next Action: [-1.279\n",
      "Step reward: -15.865170342288742, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1469.501180991782\n",
      "Episode: 12, Step: 93\n",
      "Next Action: [-1.081\n",
      "Step reward: -15.912994142631819, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1485.4141751344137\n",
      "Episode: 12, Step: 94\n",
      "Next Action: [-1.036\n",
      "Step reward: -15.924259681249493, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1501.338434815663\n",
      "Episode: 12, Step: 95\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.919853186867597, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1517.2582880025307\n",
      "Episode: 12, Step: 96\n",
      "Next Action: [-1.262\n",
      "Step reward: -15.90978245588652, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1533.168070458417\n",
      "Episode: 12, Step: 97\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.894350767427587, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1549.0624212258447\n",
      "Episode: 12, Step: 98\n",
      "Next Action: [-1.298\n",
      "Step reward: -15.885727622296105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1564.9481488481408\n",
      "Episode: 12, Step: 99\n",
      "Next Action: [-1.331\n",
      "Step reward: -15.84860077693776, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1580.7967496250785\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 42.41485595703125\n",
      "Actor loss: 25.832489013671875\n",
      "Critic loss: 15.819348335266113\n",
      "Actor loss: 47.4340934753418\n",
      "Critic loss: 6.896350860595703\n",
      "Actor loss: 39.649681091308594\n",
      "Critic loss: 1.979465365409851\n",
      "Actor loss: 41.227108001708984\n",
      "Critic loss: 136.6610565185547\n",
      "Actor loss: 20.290325164794922\n",
      "Critic loss: 23.627763748168945\n",
      "Actor loss: 33.88248825073242\n",
      "Critic loss: 3.130383253097534\n",
      "Actor loss: 37.447608947753906\n",
      "Critic loss: 36.646453857421875\n",
      "Actor loss: 30.42862319946289\n",
      "Critic loss: 37.04204559326172\n",
      "Actor loss: 24.540510177612305\n",
      "Critic loss: 45.5974006652832\n",
      "Actor loss: 28.560352325439453\n",
      "Episode: 13\n",
      "Episode: 13, Step: 0\n",
      "Next Action: [-1.320\n",
      "Step reward: -11.63457552979435, Next State: [-0.0\n",
      "Total episode reward: -11.63457552979435\n",
      "Episode: 13, Step: 1\n",
      "Next Action: [-1.566\n",
      "Step reward: -14.619945278093839, Next State: [-1.\n",
      "Total episode reward: -26.25452080788819\n",
      "Episode: 13, Step: 2\n",
      "Next Action: [-1.360\n",
      "Step reward: -15.508860425236357, Next State: [-1.\n",
      "Total episode reward: -41.76338123312455\n",
      "Episode: 13, Step: 3\n",
      "Next Action: [-1.172\n",
      "Step reward: -15.666441955340737, Next State: [-1.\n",
      "Total episode reward: -57.429823188465285\n",
      "Episode: 13, Step: 4\n",
      "Next Action: [-1.527\n",
      "Step reward: -15.720252704656131, Next State: [-1.\n",
      "Total episode reward: -73.15007589312141\n",
      "Episode: 13, Step: 5\n",
      "Next Action: [-1.270\n",
      "Step reward: -15.77006182553667, Next State: [-1. \n",
      "Total episode reward: -88.92013771865808\n",
      "Episode: 13, Step: 6\n",
      "Next Action: [-1.147\n",
      "Step reward: -15.781978588211825, Next State: [-1.\n",
      "Total episode reward: -104.70211630686991\n",
      "Episode: 13, Step: 7\n",
      "Next Action: [-0.734\n",
      "Step reward: -15.812900273603182, Next State: [-1.\n",
      "Total episode reward: -120.51501658047309\n",
      "Episode: 13, Step: 8\n",
      "Next Action: [-0.468\n",
      "Step reward: -15.825010625308309, Next State: [-1.\n",
      "Total episode reward: -136.34002720578138\n",
      "Episode: 13, Step: 9\n",
      "Next Action: [-0.605\n",
      "Step reward: -15.867443236224172, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.20747044200556\n",
      "Episode: 13, Step: 10\n",
      "Next Action: [-0.684\n",
      "Step reward: -15.895437092492623, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.10290753449817\n",
      "Episode: 13, Step: 11\n",
      "Next Action: [-0.752\n",
      "Step reward: -15.915338546449394, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.01824608094756\n",
      "Episode: 13, Step: 12\n",
      "Next Action: [-0.677\n",
      "Step reward: -15.891427918692337, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.9096739996399\n",
      "Episode: 13, Step: 13\n",
      "Next Action: [-6.661\n",
      "Step reward: -15.864208932852058, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -215.77388293249194\n",
      "Episode: 13, Step: 14\n",
      "Next Action: [-0.690\n",
      "Step reward: -15.850307193269037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.62419012576098\n",
      "Episode: 13, Step: 15\n",
      "Next Action: [-0.593\n",
      "Step reward: -15.839139781200208, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.46332990696118\n",
      "Episode: 13, Step: 16\n",
      "Next Action: [-8.626\n",
      "Step reward: -15.832444628498664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.2957745354598\n",
      "Episode: 13, Step: 17\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.821254328979808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.11702886443965\n",
      "Episode: 13, Step: 18\n",
      "Next Action: [-1.008\n",
      "Step reward: -15.78438949883874, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.90141836327837\n",
      "Episode: 13, Step: 19\n",
      "Next Action: [-9.197\n",
      "Step reward: -15.804904996560172, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.70632335983856\n",
      "Episode: 13, Step: 20\n",
      "Next Action: [-0.720\n",
      "Step reward: -15.873357899304235, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.5796812591428\n",
      "Episode: 13, Step: 21\n",
      "Next Action: [-0.624\n",
      "Step reward: -15.907815117208255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -342.48749637635103\n",
      "Episode: 13, Step: 22\n",
      "Next Action: [-0.825\n",
      "Step reward: -15.870130724861344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.35762710121236\n",
      "Episode: 13, Step: 23\n",
      "Next Action: [-0.747\n",
      "Step reward: -15.842426852937363, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.2000539541497\n",
      "Episode: 13, Step: 24\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.838188150168511, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.0382421043182\n",
      "Episode: 13, Step: 25\n",
      "Next Action: [-0.909\n",
      "Step reward: -15.86221593780854, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.90045804212673\n",
      "Episode: 13, Step: 26\n",
      "Next Action: [-0.876\n",
      "Step reward: -15.862314917270083, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.7627729593968\n",
      "Episode: 13, Step: 27\n",
      "Next Action: [-0.755\n",
      "Step reward: -15.876204377610149, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.6389773370069\n",
      "Episode: 13, Step: 28\n",
      "Next Action: [-4.390\n",
      "Step reward: -15.887413713857187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.5263910508641\n",
      "Episode: 13, Step: 29\n",
      "Next Action: [-0.693\n",
      "Step reward: -15.874993274593418, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -469.4013843254575\n",
      "Episode: 13, Step: 30\n",
      "Next Action: [-0.552\n",
      "Step reward: -15.865524006566302, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -485.26690833202383\n",
      "Episode: 13, Step: 31\n",
      "Next Action: [-0.487\n",
      "Step reward: -15.886235587384734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -501.1531439194086\n",
      "Episode: 13, Step: 32\n",
      "Next Action: [-0.465\n",
      "Step reward: -15.8561444351062, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -517.0092883545148\n",
      "Episode: 13, Step: 33\n",
      "Next Action: [-3.899\n",
      "Step reward: -15.844510411296561, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.8537987658113\n",
      "Episode: 13, Step: 34\n",
      "Next Action: [-0.376\n",
      "Step reward: -15.826553008097669, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.680351773909\n",
      "Episode: 13, Step: 35\n",
      "Next Action: [-0.836\n",
      "Step reward: -15.853694363262054, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.5340461371711\n",
      "Episode: 13, Step: 36\n",
      "Next Action: [-0.698\n",
      "Step reward: -15.86542174599154, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.3994678831626\n",
      "Episode: 13, Step: 37\n",
      "Next Action: [-6.449\n",
      "Step reward: -15.881502136632436, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -596.2809700197951\n",
      "Episode: 13, Step: 38\n",
      "Next Action: [-0.934\n",
      "Step reward: -15.884997368516588, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -612.1659673883116\n",
      "Episode: 13, Step: 39\n",
      "Next Action: [-0.938\n",
      "Step reward: -15.867025199185559, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -628.0329925874972\n",
      "Episode: 13, Step: 40\n",
      "Next Action: [-0.831\n",
      "Step reward: -15.821608496692813, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -643.85460108419\n",
      "Episode: 13, Step: 41\n",
      "Next Action: [-0.594\n",
      "Step reward: -15.811526182810491, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -659.6661272670004\n",
      "Episode: 13, Step: 42\n",
      "Next Action: [-0.762\n",
      "Step reward: -15.86804355271511, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -675.5341708197155\n",
      "Episode: 13, Step: 43\n",
      "Next Action: [-0.953\n",
      "Step reward: -15.91199649986076, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -691.4461673195763\n",
      "Episode: 13, Step: 44\n",
      "Next Action: [-0.617\n",
      "Step reward: -15.90807687754893, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -707.3542441971252\n",
      "Episode: 13, Step: 45\n",
      "Next Action: [-0.544\n",
      "Step reward: -15.90914348015358, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -723.2633876772788\n",
      "Episode: 13, Step: 46\n",
      "Next Action: [-0.460\n",
      "Step reward: -15.894048967115149, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -739.157436644394\n",
      "Episode: 13, Step: 47\n",
      "Next Action: [-0.456\n",
      "Step reward: -15.88982449487977, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -755.0472611392738\n",
      "Episode: 13, Step: 48\n",
      "Next Action: [-0.343\n",
      "Step reward: -15.885567429481391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.9328285687552\n",
      "Episode: 13, Step: 49\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.896299757193098, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -786.8291283259483\n",
      "Episode: 13, Step: 50\n",
      "Next Action: [-0.557\n",
      "Step reward: -15.884564407108483, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -802.7136927330567\n",
      "Episode: 13, Step: 51\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.869607323510364, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -818.583300056567\n",
      "Episode: 13, Step: 52\n",
      "Next Action: [-0.892\n",
      "Step reward: -15.907461896433718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -834.4907619530007\n",
      "Episode: 13, Step: 53\n",
      "Next Action: [-1.345\n",
      "Step reward: -15.935410950137184, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -850.426172903138\n",
      "Episode: 13, Step: 54\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.924888740442995, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -866.3510616435809\n",
      "Episode: 13, Step: 55\n",
      "Next Action: [-1.072\n",
      "Step reward: -15.920616152473777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -882.2716777960547\n",
      "Episode: 13, Step: 56\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.914962293821482, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -898.1866400898763\n",
      "Episode: 13, Step: 57\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.87879846637419, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -914.0654385562505\n",
      "Episode: 13, Step: 58\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.86552423730969, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -929.9309627935602\n",
      "Episode: 13, Step: 59\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.881447071139547, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -945.8124098646997\n",
      "Episode: 13, Step: 60\n",
      "Next Action: [-1.322\n",
      "Step reward: -15.879867410287154, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.6922772749868\n",
      "Episode: 13, Step: 61\n",
      "Next Action: [-1.181\n",
      "Step reward: -15.915935560364614, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.6082128353514\n",
      "Episode: 13, Step: 62\n",
      "Next Action: [-1.424\n",
      "Step reward: -15.894587232738214, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.5028000680896\n",
      "Episode: 13, Step: 63\n",
      "Next Action: [-1.375\n",
      "Step reward: -15.89905604661918, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.4018561147088\n",
      "Episode: 13, Step: 64\n",
      "Next Action: [-1.250\n",
      "Step reward: -15.899008221444756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.3008643361536\n",
      "Episode: 13, Step: 65\n",
      "Next Action: [-1.355\n",
      "Step reward: -15.884808239510742, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1041.1856725756643\n",
      "Episode: 13, Step: 66\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.950468426923505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1057.1361410025877\n",
      "Episode: 13, Step: 67\n",
      "Next Action: [-1.164\n",
      "Step reward: -15.976510830817329, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1073.112651833405\n",
      "Episode: 13, Step: 68\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.960753302085092, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1089.07340513549\n",
      "Episode: 13, Step: 69\n",
      "Next Action: [-1.077\n",
      "Step reward: -15.949314924021381, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1105.0227200595114\n",
      "Episode: 13, Step: 70\n",
      "Next Action: [-1.090\n",
      "Step reward: -15.924341874850542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1120.947061934362\n",
      "Episode: 13, Step: 71\n",
      "Next Action: [-0.746\n",
      "Step reward: -15.902409538589207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1136.849471472951\n",
      "Episode: 13, Step: 72\n",
      "Next Action: [-0.997\n",
      "Step reward: -15.910130107276332, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1152.7596015802274\n",
      "Episode: 13, Step: 73\n",
      "Next Action: [-0.847\n",
      "Step reward: -15.930359147583541, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1168.6899607278108\n",
      "Episode: 13, Step: 74\n",
      "Next Action: [-0.383\n",
      "Step reward: -15.949373518908372, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1184.639334246719\n",
      "Episode: 13, Step: 75\n",
      "Next Action: [-0.312\n",
      "Step reward: -15.938937431697317, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1200.5782716784163\n",
      "Episode: 13, Step: 76\n",
      "Next Action: [-0.445\n",
      "Step reward: -15.940279656284387, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1216.5185513347008\n",
      "Episode: 13, Step: 77\n",
      "Next Action: [-0.737\n",
      "Step reward: -15.930731419174911, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1232.4492827538757\n",
      "Episode: 13, Step: 78\n",
      "Next Action: [-0.564\n",
      "Step reward: -15.931267051851327, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1248.380549805727\n",
      "Episode: 13, Step: 79\n",
      "Next Action: [-0.457\n",
      "Step reward: -15.932444455305605, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1264.3129942610326\n",
      "Episode: 13, Step: 80\n",
      "Next Action: [-0.391\n",
      "Step reward: -15.945723695052942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1280.2587179560855\n",
      "Episode: 13, Step: 81\n",
      "Next Action: [-0.405\n",
      "Step reward: -15.931022165262698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1296.1897401213482\n",
      "Episode: 13, Step: 82\n",
      "Next Action: [-0.528\n",
      "Step reward: -15.883177824658828, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1312.072917946007\n",
      "Episode: 13, Step: 83\n",
      "Next Action: [-0.211\n",
      "Step reward: -15.84579342519832, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1327.9187113712053\n",
      "Episode: 13, Step: 84\n",
      "Next Action: [-0.009\n",
      "Step reward: -15.857606744615634, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1343.776318115821\n",
      "Episode: 13, Step: 85\n",
      "Next Action: [-3.973\n",
      "Step reward: -15.878564710060779, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1359.6548828258817\n",
      "Episode: 13, Step: 86\n",
      "Next Action: [-0.643\n",
      "Step reward: -15.8646549534519, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1375.5195377793336\n",
      "Episode: 13, Step: 87\n",
      "Next Action: [-0.945\n",
      "Step reward: -15.877375720327022, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1391.3969134996607\n",
      "Episode: 13, Step: 88\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.905709973483848, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1407.3026234731446\n",
      "Episode: 13, Step: 89\n",
      "Next Action: [-0.886\n",
      "Step reward: -15.910922763654344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1423.213546236799\n",
      "Episode: 13, Step: 90\n",
      "Next Action: [-1.102\n",
      "Step reward: -15.893306992391283, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1439.1068532291902\n",
      "Episode: 13, Step: 91\n",
      "Next Action: [-0.995\n",
      "Step reward: -15.904224355763041, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1455.0110775849532\n",
      "Episode: 13, Step: 92\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.920146571965264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1470.9312241569185\n",
      "Episode: 13, Step: 93\n",
      "Next Action: [-1.444\n",
      "Step reward: -15.941491979315815, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1486.8727161362342\n",
      "Episode: 13, Step: 94\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.914115587378463, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1502.7868317236127\n",
      "Episode: 13, Step: 95\n",
      "Next Action: [-1.382\n",
      "Step reward: -15.901809127420991, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1518.6886408510338\n",
      "Episode: 13, Step: 96\n",
      "Next Action: [-1.148\n",
      "Step reward: -15.94150814415324, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1534.630148995187\n",
      "Episode: 13, Step: 97\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.968245342695639, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1550.5983943378826\n",
      "Episode: 13, Step: 98\n",
      "Next Action: [-0.785\n",
      "Step reward: -15.935266491512571, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1566.533660829395\n",
      "Episode: 13, Step: 99\n",
      "Next Action: [-0.902\n",
      "Step reward: -15.917433948970734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1582.4510947783658\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 2.9763896465301514\n",
      "Actor loss: 34.52184295654297\n",
      "Critic loss: 21.209083557128906\n",
      "Actor loss: 36.144466400146484\n",
      "Critic loss: 4.676401615142822\n",
      "Actor loss: 36.533729553222656\n",
      "Critic loss: 1.5724241733551025\n",
      "Actor loss: 51.25257110595703\n",
      "Critic loss: 16.37744903564453\n",
      "Actor loss: 55.466346740722656\n",
      "Critic loss: 1.5217607021331787\n",
      "Actor loss: 47.733055114746094\n",
      "Critic loss: 2.9436776638031006\n",
      "Actor loss: 51.13312530517578\n",
      "Critic loss: 19.369171142578125\n",
      "Actor loss: 58.4865608215332\n",
      "Critic loss: 2.4533841609954834\n",
      "Actor loss: 46.956634521484375\n",
      "Critic loss: 5.366441249847412\n",
      "Actor loss: 54.358985900878906\n",
      "Episode: 14\n",
      "Episode: 14, Step: 0\n",
      "Next Action: [-0.491\n",
      "Step reward: -11.46932669685756, Next State: [-0.3\n",
      "Total episode reward: -11.46932669685756\n",
      "Episode: 14, Step: 1\n",
      "Next Action: [-0.799\n",
      "Step reward: -14.153540079128291, Next State: [-1.\n",
      "Total episode reward: -25.62286677598585\n",
      "Episode: 14, Step: 2\n",
      "Next Action: [-0.736\n",
      "Step reward: -15.273824722219349, Next State: [-1.\n",
      "Total episode reward: -40.8966914982052\n",
      "Episode: 14, Step: 3\n",
      "Next Action: [-0.714\n",
      "Step reward: -15.610269759492228, Next State: [-1.\n",
      "Total episode reward: -56.50696125769743\n",
      "Episode: 14, Step: 4\n",
      "Next Action: [-7.319\n",
      "Step reward: -15.696564831966278, Next State: [-1.\n",
      "Total episode reward: -72.20352608966371\n",
      "Episode: 14, Step: 5\n",
      "Next Action: [-0.691\n",
      "Step reward: -15.817221260825482, Next State: [-1.\n",
      "Total episode reward: -88.02074735048919\n",
      "Episode: 14, Step: 6\n",
      "Next Action: [-0.757\n",
      "Step reward: -15.851849334856118, Next State: [-1.\n",
      "Total episode reward: -103.8725966853453\n",
      "Episode: 14, Step: 7\n",
      "Next Action: [-0.883\n",
      "Step reward: -15.879174143186873, Next State: [-1.\n",
      "Total episode reward: -119.75177082853217\n",
      "Episode: 14, Step: 8\n",
      "Next Action: [-0.651\n",
      "Step reward: -15.860079269881929, Next State: [-1.\n",
      "Total episode reward: -135.6118500984141\n",
      "Episode: 14, Step: 9\n",
      "Next Action: [-0.630\n",
      "Step reward: -15.84963099402076, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.46148109243487\n",
      "Episode: 14, Step: 10\n",
      "Next Action: [-0.701\n",
      "Step reward: -15.927735527139452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.3892166195743\n",
      "Episode: 14, Step: 11\n",
      "Next Action: [-0.637\n",
      "Step reward: -15.922330671104314, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.31154729067862\n",
      "Episode: 14, Step: 12\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.923768601229964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.23531589190858\n",
      "Episode: 14, Step: 13\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.910956400797115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -215.1462722927057\n",
      "Episode: 14, Step: 14\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.931070708074268, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -231.07734300077996\n",
      "Episode: 14, Step: 15\n",
      "Next Action: [-1.618\n",
      "Step reward: -15.96081576464393, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.0381587654239\n",
      "Episode: 14, Step: 16\n",
      "Next Action: [-1.591\n",
      "Step reward: -15.943232424684412, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.9813911901083\n",
      "Episode: 14, Step: 17\n",
      "Next Action: [-1.605\n",
      "Step reward: -15.937486071063026, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.9188772611713\n",
      "Episode: 14, Step: 18\n",
      "Next Action: [-1.530\n",
      "Step reward: -15.952414489836837, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.87129175100813\n",
      "Episode: 14, Step: 19\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.957408364042726, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.8287001150509\n",
      "Episode: 14, Step: 20\n",
      "Next Action: [-1.223\n",
      "Step reward: -15.93023553982627, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.7589356548772\n",
      "Episode: 14, Step: 21\n",
      "Next Action: [-1.165\n",
      "Step reward: -15.925865027430264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -342.68480068230747\n",
      "Episode: 14, Step: 22\n",
      "Next Action: [-1.087\n",
      "Step reward: -15.900548253629918, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.58534893593736\n",
      "Episode: 14, Step: 23\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.914350909481772, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.4996998454191\n",
      "Episode: 14, Step: 24\n",
      "Next Action: [-0.940\n",
      "Step reward: -15.92157660500569, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.4212764504248\n",
      "Episode: 14, Step: 25\n",
      "Next Action: [-1.144\n",
      "Step reward: -15.909610563163978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -406.3308870135888\n",
      "Episode: 14, Step: 26\n",
      "Next Action: [-1.009\n",
      "Step reward: -15.87463614126664, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -422.2055231548554\n",
      "Episode: 14, Step: 27\n",
      "Next Action: [-1.095\n",
      "Step reward: -15.857545146299518, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -438.06306830115494\n",
      "Episode: 14, Step: 28\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.891212274912158, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.9542805760671\n",
      "Episode: 14, Step: 29\n",
      "Next Action: [-1.326\n",
      "Step reward: -15.937071713502696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -469.8913522895698\n",
      "Episode: 14, Step: 30\n",
      "Next Action: [-1.318\n",
      "Step reward: -15.954649838132394, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -485.8460021277022\n",
      "Episode: 14, Step: 31\n",
      "Next Action: [-1.523\n",
      "Step reward: -15.975427067068473, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -501.8214291947707\n",
      "Episode: 14, Step: 32\n",
      "Next Action: [-1.397\n",
      "Step reward: -15.992231267043284, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -517.813660461814\n",
      "Episode: 14, Step: 33\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.963623542216872, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -533.7772840040309\n",
      "Episode: 14, Step: 34\n",
      "Next Action: [-1.255\n",
      "Step reward: -15.975510646122542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -549.7527946501534\n",
      "Episode: 14, Step: 35\n",
      "Next Action: [-1.193\n",
      "Step reward: -15.984777994667573, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -565.737572644821\n",
      "Episode: 14, Step: 36\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.96594962494759, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -581.7035222697685\n",
      "Episode: 14, Step: 37\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.947916816676495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -597.6514390864451\n",
      "Episode: 14, Step: 38\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.97943175617493, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -613.63087084262\n",
      "Episode: 14, Step: 39\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.994577531485223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -629.6254483741052\n",
      "Episode: 14, Step: 40\n",
      "Next Action: [-0.811\n",
      "Step reward: -15.98025162515543, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -645.6056999992607\n",
      "Episode: 14, Step: 41\n",
      "Next Action: [-0.768\n",
      "Step reward: -15.97780995085433, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -661.583509950115\n",
      "Episode: 14, Step: 42\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.96442904432705, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -677.547938994442\n",
      "Episode: 14, Step: 43\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.961297800538478, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -693.5092367949804\n",
      "Episode: 14, Step: 44\n",
      "Next Action: [-0.955\n",
      "Step reward: -15.956134902079643, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -709.46537169706\n",
      "Episode: 14, Step: 45\n",
      "Next Action: [-1.079\n",
      "Step reward: -15.950160816947259, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -725.4155325140073\n",
      "Episode: 14, Step: 46\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.946804816298512, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -741.3623373303058\n",
      "Episode: 14, Step: 47\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.923640417617507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -757.2859777479233\n",
      "Episode: 14, Step: 48\n",
      "Next Action: [-0.596\n",
      "Step reward: -15.94473933247547, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -773.2307170803988\n",
      "Episode: 14, Step: 49\n",
      "Next Action: [-0.533\n",
      "Step reward: -15.99899680333487, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -789.2297138837337\n",
      "Episode: 14, Step: 50\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.98932861336015, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -805.2190424970938\n",
      "Episode: 14, Step: 51\n",
      "Next Action: [-0.786\n",
      "Step reward: -15.993719433507959, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -821.2127619306018\n",
      "Episode: 14, Step: 52\n",
      "Next Action: [-0.981\n",
      "Step reward: -16.0, Next State: [-1. -1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -837.2127619306018\n",
      "Episode: 14, Step: 53\n",
      "Next Action: [-1.128\n",
      "Step reward: -15.987200323520156, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -853.1999622541219\n",
      "Episode: 14, Step: 54\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.97065905730273, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -869.1706213114247\n",
      "Episode: 14, Step: 55\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.955900261582476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -885.1265215730072\n",
      "Episode: 14, Step: 56\n",
      "Next Action: [-0.982\n",
      "Step reward: -15.966614236066759, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -901.0931358090739\n",
      "Episode: 14, Step: 57\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.961281959950247, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -917.0544177690242\n",
      "Episode: 14, Step: 58\n",
      "Next Action: [-1.236\n",
      "Step reward: -15.954029883929081, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -933.0084476529532\n",
      "Episode: 14, Step: 59\n",
      "Next Action: [-1.371\n",
      "Step reward: -15.96695430520049, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -948.9754019581537\n",
      "Episode: 14, Step: 60\n",
      "Next Action: [-1.583\n",
      "Step reward: -15.98346604396221, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -964.9588680021159\n",
      "Episode: 14, Step: 61\n",
      "Next Action: [-1.765\n",
      "Step reward: -15.954963271470389, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -980.9138312735863\n",
      "Episode: 14, Step: 62\n",
      "Next Action: [-1.868\n",
      "Step reward: -15.933566805920915, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -996.8473980795072\n",
      "Episode: 14, Step: 63\n",
      "Next Action: [-1.705\n",
      "Step reward: -15.928671114170296, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1012.7760691936775\n",
      "Episode: 14, Step: 64\n",
      "Next Action: [-1.392\n",
      "Step reward: -15.921138709599091, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1028.6972079032766\n",
      "Episode: 14, Step: 65\n",
      "Next Action: [-1.420\n",
      "Step reward: -15.913791410454763, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1044.6109993137313\n",
      "Episode: 14, Step: 66\n",
      "Next Action: [-1.376\n",
      "Step reward: -15.922195759771686, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1060.533195073503\n",
      "Episode: 14, Step: 67\n",
      "Next Action: [-1.642\n",
      "Step reward: -15.924791521943677, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1076.4579865954468\n",
      "Episode: 14, Step: 68\n",
      "Next Action: [-1.577\n",
      "Step reward: -15.920727372084999, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1092.3787139675317\n",
      "Episode: 14, Step: 69\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.896098308069845, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1108.2748122756016\n",
      "Episode: 14, Step: 70\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.873943528833289, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1124.1487558044348\n",
      "Episode: 14, Step: 71\n",
      "Next Action: [-1.213\n",
      "Step reward: -15.889831750770565, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1140.0385875552054\n",
      "Episode: 14, Step: 72\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.904688618052162, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1155.9432761732576\n",
      "Episode: 14, Step: 73\n",
      "Next Action: [-1.376\n",
      "Step reward: -15.91039829863702, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1171.8536744718947\n",
      "Episode: 14, Step: 74\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.930922126777773, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1187.7845965986726\n",
      "Episode: 14, Step: 75\n",
      "Next Action: [-1.009\n",
      "Step reward: -15.937253474453401, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1203.721850073126\n",
      "Episode: 14, Step: 76\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.94343415646077, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1219.6652842295869\n",
      "Episode: 14, Step: 77\n",
      "Next Action: [-1.096\n",
      "Step reward: -15.940259367620982, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1235.6055435972078\n",
      "Episode: 14, Step: 78\n",
      "Next Action: [-1.243\n",
      "Step reward: -15.928370738098234, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1251.533914335306\n",
      "Episode: 14, Step: 79\n",
      "Next Action: [-1.012\n",
      "Step reward: -15.926933958381506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1267.4608482936876\n",
      "Episode: 14, Step: 80\n",
      "Next Action: [-1.100\n",
      "Step reward: -15.941901375892021, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1283.4027496695796\n",
      "Episode: 14, Step: 81\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.95580134828489, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1299.3585510178646\n",
      "Episode: 14, Step: 82\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.941292756976956, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1315.2998437748415\n",
      "Episode: 14, Step: 83\n",
      "Next Action: [-1.027\n",
      "Step reward: -15.901856396059472, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1331.201700170901\n",
      "Episode: 14, Step: 84\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.902261113740776, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1347.1039612846416\n",
      "Episode: 14, Step: 85\n",
      "Next Action: [-0.928\n",
      "Step reward: -15.905026719060734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1363.0089880037024\n",
      "Episode: 14, Step: 86\n",
      "Next Action: [-0.565\n",
      "Step reward: -15.909444981823938, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1378.9184329855264\n",
      "Episode: 14, Step: 87\n",
      "Next Action: [-0.955\n",
      "Step reward: -15.921068156970009, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1394.8395011424964\n",
      "Episode: 14, Step: 88\n",
      "Next Action: [-0.593\n",
      "Step reward: -15.948476237172816, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1410.7879773796692\n",
      "Episode: 14, Step: 89\n",
      "Next Action: [-0.196\n",
      "Step reward: -15.963513110209012, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1426.7514904898783\n",
      "Episode: 14, Step: 90\n",
      "Next Action: [-0.133\n",
      "Step reward: -15.939187255010072, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1442.6906777448883\n",
      "Episode: 14, Step: 91\n",
      "Next Action: [-0.460\n",
      "Step reward: -15.941563299751984, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1458.6322410446403\n",
      "Episode: 14, Step: 92\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.952366229581164, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1474.5846072742215\n",
      "Episode: 14, Step: 93\n",
      "Next Action: [-0.620\n",
      "Step reward: -15.957943446890553, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1490.542550721112\n",
      "Episode: 14, Step: 94\n",
      "Next Action: [-0.793\n",
      "Step reward: -15.97282691776892, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1506.515377638881\n",
      "Episode: 14, Step: 95\n",
      "Next Action: [-0.845\n",
      "Step reward: -15.931424873510469, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1522.4468025123915\n",
      "Episode: 14, Step: 96\n",
      "Next Action: [-5.530\n",
      "Step reward: -15.909618293417289, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1538.3564208058087\n",
      "Episode: 14, Step: 97\n",
      "Next Action: [-0.512\n",
      "Step reward: -15.908813133637194, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1554.265233939446\n",
      "Episode: 14, Step: 98\n",
      "Next Action: [-0.616\n",
      "Step reward: -15.910512814406237, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1570.1757467538523\n",
      "Episode: 14, Step: 99\n",
      "Next Action: [-0.808\n",
      "Step reward: -15.897337746046484, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1586.0730844998989\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 4.793241024017334\n",
      "Actor loss: 49.317970275878906\n",
      "Critic loss: 7.692881107330322\n",
      "Actor loss: 38.41120529174805\n",
      "Critic loss: 68.48979187011719\n",
      "Actor loss: 40.960426330566406\n",
      "Critic loss: 3.2230803966522217\n",
      "Actor loss: 44.65737533569336\n",
      "Critic loss: 1.7535830736160278\n",
      "Actor loss: 40.70527267456055\n",
      "Critic loss: 3.109480381011963\n",
      "Actor loss: 44.7930793762207\n",
      "Critic loss: 7.1706719398498535\n",
      "Actor loss: 33.44378662109375\n",
      "Critic loss: 8.357497215270996\n",
      "Actor loss: 29.584148406982422\n",
      "Critic loss: 10.186712265014648\n",
      "Actor loss: 34.62162780761719\n",
      "Critic loss: 2.3730249404907227\n",
      "Actor loss: 44.268985748291016\n",
      "Episode: 15\n",
      "Episode: 15, Step: 0\n",
      "Next Action: [-3.623\n",
      "Step reward: -11.56892037052855, Next State: [ 0.5\n",
      "Total episode reward: -11.56892037052855\n",
      "Episode: 15, Step: 1\n",
      "Next Action: [-0.706\n",
      "Step reward: -14.367407282067939, Next State: [-0.\n",
      "Total episode reward: -25.93632765259649\n",
      "Episode: 15, Step: 2\n",
      "Next Action: [-0.585\n",
      "Step reward: -15.370073770362414, Next State: [-0.\n",
      "Total episode reward: -41.3064014229589\n",
      "Episode: 15, Step: 3\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.609559208513504, Next State: [-1.\n",
      "Total episode reward: -56.9159606314724\n",
      "Episode: 15, Step: 4\n",
      "Next Action: [-0.769\n",
      "Step reward: -15.636788438128553, Next State: [-1.\n",
      "Total episode reward: -72.55274906960095\n",
      "Episode: 15, Step: 5\n",
      "Next Action: [-0.725\n",
      "Step reward: -15.721880494017535, Next State: [-1.\n",
      "Total episode reward: -88.27462956361849\n",
      "Episode: 15, Step: 6\n",
      "Next Action: [-0.372\n",
      "Step reward: -15.78913616316837, Next State: [-1. \n",
      "Total episode reward: -104.06376572678685\n",
      "Episode: 15, Step: 7\n",
      "Next Action: [-0.735\n",
      "Step reward: -15.832987964145156, Next State: [-1.\n",
      "Total episode reward: -119.89675369093202\n",
      "Episode: 15, Step: 8\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.851525872581792, Next State: [-1.\n",
      "Total episode reward: -135.74827956351382\n",
      "Episode: 15, Step: 9\n",
      "Next Action: [-0.723\n",
      "Step reward: -15.850929821346424, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.59920938486025\n",
      "Episode: 15, Step: 10\n",
      "Next Action: [-0.830\n",
      "Step reward: -15.827413474642853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.42662285950308\n",
      "Episode: 15, Step: 11\n",
      "Next Action: [-8.673\n",
      "Step reward: -15.793164481414745, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.21978734091783\n",
      "Episode: 15, Step: 12\n",
      "Next Action: [-7.856\n",
      "Step reward: -15.78959768777328, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -199.0093850286911\n",
      "Episode: 15, Step: 13\n",
      "Next Action: [-0.622\n",
      "Step reward: -15.818730301969431, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.82811533066052\n",
      "Episode: 15, Step: 14\n",
      "Next Action: [-0.274\n",
      "Step reward: -15.838180397894535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.66629572855507\n",
      "Episode: 15, Step: 15\n",
      "Next Action: [-0.265\n",
      "Step reward: -15.863006866007952, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.52930259456303\n",
      "Episode: 15, Step: 16\n",
      "Next Action: [-0.325\n",
      "Step reward: -15.901084209653527, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.43038680421654\n",
      "Episode: 15, Step: 17\n",
      "Next Action: [-0.574\n",
      "Step reward: -15.924750763674476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.355137567891\n",
      "Episode: 15, Step: 18\n",
      "Next Action: [-0.402\n",
      "Step reward: -15.931074035562684, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.2862116034537\n",
      "Episode: 15, Step: 19\n",
      "Next Action: [-0.522\n",
      "Step reward: -15.919951188675487, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.2061627921292\n",
      "Episode: 15, Step: 20\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.88036930087493, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -326.0865320930041\n",
      "Episode: 15, Step: 21\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.872260982854929, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.95879307585903\n",
      "Episode: 15, Step: 22\n",
      "Next Action: [-9.044\n",
      "Step reward: -15.861344083343571, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -357.8201371592026\n",
      "Episode: 15, Step: 23\n",
      "Next Action: [-8.491\n",
      "Step reward: -15.833254923851051, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -373.65339208305363\n",
      "Episode: 15, Step: 24\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.81535226155075, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.4687443446044\n",
      "Episode: 15, Step: 25\n",
      "Next Action: [-0.721\n",
      "Step reward: -15.86145021197023, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.3301945565746\n",
      "Episode: 15, Step: 26\n",
      "Next Action: [-0.696\n",
      "Step reward: -15.887270338496105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.2174648950707\n",
      "Episode: 15, Step: 27\n",
      "Next Action: [-0.757\n",
      "Step reward: -15.876677217125854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.09414211219655\n",
      "Episode: 15, Step: 28\n",
      "Next Action: [-0.645\n",
      "Step reward: -15.895605925076644, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -452.9897480372732\n",
      "Episode: 15, Step: 29\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.88970990934059, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -468.8794579466138\n",
      "Episode: 15, Step: 30\n",
      "Next Action: [-0.899\n",
      "Step reward: -15.883869951054614, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -484.76332789766843\n",
      "Episode: 15, Step: 31\n",
      "Next Action: [-1.266\n",
      "Step reward: -15.889597247969835, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -500.6529251456383\n",
      "Episode: 15, Step: 32\n",
      "Next Action: [-1.493\n",
      "Step reward: -15.895317456085756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -516.548242601724\n",
      "Episode: 15, Step: 33\n",
      "Next Action: [-1.664\n",
      "Step reward: -15.906334397548033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -532.454576999272\n",
      "Episode: 15, Step: 34\n",
      "Next Action: [-1.375\n",
      "Step reward: -15.908349789529735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.3629267888018\n",
      "Episode: 15, Step: 35\n",
      "Next Action: [-1.523\n",
      "Step reward: -15.937473522222225, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.3004003110241\n",
      "Episode: 15, Step: 36\n",
      "Next Action: [-1.377\n",
      "Step reward: -15.939231294022617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.2396316050467\n",
      "Episode: 15, Step: 37\n",
      "Next Action: [-1.521\n",
      "Step reward: -15.944115106775952, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -596.1837467118227\n",
      "Episode: 15, Step: 38\n",
      "Next Action: [-1.508\n",
      "Step reward: -15.925700891501112, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -612.1094476033238\n",
      "Episode: 15, Step: 39\n",
      "Next Action: [-1.649\n",
      "Step reward: -15.90846393987782, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -628.0179115432016\n",
      "Episode: 15, Step: 40\n",
      "Next Action: [-1.554\n",
      "Step reward: -15.94301363761547, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -643.9609251808171\n",
      "Episode: 15, Step: 41\n",
      "Next Action: [-1.423\n",
      "Step reward: -15.91660783080513, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -659.8775330116222\n",
      "Episode: 15, Step: 42\n",
      "Next Action: [-1.088\n",
      "Step reward: -15.89720461798788, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -675.7747376296101\n",
      "Episode: 15, Step: 43\n",
      "Next Action: [-1.257\n",
      "Step reward: -15.865891558956147, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -691.6406291885662\n",
      "Episode: 15, Step: 44\n",
      "Next Action: [-1.162\n",
      "Step reward: -15.843335049104914, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -707.4839642376711\n",
      "Episode: 15, Step: 45\n",
      "Next Action: [-1.231\n",
      "Step reward: -15.847561048609716, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -723.3315252862808\n",
      "Episode: 15, Step: 46\n",
      "Next Action: [-1.314\n",
      "Step reward: -15.86589666547504, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -739.1974219517558\n",
      "Episode: 15, Step: 47\n",
      "Next Action: [-1.228\n",
      "Step reward: -15.875586053127284, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -755.0730080048831\n",
      "Episode: 15, Step: 48\n",
      "Next Action: [-1.053\n",
      "Step reward: -15.909039500870158, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -770.9820475057533\n",
      "Episode: 15, Step: 49\n",
      "Next Action: [-1.051\n",
      "Step reward: -15.91785787730101, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -786.8999053830543\n",
      "Episode: 15, Step: 50\n",
      "Next Action: [-0.494\n",
      "Step reward: -15.897028032212969, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -802.7969334152672\n",
      "Episode: 15, Step: 51\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.888511761028886, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -818.6854451762961\n",
      "Episode: 15, Step: 52\n",
      "Next Action: [-1.020\n",
      "Step reward: -15.89252855387217, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -834.5779737301682\n",
      "Episode: 15, Step: 53\n",
      "Next Action: [-1.032\n",
      "Step reward: -15.898953028593946, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -850.4769267587621\n",
      "Episode: 15, Step: 54\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.895865909043271, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -866.3727926678055\n",
      "Episode: 15, Step: 55\n",
      "Next Action: [-1.110\n",
      "Step reward: -15.909279224205989, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -882.2820718920115\n",
      "Episode: 15, Step: 56\n",
      "Next Action: [-1.267\n",
      "Step reward: -15.934655808430112, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -898.2167277004415\n",
      "Episode: 15, Step: 57\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.937089063344303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -914.1538167637858\n",
      "Episode: 15, Step: 58\n",
      "Next Action: [-1.033\n",
      "Step reward: -15.926179093010754, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -930.0799958567966\n",
      "Episode: 15, Step: 59\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.942476823994967, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -946.0224726807916\n",
      "Episode: 15, Step: 60\n",
      "Next Action: [-1.226\n",
      "Step reward: -15.89153306800816, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -961.9140057487997\n",
      "Episode: 15, Step: 61\n",
      "Next Action: [-1.352\n",
      "Step reward: -15.82696251433589, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -977.7409682631355\n",
      "Episode: 15, Step: 62\n",
      "Next Action: [-1.334\n",
      "Step reward: -15.821343118090795, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.5623113812263\n",
      "Episode: 15, Step: 63\n",
      "Next Action: [-1.022\n",
      "Step reward: -15.850738313506396, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.4130496947328\n",
      "Episode: 15, Step: 64\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.900606850091636, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.3136565448244\n",
      "Episode: 15, Step: 65\n",
      "Next Action: [-1.234\n",
      "Step reward: -15.94949482835947, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1041.263151373184\n",
      "Episode: 15, Step: 66\n",
      "Next Action: [-1.200\n",
      "Step reward: -15.975259387364016, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1057.238410760548\n",
      "Episode: 15, Step: 67\n",
      "Next Action: [-0.811\n",
      "Step reward: -15.96868839854812, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1073.207099159096\n",
      "Episode: 15, Step: 68\n",
      "Next Action: [-0.922\n",
      "Step reward: -15.916005074925991, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1089.123104234022\n",
      "Episode: 15, Step: 69\n",
      "Next Action: [-0.576\n",
      "Step reward: -15.895492707821695, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1105.0185969418437\n",
      "Episode: 15, Step: 70\n",
      "Next Action: [-0.365\n",
      "Step reward: -15.915148527005977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1120.9337454688498\n",
      "Episode: 15, Step: 71\n",
      "Next Action: [-0.722\n",
      "Step reward: -15.940440679506494, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1136.8741861483563\n",
      "Episode: 15, Step: 72\n",
      "Next Action: [-0.590\n",
      "Step reward: -15.926166177080086, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1152.8003523254365\n",
      "Episode: 15, Step: 73\n",
      "Next Action: [-0.168\n",
      "Step reward: -15.895881985021548, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1168.696234310458\n",
      "Episode: 15, Step: 74\n",
      "Next Action: [-0.549\n",
      "Step reward: -15.880436456792516, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1184.5766707672506\n",
      "Episode: 15, Step: 75\n",
      "Next Action: [-0.513\n",
      "Step reward: -15.896620661016318, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1200.473291428267\n",
      "Episode: 15, Step: 76\n",
      "Next Action: [-0.504\n",
      "Step reward: -15.915661591827703, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1216.3889530200947\n",
      "Episode: 15, Step: 77\n",
      "Next Action: [-0.898\n",
      "Step reward: -15.915656205135843, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1232.3046092252305\n",
      "Episode: 15, Step: 78\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.93770825209582, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1248.2423174773262\n",
      "Episode: 15, Step: 79\n",
      "Next Action: [-0.377\n",
      "Step reward: -15.935966205483778, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1264.17828368281\n",
      "Episode: 15, Step: 80\n",
      "Next Action: [-0.374\n",
      "Step reward: -15.923970295861569, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1280.1022539786716\n",
      "Episode: 15, Step: 81\n",
      "Next Action: [-4.339\n",
      "Step reward: -15.93660718337037, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1296.038861162042\n",
      "Episode: 15, Step: 82\n",
      "Next Action: [-0.495\n",
      "Step reward: -15.895676905520077, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1311.934538067562\n",
      "Episode: 15, Step: 83\n",
      "Next Action: [-0.605\n",
      "Step reward: -15.881542304507933, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1327.81608037207\n",
      "Episode: 15, Step: 84\n",
      "Next Action: [-0.825\n",
      "Step reward: -15.8773305063211, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1343.6934108783912\n",
      "Episode: 15, Step: 85\n",
      "Next Action: [-0.797\n",
      "Step reward: -15.870267699997122, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1359.5636785783884\n",
      "Episode: 15, Step: 86\n",
      "Next Action: [-9.320\n",
      "Step reward: -15.88452080553142, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1375.4481993839197\n",
      "Episode: 15, Step: 87\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.866205787063187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1391.3144051709828\n",
      "Episode: 15, Step: 88\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.86710488340779, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1407.1815100543906\n",
      "Episode: 15, Step: 89\n",
      "Next Action: [-1.056\n",
      "Step reward: -15.86526902563887, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1423.0467790800294\n",
      "Episode: 15, Step: 90\n",
      "Next Action: [-1.290\n",
      "Step reward: -15.877114015799265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1438.9238930958286\n",
      "Episode: 15, Step: 91\n",
      "Next Action: [-1.041\n",
      "Step reward: -15.878398444989347, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1454.802291540818\n",
      "Episode: 15, Step: 92\n",
      "Next Action: [-1.236\n",
      "Step reward: -15.863759296123677, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1470.6660508369416\n",
      "Episode: 15, Step: 93\n",
      "Next Action: [-1.380\n",
      "Step reward: -15.83128127855181, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1486.4973321154935\n",
      "Episode: 15, Step: 94\n",
      "Next Action: [-1.217\n",
      "Step reward: -15.815496093358245, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1502.3128282088517\n",
      "Episode: 15, Step: 95\n",
      "Next Action: [-1.464\n",
      "Step reward: -15.854226763793706, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1518.1670549726455\n",
      "Episode: 15, Step: 96\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.839405755285279, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1534.0064607279307\n",
      "Episode: 15, Step: 97\n",
      "Next Action: [-1.613\n",
      "Step reward: -15.848399876050902, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1549.8548606039817\n",
      "Episode: 15, Step: 98\n",
      "Next Action: [-1.515\n",
      "Step reward: -15.848081061141748, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1565.7029416651235\n",
      "Episode: 15, Step: 99\n",
      "Next Action: [-1.441\n",
      "Step reward: -15.811884604042046, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1581.5148262691655\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 1.6685458421707153\n",
      "Actor loss: 29.7120418548584\n",
      "Critic loss: 94.16179656982422\n",
      "Actor loss: 24.59842300415039\n",
      "Critic loss: 2.0840134620666504\n",
      "Actor loss: 30.065876007080078\n",
      "Critic loss: 7.178269386291504\n",
      "Actor loss: 28.02969741821289\n",
      "Critic loss: 10.164159774780273\n",
      "Actor loss: 39.26658248901367\n",
      "Critic loss: 88.87709045410156\n",
      "Actor loss: 20.99493408203125\n",
      "Critic loss: 8.317184448242188\n",
      "Actor loss: 40.1008415222168\n",
      "Critic loss: 4.938302040100098\n",
      "Actor loss: 54.006629943847656\n",
      "Critic loss: 8.700560569763184\n",
      "Actor loss: 41.16088104248047\n",
      "Critic loss: 10.210122108459473\n",
      "Actor loss: 46.981117248535156\n",
      "Episode: 16\n",
      "Episode: 16, Step: 0\n",
      "Next Action: [-1.136\n",
      "Step reward: -12.126117571980293, Next State: [-0.\n",
      "Total episode reward: -12.126117571980293\n",
      "Episode: 16, Step: 1\n",
      "Next Action: [-1.088\n",
      "Step reward: -14.814606502995638, Next State: [-1.\n",
      "Total episode reward: -26.940724074975932\n",
      "Episode: 16, Step: 2\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.390827074942894, Next State: [-1.\n",
      "Total episode reward: -42.33155114991882\n",
      "Episode: 16, Step: 3\n",
      "Next Action: [-0.781\n",
      "Step reward: -15.611174777132753, Next State: [-1.\n",
      "Total episode reward: -57.942725927051576\n",
      "Episode: 16, Step: 4\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.805628906199715, Next State: [-1.\n",
      "Total episode reward: -73.74835483325128\n",
      "Episode: 16, Step: 5\n",
      "Next Action: [-6.786\n",
      "Step reward: -15.886598499858268, Next State: [-1.\n",
      "Total episode reward: -89.63495333310955\n",
      "Episode: 16, Step: 6\n",
      "Next Action: [-0.674\n",
      "Step reward: -15.884653931536585, Next State: [-1.\n",
      "Total episode reward: -105.51960726464614\n",
      "Episode: 16, Step: 7\n",
      "Next Action: [-0.633\n",
      "Step reward: -15.875443760816944, Next State: [-1.\n",
      "Total episode reward: -121.39505102546309\n",
      "Episode: 16, Step: 8\n",
      "Next Action: [-6.578\n",
      "Step reward: -15.8980368980463, Next State: [-1.  \n",
      "Total episode reward: -137.2930879235094\n",
      "Episode: 16, Step: 9\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.915539692133452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.20862761564285\n",
      "Episode: 16, Step: 10\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.959656576992266, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.16828419263513\n",
      "Episode: 16, Step: 11\n",
      "Next Action: [-1.004\n",
      "Step reward: -15.98488011259775, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.15316430523288\n",
      "Episode: 16, Step: 12\n",
      "Next Action: [-0.875\n",
      "Step reward: -15.95081241453243, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.1039767197653\n",
      "Episode: 16, Step: 13\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.9372080662837, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.041184786049\n",
      "Episode: 16, Step: 14\n",
      "Next Action: [-0.525\n",
      "Step reward: -15.944095283701115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.9852800697501\n",
      "Episode: 16, Step: 15\n",
      "Next Action: [-1.005\n",
      "Step reward: -15.980829716641441, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.96610978639154\n",
      "Episode: 16, Step: 16\n",
      "Next Action: [-9.853\n",
      "Step reward: -15.934060392884579, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.90017017927613\n",
      "Episode: 16, Step: 17\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.87714433794902, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.77731451722514\n",
      "Episode: 16, Step: 18\n",
      "Next Action: [-1.325\n",
      "Step reward: -15.871843108083766, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.64915762530893\n",
      "Episode: 16, Step: 19\n",
      "Next Action: [-0.894\n",
      "Step reward: -15.883295790134035, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.53245341544294\n",
      "Episode: 16, Step: 20\n",
      "Next Action: [-0.687\n",
      "Step reward: -15.862971119223316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.39542453466623\n",
      "Episode: 16, Step: 21\n",
      "Next Action: [-0.893\n",
      "Step reward: -15.8613580085879, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.25678254325413\n",
      "Episode: 16, Step: 22\n",
      "Next Action: [-7.772\n",
      "Step reward: -15.85991775151909, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.11670029477324\n",
      "Episode: 16, Step: 23\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.864717215200756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.981417509974\n",
      "Episode: 16, Step: 24\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.878274636803281, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.85969214677726\n",
      "Episode: 16, Step: 25\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.91778287504424, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.7774750218215\n",
      "Episode: 16, Step: 26\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.953442824517293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.7309178463388\n",
      "Episode: 16, Step: 27\n",
      "Next Action: [-0.997\n",
      "Step reward: -15.93570046024883, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.6666183065876\n",
      "Episode: 16, Step: 28\n",
      "Next Action: [-9.102\n",
      "Step reward: -15.934991857801727, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.60161016438934\n",
      "Episode: 16, Step: 29\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.930566888969249, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.53217705335857\n",
      "Episode: 16, Step: 30\n",
      "Next Action: [-0.764\n",
      "Step reward: -15.948926836744821, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.4811038901034\n",
      "Episode: 16, Step: 31\n",
      "Next Action: [-0.303\n",
      "Step reward: -15.94691624230656, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.4280201324099\n",
      "Episode: 16, Step: 32\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.942946504494452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.3709666369044\n",
      "Episode: 16, Step: 33\n",
      "Next Action: [-0.660\n",
      "Step reward: -15.968514549786555, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.3394811866909\n",
      "Episode: 16, Step: 34\n",
      "Next Action: [-0.394\n",
      "Step reward: -15.974486916764684, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.3139681034556\n",
      "Episode: 16, Step: 35\n",
      "Next Action: [-0.571\n",
      "Step reward: -15.941028328368601, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -567.2549964318242\n",
      "Episode: 16, Step: 36\n",
      "Next Action: [-0.627\n",
      "Step reward: -15.897557451027629, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -583.1525538828519\n",
      "Episode: 16, Step: 37\n",
      "Next Action: [-0.670\n",
      "Step reward: -15.8943203003763, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -599.0468741832282\n",
      "Episode: 16, Step: 38\n",
      "Next Action: [-0.976\n",
      "Step reward: -15.922355464815851, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -614.969229648044\n",
      "Episode: 16, Step: 39\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.896694958059218, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -630.8659246061032\n",
      "Episode: 16, Step: 40\n",
      "Next Action: [-6.271\n",
      "Step reward: -15.966629918277562, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -646.8325545243808\n",
      "Episode: 16, Step: 41\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.958356668167085, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -662.7909111925479\n",
      "Episode: 16, Step: 42\n",
      "Next Action: [-1.038\n",
      "Step reward: -15.923943184717658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -678.7148543772656\n",
      "Episode: 16, Step: 43\n",
      "Next Action: [-1.209\n",
      "Step reward: -15.896537822607225, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -694.6113921998729\n",
      "Episode: 16, Step: 44\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.846248607578845, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -710.4576408074518\n",
      "Episode: 16, Step: 45\n",
      "Next Action: [-1.448\n",
      "Step reward: -15.799619245459144, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -726.2572600529109\n",
      "Episode: 16, Step: 46\n",
      "Next Action: [-1.241\n",
      "Step reward: -15.809449571555017, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -742.0667096244658\n",
      "Episode: 16, Step: 47\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.827399480239073, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -757.8941091047049\n",
      "Episode: 16, Step: 48\n",
      "Next Action: [-1.102\n",
      "Step reward: -15.855119713229788, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -773.7492288179346\n",
      "Episode: 16, Step: 49\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.86698332582669, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -789.6162121437613\n",
      "Episode: 16, Step: 50\n",
      "Next Action: [-1.180\n",
      "Step reward: -15.87576793304067, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -805.4919800768021\n",
      "Episode: 16, Step: 51\n",
      "Next Action: [-1.111\n",
      "Step reward: -15.891253969576889, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -821.3832340463789\n",
      "Episode: 16, Step: 52\n",
      "Next Action: [-1.257\n",
      "Step reward: -15.917184869763993, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -837.3004189161429\n",
      "Episode: 16, Step: 53\n",
      "Next Action: [-1.189\n",
      "Step reward: -15.920216068359926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -853.2206349845028\n",
      "Episode: 16, Step: 54\n",
      "Next Action: [-1.091\n",
      "Step reward: -15.925891403602321, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -869.1465263881051\n",
      "Episode: 16, Step: 55\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.917194486503071, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -885.0637208746082\n",
      "Episode: 16, Step: 56\n",
      "Next Action: [-1.437\n",
      "Step reward: -15.889435173781354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -900.9531560483896\n",
      "Episode: 16, Step: 57\n",
      "Next Action: [-1.693\n",
      "Step reward: -15.842226472299194, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -916.7953825206888\n",
      "Episode: 16, Step: 58\n",
      "Next Action: [-1.846\n",
      "Step reward: -15.835240409765467, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -932.6306229304543\n",
      "Episode: 16, Step: 59\n",
      "Next Action: [-1.775\n",
      "Step reward: -15.818485423194995, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -948.4491083536492\n",
      "Episode: 16, Step: 60\n",
      "Next Action: [-1.739\n",
      "Step reward: -15.840485613995497, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -964.2895939676447\n",
      "Episode: 16, Step: 61\n",
      "Next Action: [-1.562\n",
      "Step reward: -15.870843242400841, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -980.1604372100455\n",
      "Episode: 16, Step: 62\n",
      "Next Action: [-1.626\n",
      "Step reward: -15.896469777970273, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -996.0569069880158\n",
      "Episode: 16, Step: 63\n",
      "Next Action: [-1.490\n",
      "Step reward: -15.933953290879229, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1011.9908602788951\n",
      "Episode: 16, Step: 64\n",
      "Next Action: [-1.266\n",
      "Step reward: -15.935697901771448, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1027.9265581806665\n",
      "Episode: 16, Step: 65\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.946086567006134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1043.8726447476727\n",
      "Episode: 16, Step: 66\n",
      "Next Action: [-1.540\n",
      "Step reward: -15.961430430472697, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1059.8340751781454\n",
      "Episode: 16, Step: 67\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.958758087129146, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1075.7928332652746\n",
      "Episode: 16, Step: 68\n",
      "Next Action: [-1.033\n",
      "Step reward: -15.939311215612108, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1091.7321444808867\n",
      "Episode: 16, Step: 69\n",
      "Next Action: [-1.120\n",
      "Step reward: -15.899906907398144, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1107.632051388285\n",
      "Episode: 16, Step: 70\n",
      "Next Action: [-0.984\n",
      "Step reward: -15.873128917952181, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1123.5051803062372\n",
      "Episode: 16, Step: 71\n",
      "Next Action: [-1.090\n",
      "Step reward: -15.904115558430888, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1139.4092958646681\n",
      "Episode: 16, Step: 72\n",
      "Next Action: [-1.366\n",
      "Step reward: -15.921239751716055, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1155.3305356163842\n",
      "Episode: 16, Step: 73\n",
      "Next Action: [-1.407\n",
      "Step reward: -15.914507054404849, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1171.245042670789\n",
      "Episode: 16, Step: 74\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.961730079475359, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1187.2067727502642\n",
      "Episode: 16, Step: 75\n",
      "Next Action: [-1.688\n",
      "Step reward: -15.937326598608534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1203.1440993488727\n",
      "Episode: 16, Step: 76\n",
      "Next Action: [-1.688\n",
      "Step reward: -15.90790934472226, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1219.0520086935949\n",
      "Episode: 16, Step: 77\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.89400287500893, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1234.9460115686038\n",
      "Episode: 16, Step: 78\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.879290225244452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1250.8253017938482\n",
      "Episode: 16, Step: 79\n",
      "Next Action: [-1.175\n",
      "Step reward: -15.87988920029617, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1266.7051909941445\n",
      "Episode: 16, Step: 80\n",
      "Next Action: [-1.009\n",
      "Step reward: -15.896362976756594, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1282.601553970901\n",
      "Episode: 16, Step: 81\n",
      "Next Action: [-0.799\n",
      "Step reward: -15.918511531252584, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1298.5200655021536\n",
      "Episode: 16, Step: 82\n",
      "Next Action: [-0.791\n",
      "Step reward: -15.970864717781724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1314.4909302199353\n",
      "Episode: 16, Step: 83\n",
      "Next Action: [-0.939\n",
      "Step reward: -15.972881276374567, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1330.46381149631\n",
      "Episode: 16, Step: 84\n",
      "Next Action: [-1.146\n",
      "Step reward: -15.991970962756216, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1346.455782459066\n",
      "Episode: 16, Step: 85\n",
      "Next Action: [-0.826\n",
      "Step reward: -15.993866084595398, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1362.4496485436614\n",
      "Episode: 16, Step: 86\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.995405669574962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1378.4450542132363\n",
      "Episode: 16, Step: 87\n",
      "Next Action: [-0.923\n",
      "Step reward: -15.959566330350588, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1394.4046205435868\n",
      "Episode: 16, Step: 88\n",
      "Next Action: [-0.876\n",
      "Step reward: -15.924754431469836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1410.3293749750567\n",
      "Episode: 16, Step: 89\n",
      "Next Action: [-0.928\n",
      "Step reward: -15.869964810590545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1426.1993397856472\n",
      "Episode: 16, Step: 90\n",
      "Next Action: [-0.955\n",
      "Step reward: -15.833155553243047, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1442.0324953388904\n",
      "Episode: 16, Step: 91\n",
      "Next Action: [-1.083\n",
      "Step reward: -15.867415941403962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1457.8999112802944\n",
      "Episode: 16, Step: 92\n",
      "Next Action: [-1.293\n",
      "Step reward: -15.878924922022915, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1473.7788362023173\n",
      "Episode: 16, Step: 93\n",
      "Next Action: [-1.230\n",
      "Step reward: -15.896418343363006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1489.6752545456802\n",
      "Episode: 16, Step: 94\n",
      "Next Action: [-1.469\n",
      "Step reward: -15.887960195428128, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1505.5632147411084\n",
      "Episode: 16, Step: 95\n",
      "Next Action: [-1.013\n",
      "Step reward: -15.905191514319721, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1521.4684062554281\n",
      "Episode: 16, Step: 96\n",
      "Next Action: [-0.908\n",
      "Step reward: -15.914033078418601, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1537.3824393338468\n",
      "Episode: 16, Step: 97\n",
      "Next Action: [-0.701\n",
      "Step reward: -15.915604316061954, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1553.2980436499088\n",
      "Episode: 16, Step: 98\n",
      "Next Action: [-0.450\n",
      "Step reward: -15.91464295697009, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1569.2126866068788\n",
      "Episode: 16, Step: 99\n",
      "Next Action: [-0.556\n",
      "Step reward: -15.938240377289839, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1585.1509269841686\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.950571060180664\n",
      "Actor loss: 46.476524353027344\n",
      "Critic loss: 1.4548465013504028\n",
      "Actor loss: 55.413551330566406\n",
      "Critic loss: 64.01007843017578\n",
      "Actor loss: 30.246768951416016\n",
      "Critic loss: 51.036312103271484\n",
      "Actor loss: 50.27298355102539\n",
      "Critic loss: 4.956859111785889\n",
      "Actor loss: 44.01889419555664\n",
      "Critic loss: 1.9869710206985474\n",
      "Actor loss: 68.00611877441406\n",
      "Critic loss: 5.353668212890625\n",
      "Actor loss: 72.69349670410156\n",
      "Critic loss: 35.19911193847656\n",
      "Actor loss: 53.85139083862305\n",
      "Critic loss: 1.8661835193634033\n",
      "Actor loss: 79.63680267333984\n",
      "Critic loss: 2.266617774963379\n",
      "Actor loss: 77.86770629882812\n",
      "Episode: 17\n",
      "Episode: 17, Step: 0\n",
      "Next Action: [-0.437\n",
      "Step reward: -11.558039401626669, Next State: [-1.\n",
      "Total episode reward: -11.558039401626669\n",
      "Episode: 17, Step: 1\n",
      "Next Action: [-0.765\n",
      "Step reward: -14.381459102075564, Next State: [-1.\n",
      "Total episode reward: -25.939498503702232\n",
      "Episode: 17, Step: 2\n",
      "Next Action: [-0.601\n",
      "Step reward: -15.219440912000536, Next State: [-1.\n",
      "Total episode reward: -41.15893941570277\n",
      "Episode: 17, Step: 3\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.449728138068672, Next State: [-1.\n",
      "Total episode reward: -56.60866755377144\n",
      "Episode: 17, Step: 4\n",
      "Next Action: [-0.743\n",
      "Step reward: -15.662408591961, Next State: [-1.   \n",
      "Total episode reward: -72.27107614573244\n",
      "Episode: 17, Step: 5\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.770214191411078, Next State: [-1.\n",
      "Total episode reward: -88.04129033714352\n",
      "Episode: 17, Step: 6\n",
      "Next Action: [-0.911\n",
      "Step reward: -15.80343105096307, Next State: [-1. \n",
      "Total episode reward: -103.84472138810659\n",
      "Episode: 17, Step: 7\n",
      "Next Action: [-0.960\n",
      "Step reward: -15.827693126538664, Next State: [-1.\n",
      "Total episode reward: -119.67241451464525\n",
      "Episode: 17, Step: 8\n",
      "Next Action: [-1.088\n",
      "Step reward: -15.853457118064588, Next State: [-1.\n",
      "Total episode reward: -135.52587163270985\n",
      "Episode: 17, Step: 9\n",
      "Next Action: [-0.977\n",
      "Step reward: -15.854928350808422, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -151.38079998351827\n",
      "Episode: 17, Step: 10\n",
      "Next Action: [-1.437\n",
      "Step reward: -15.862622721608526, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -167.2434227051268\n",
      "Episode: 17, Step: 11\n",
      "Next Action: [-1.494\n",
      "Step reward: -15.861562054943885, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -183.10498476007066\n",
      "Episode: 17, Step: 12\n",
      "Next Action: [-1.683\n",
      "Step reward: -15.853911158216528, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -198.9588959182872\n",
      "Episode: 17, Step: 13\n",
      "Next Action: [-1.477\n",
      "Step reward: -15.831143631727864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.79003955001505\n",
      "Episode: 17, Step: 14\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.843879356578633, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.6339189065937\n",
      "Episode: 17, Step: 15\n",
      "Next Action: [-1.434\n",
      "Step reward: -15.877283577217787, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.51120248381147\n",
      "Episode: 17, Step: 16\n",
      "Next Action: [-1.597\n",
      "Step reward: -15.891249556845075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -262.40245204065656\n",
      "Episode: 17, Step: 17\n",
      "Next Action: [-1.473\n",
      "Step reward: -15.919045059862748, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -278.3214971005193\n",
      "Episode: 17, Step: 18\n",
      "Next Action: [-1.506\n",
      "Step reward: -15.908468913535915, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -294.2299660140552\n",
      "Episode: 17, Step: 19\n",
      "Next Action: [-1.533\n",
      "Step reward: -15.882483814388998, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -310.1124498284442\n",
      "Episode: 17, Step: 20\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.864404852533887, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -325.9768546809781\n",
      "Episode: 17, Step: 21\n",
      "Next Action: [-1.349\n",
      "Step reward: -15.872195610946484, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.8490502919246\n",
      "Episode: 17, Step: 22\n",
      "Next Action: [-1.331\n",
      "Step reward: -15.925791997778306, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -357.7748422897029\n",
      "Episode: 17, Step: 23\n",
      "Next Action: [-1.634\n",
      "Step reward: -15.922643272735785, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -373.6974855624387\n",
      "Episode: 17, Step: 24\n",
      "Next Action: [-1.556\n",
      "Step reward: -15.933289582760462, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -389.63077514519915\n",
      "Episode: 17, Step: 25\n",
      "Next Action: [-1.476\n",
      "Step reward: -15.956843440394122, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -405.58761858559325\n",
      "Episode: 17, Step: 26\n",
      "Next Action: [-1.290\n",
      "Step reward: -15.973519356354995, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -421.56113794194823\n",
      "Episode: 17, Step: 27\n",
      "Next Action: [-1.297\n",
      "Step reward: -15.970960296027636, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -437.53209823797584\n",
      "Episode: 17, Step: 28\n",
      "Next Action: [-1.267\n",
      "Step reward: -15.94584248820675, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -453.4779407261826\n",
      "Episode: 17, Step: 29\n",
      "Next Action: [-0.877\n",
      "Step reward: -15.926560998463207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -469.40450172464585\n",
      "Episode: 17, Step: 30\n",
      "Next Action: [-1.095\n",
      "Step reward: -15.926220148716757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -485.3307218733626\n",
      "Episode: 17, Step: 31\n",
      "Next Action: [-1.029\n",
      "Step reward: -15.915932345002798, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -501.2466542183654\n",
      "Episode: 17, Step: 32\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.907591189632388, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -517.1542454079978\n",
      "Episode: 17, Step: 33\n",
      "Next Action: [-1.302\n",
      "Step reward: -15.878579059002943, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -533.0328244670008\n",
      "Episode: 17, Step: 34\n",
      "Next Action: [-1.303\n",
      "Step reward: -15.86500049075787, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -548.8978249577586\n",
      "Episode: 17, Step: 35\n",
      "Next Action: [-1.228\n",
      "Step reward: -15.873482575928863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -564.7713075336875\n",
      "Episode: 17, Step: 36\n",
      "Next Action: [-0.920\n",
      "Step reward: -15.936139131606803, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -580.7074466652942\n",
      "Episode: 17, Step: 37\n",
      "Next Action: [-0.997\n",
      "Step reward: -15.942388662123099, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -596.6498353274173\n",
      "Episode: 17, Step: 38\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.962403125943016, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -612.6122384533604\n",
      "Episode: 17, Step: 39\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.9580537486539, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -628.5702922020143\n",
      "Episode: 17, Step: 40\n",
      "Next Action: [-0.789\n",
      "Step reward: -15.94260897197452, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -644.5129011739889\n",
      "Episode: 17, Step: 41\n",
      "Next Action: [-1.142\n",
      "Step reward: -15.929883391844307, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -660.4427845658332\n",
      "Episode: 17, Step: 42\n",
      "Next Action: [-1.315\n",
      "Step reward: -15.9354396907237, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -676.3782242565569\n",
      "Episode: 17, Step: 43\n",
      "Next Action: [-1.195\n",
      "Step reward: -15.919890248792331, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -692.2981145053492\n",
      "Episode: 17, Step: 44\n",
      "Next Action: [-0.843\n",
      "Step reward: -15.919653521836148, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -708.2177680271853\n",
      "Episode: 17, Step: 45\n",
      "Next Action: [-0.467\n",
      "Step reward: -15.898848701170659, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -724.116616728356\n",
      "Episode: 17, Step: 46\n",
      "Next Action: [-0.359\n",
      "Step reward: -15.868562410291556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -739.9851791386476\n",
      "Episode: 17, Step: 47\n",
      "Next Action: [-0.206\n",
      "Step reward: -15.825022890943417, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -755.810202029591\n",
      "Episode: 17, Step: 48\n",
      "Next Action: [-0.220\n",
      "Step reward: -15.81612368593615, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -771.6263257155272\n",
      "Episode: 17, Step: 49\n",
      "Next Action: [-0.333\n",
      "Step reward: -15.835802844537035, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -787.4621285600642\n",
      "Episode: 17, Step: 50\n",
      "Next Action: [-0.503\n",
      "Step reward: -15.874385966203787, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -803.336514526268\n",
      "Episode: 17, Step: 51\n",
      "Next Action: [-0.558\n",
      "Step reward: -15.8872373373681, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -819.2237518636362\n",
      "Episode: 17, Step: 52\n",
      "Next Action: [-0.572\n",
      "Step reward: -15.843533211999057, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -835.0672850756353\n",
      "Episode: 17, Step: 53\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.832723262885205, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -850.9000083385205\n",
      "Episode: 17, Step: 54\n",
      "Next Action: [-0.502\n",
      "Step reward: -15.88151481773993, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -866.7815231562604\n",
      "Episode: 17, Step: 55\n",
      "Next Action: [-8.363\n",
      "Step reward: -15.886198503006446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -882.6677216592668\n",
      "Episode: 17, Step: 56\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.881558758522274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -898.5492804177891\n",
      "Episode: 17, Step: 57\n",
      "Next Action: [-0.906\n",
      "Step reward: -15.853781785385634, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -914.4030622031747\n",
      "Episode: 17, Step: 58\n",
      "Next Action: [-0.772\n",
      "Step reward: -15.891267068442612, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -930.2943292716174\n",
      "Episode: 17, Step: 59\n",
      "Next Action: [-7.072\n",
      "Step reward: -15.890933930213956, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -946.1852632018313\n",
      "Episode: 17, Step: 60\n",
      "Next Action: [-1.027\n",
      "Step reward: -15.909795480133802, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -962.095058681965\n",
      "Episode: 17, Step: 61\n",
      "Next Action: [-1.228\n",
      "Step reward: -15.943302647145483, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -978.0383613291106\n",
      "Episode: 17, Step: 62\n",
      "Next Action: [-1.093\n",
      "Step reward: -15.937934838265814, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -993.9762961673764\n",
      "Episode: 17, Step: 63\n",
      "Next Action: [-1.186\n",
      "Step reward: -15.965525426991487, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1009.9418215943679\n",
      "Episode: 17, Step: 64\n",
      "Next Action: [-1.159\n",
      "Step reward: -15.946036653890717, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1025.8878582482587\n",
      "Episode: 17, Step: 65\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.926682636228865, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1041.8145408844875\n",
      "Episode: 17, Step: 66\n",
      "Next Action: [-1.442\n",
      "Step reward: -15.894783763883664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1057.7093246483712\n",
      "Episode: 17, Step: 67\n",
      "Next Action: [-1.405\n",
      "Step reward: -15.886637652623666, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1073.5959623009949\n",
      "Episode: 17, Step: 68\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.898589030079423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1089.4945513310743\n",
      "Episode: 17, Step: 69\n",
      "Next Action: [-0.906\n",
      "Step reward: -15.89703807347128, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1105.3915894045456\n",
      "Episode: 17, Step: 70\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.883721193702737, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1121.2753105982483\n",
      "Episode: 17, Step: 71\n",
      "Next Action: [-1.211\n",
      "Step reward: -15.909423691061642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1137.18473428931\n",
      "Episode: 17, Step: 72\n",
      "Next Action: [-0.899\n",
      "Step reward: -15.939562278713673, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1153.1242965680237\n",
      "Episode: 17, Step: 73\n",
      "Next Action: [-0.765\n",
      "Step reward: -15.905935660491476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1169.0302322285152\n",
      "Episode: 17, Step: 74\n",
      "Next Action: [-0.523\n",
      "Step reward: -15.893011997020585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1184.9232442255357\n",
      "Episode: 17, Step: 75\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.900492589723534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1200.8237368152593\n",
      "Episode: 17, Step: 76\n",
      "Next Action: [-0.989\n",
      "Step reward: -15.898028859918114, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1216.7217656751775\n",
      "Episode: 17, Step: 77\n",
      "Next Action: [-1.215\n",
      "Step reward: -15.901426722164768, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1232.6231923973423\n",
      "Episode: 17, Step: 78\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.891659070344945, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1248.5148514676873\n",
      "Episode: 17, Step: 79\n",
      "Next Action: [-0.780\n",
      "Step reward: -15.8792445473405, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1264.3940960150278\n",
      "Episode: 17, Step: 80\n",
      "Next Action: [-0.829\n",
      "Step reward: -15.86895825390543, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1280.2630542689333\n",
      "Episode: 17, Step: 81\n",
      "Next Action: [-0.963\n",
      "Step reward: -15.857996479635977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1296.1210507485694\n",
      "Episode: 17, Step: 82\n",
      "Next Action: [-1.254\n",
      "Step reward: -15.878429615420975, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1311.9994803639904\n",
      "Episode: 17, Step: 83\n",
      "Next Action: [-1.249\n",
      "Step reward: -15.889730396792036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1327.8892107607824\n",
      "Episode: 17, Step: 84\n",
      "Next Action: [-1.355\n",
      "Step reward: -15.878514093470164, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1343.7677248542525\n",
      "Episode: 17, Step: 85\n",
      "Next Action: [-1.538\n",
      "Step reward: -15.912374623355438, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1359.6800994776079\n",
      "Episode: 17, Step: 86\n",
      "Next Action: [-1.147\n",
      "Step reward: -15.897240611632975, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1375.5773400892408\n",
      "Episode: 17, Step: 87\n",
      "Next Action: [-1.553\n",
      "Step reward: -15.92679290024102, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1391.5041329894818\n",
      "Episode: 17, Step: 88\n",
      "Next Action: [-1.507\n",
      "Step reward: -15.954882834749041, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1407.4590158242308\n",
      "Episode: 17, Step: 89\n",
      "Next Action: [-1.482\n",
      "Step reward: -15.970027136872794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1423.4290429611037\n",
      "Episode: 17, Step: 90\n",
      "Next Action: [-1.641\n",
      "Step reward: -15.951465221751414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1439.3805081828552\n",
      "Episode: 17, Step: 91\n",
      "Next Action: [-1.113\n",
      "Step reward: -15.94379490983269, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1455.3243030926878\n",
      "Episode: 17, Step: 92\n",
      "Next Action: [-1.108\n",
      "Step reward: -15.930247701175563, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1471.2545507938635\n",
      "Episode: 17, Step: 93\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.935267360900772, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1487.1898181547642\n",
      "Episode: 17, Step: 94\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.94529341008557, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1503.1351115648497\n",
      "Episode: 17, Step: 95\n",
      "Next Action: [-1.383\n",
      "Step reward: -15.943998608044108, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1519.0791101728937\n",
      "Episode: 17, Step: 96\n",
      "Next Action: [-1.186\n",
      "Step reward: -15.942846563248539, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1535.0219567361423\n",
      "Episode: 17, Step: 97\n",
      "Next Action: [-1.507\n",
      "Step reward: -15.918068514445915, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1550.9400252505882\n",
      "Episode: 17, Step: 98\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.892027856958041, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1566.8320531075462\n",
      "Episode: 17, Step: 99\n",
      "Next Action: [-1.336\n",
      "Step reward: -15.882104729832438, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1582.7141578373787\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 56.980567932128906\n",
      "Actor loss: 60.66617965698242\n",
      "Critic loss: 3.2319183349609375\n",
      "Actor loss: 68.65586853027344\n",
      "Critic loss: 2.6556429862976074\n",
      "Actor loss: 98.40779113769531\n",
      "Critic loss: 10.589960098266602\n",
      "Actor loss: 101.73497009277344\n",
      "Critic loss: 31.135818481445312\n",
      "Actor loss: 84.63906860351562\n",
      "Critic loss: 3.4348056316375732\n",
      "Actor loss: 83.17601776123047\n",
      "Critic loss: 3.996124505996704\n",
      "Actor loss: 85.0345230102539\n",
      "Critic loss: 114.62688446044922\n",
      "Actor loss: 52.2661018371582\n",
      "Critic loss: 115.71990203857422\n",
      "Actor loss: 65.61796569824219\n",
      "Critic loss: 15.483617782592773\n",
      "Actor loss: 51.3618049621582\n",
      "Episode: 18\n",
      "Episode: 18, Step: 0\n",
      "Next Action: [-1.503\n",
      "Step reward: -11.69644741682178, Next State: [-1. \n",
      "Total episode reward: -11.69644741682178\n",
      "Episode: 18, Step: 1\n",
      "Next Action: [-1.746\n",
      "Step reward: -14.290559903691546, Next State: [-1.\n",
      "Total episode reward: -25.987007320513328\n",
      "Episode: 18, Step: 2\n",
      "Next Action: [-1.509\n",
      "Step reward: -15.120665920223354, Next State: [-1.\n",
      "Total episode reward: -41.10767324073668\n",
      "Episode: 18, Step: 3\n",
      "Next Action: [-1.370\n",
      "Step reward: -15.42503595357044, Next State: [-1. \n",
      "Total episode reward: -56.53270919430712\n",
      "Episode: 18, Step: 4\n",
      "Next Action: [-1.209\n",
      "Step reward: -15.546173981160056, Next State: [-1.\n",
      "Total episode reward: -72.07888317546717\n",
      "Episode: 18, Step: 5\n",
      "Next Action: [-1.313\n",
      "Step reward: -15.628415749588298, Next State: [-1.\n",
      "Total episode reward: -87.70729892505547\n",
      "Episode: 18, Step: 6\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.726393328437194, Next State: [-1.\n",
      "Total episode reward: -103.43369225349267\n",
      "Episode: 18, Step: 7\n",
      "Next Action: [-1.580\n",
      "Step reward: -15.759701726318713, Next State: [-1.\n",
      "Total episode reward: -119.19339397981138\n",
      "Episode: 18, Step: 8\n",
      "Next Action: [-1.595\n",
      "Step reward: -15.773702878036515, Next State: [-1.\n",
      "Total episode reward: -134.96709685784788\n",
      "Episode: 18, Step: 9\n",
      "Next Action: [-1.824\n",
      "Step reward: -15.827560394263694, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -150.79465725211156\n",
      "Episode: 18, Step: 10\n",
      "Next Action: [-1.547\n",
      "Step reward: -15.8670710091705, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -166.66172826128206\n",
      "Episode: 18, Step: 11\n",
      "Next Action: [-1.349\n",
      "Step reward: -15.881444634129307, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -182.54317289541137\n",
      "Episode: 18, Step: 12\n",
      "Next Action: [-1.275\n",
      "Step reward: -15.919840401400204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -198.46301329681157\n",
      "Episode: 18, Step: 13\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.901932095477216, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -214.36494539228877\n",
      "Episode: 18, Step: 14\n",
      "Next Action: [-1.285\n",
      "Step reward: -15.897837325907883, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -230.26278271819666\n",
      "Episode: 18, Step: 15\n",
      "Next Action: [-1.160\n",
      "Step reward: -15.851795692824032, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -246.1145784110207\n",
      "Episode: 18, Step: 16\n",
      "Next Action: [-1.370\n",
      "Step reward: -15.838879010814837, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -261.95345742183554\n",
      "Episode: 18, Step: 17\n",
      "Next Action: [-0.840\n",
      "Step reward: -15.837243065050183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -277.7907004868857\n",
      "Episode: 18, Step: 18\n",
      "Next Action: [-1.022\n",
      "Step reward: -15.873249740798373, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -293.6639502276841\n",
      "Episode: 18, Step: 19\n",
      "Next Action: [-1.044\n",
      "Step reward: -15.876944219165056, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -309.54089444684917\n",
      "Episode: 18, Step: 20\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.901740603642013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -325.44263505049116\n",
      "Episode: 18, Step: 21\n",
      "Next Action: [-1.234\n",
      "Step reward: -15.900414508670167, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -341.34304955916133\n",
      "Episode: 18, Step: 22\n",
      "Next Action: [-1.156\n",
      "Step reward: -15.862801150486549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -357.2058507096479\n",
      "Episode: 18, Step: 23\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.844285845328802, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -373.0501365549767\n",
      "Episode: 18, Step: 24\n",
      "Next Action: [-1.115\n",
      "Step reward: -15.830046287884713, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -388.8801828428614\n",
      "Episode: 18, Step: 25\n",
      "Next Action: [-0.838\n",
      "Step reward: -15.819999067465972, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -404.7001819103274\n",
      "Episode: 18, Step: 26\n",
      "Next Action: [-0.776\n",
      "Step reward: -15.808551055782763, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -420.50873296611013\n",
      "Episode: 18, Step: 27\n",
      "Next Action: [-0.674\n",
      "Step reward: -15.800996500340835, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -436.30972946645096\n",
      "Episode: 18, Step: 28\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.773213073789131, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -452.0829425402401\n",
      "Episode: 18, Step: 29\n",
      "Next Action: [-1.175\n",
      "Step reward: -15.781621668674019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -467.8645642089141\n",
      "Episode: 18, Step: 30\n",
      "Next Action: [-1.098\n",
      "Step reward: -15.777030442287774, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -483.64159465120184\n",
      "Episode: 18, Step: 31\n",
      "Next Action: [-1.125\n",
      "Step reward: -15.806700368173265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -499.4482950193751\n",
      "Episode: 18, Step: 32\n",
      "Next Action: [-0.995\n",
      "Step reward: -15.819598266653767, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -515.2678932860289\n",
      "Episode: 18, Step: 33\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.838011730439236, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -531.1059050164681\n",
      "Episode: 18, Step: 34\n",
      "Next Action: [-0.924\n",
      "Step reward: -15.909204067596953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -547.0151090840651\n",
      "Episode: 18, Step: 35\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.934757627697573, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -562.9498667117626\n",
      "Episode: 18, Step: 36\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.953103139321568, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -578.9029698510842\n",
      "Episode: 18, Step: 37\n",
      "Next Action: [-1.049\n",
      "Step reward: -15.957770612421571, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -594.8607404635057\n",
      "Episode: 18, Step: 38\n",
      "Next Action: [-1.492\n",
      "Step reward: -15.94469558301332, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -610.805436046519\n",
      "Episode: 18, Step: 39\n",
      "Next Action: [-1.627\n",
      "Step reward: -15.902382293647413, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -626.7078183401665\n",
      "Episode: 18, Step: 40\n",
      "Next Action: [-1.402\n",
      "Step reward: -15.871444798563225, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -642.5792631387296\n",
      "Episode: 18, Step: 41\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.874259571462778, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -658.4535227101924\n",
      "Episode: 18, Step: 42\n",
      "Next Action: [-1.588\n",
      "Step reward: -15.865309251700964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -674.3188319618934\n",
      "Episode: 18, Step: 43\n",
      "Next Action: [-1.436\n",
      "Step reward: -15.892480074087828, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -690.2113120359812\n",
      "Episode: 18, Step: 44\n",
      "Next Action: [-1.392\n",
      "Step reward: -15.877332345938365, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -706.0886443819196\n",
      "Episode: 18, Step: 45\n",
      "Next Action: [-1.276\n",
      "Step reward: -15.866679777927859, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -721.9553241598475\n",
      "Episode: 18, Step: 46\n",
      "Next Action: [-1.108\n",
      "Step reward: -15.866324020588442, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -737.8216481804359\n",
      "Episode: 18, Step: 47\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.847316045284037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -753.66896422572\n",
      "Episode: 18, Step: 48\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.855478823071714, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -769.5244430487917\n",
      "Episode: 18, Step: 49\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.855315659188541, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -785.3797587079803\n",
      "Episode: 18, Step: 50\n",
      "Next Action: [-0.764\n",
      "Step reward: -15.852464698786319, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -801.2322234067666\n",
      "Episode: 18, Step: 51\n",
      "Next Action: [-0.908\n",
      "Step reward: -15.842112405719194, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -817.0743358124857\n",
      "Episode: 18, Step: 52\n",
      "Next Action: [-0.724\n",
      "Step reward: -15.825692344951412, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -832.9000281574372\n",
      "Episode: 18, Step: 53\n",
      "Next Action: [-0.683\n",
      "Step reward: -15.82279150132657, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -848.7228196587638\n",
      "Episode: 18, Step: 54\n",
      "Next Action: [-8.168\n",
      "Step reward: -15.866864444647607, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -864.5896841034114\n",
      "Episode: 18, Step: 55\n",
      "Next Action: [-1.082\n",
      "Step reward: -15.871342102695255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -880.4610262061067\n",
      "Episode: 18, Step: 56\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.870686532673945, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -896.3317127387807\n",
      "Episode: 18, Step: 57\n",
      "Next Action: [-0.987\n",
      "Step reward: -15.843336701147281, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -912.175049439928\n",
      "Episode: 18, Step: 58\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.813167447142671, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -927.9882168870706\n",
      "Episode: 18, Step: 59\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.806965125513885, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -943.7951820125845\n",
      "Episode: 18, Step: 60\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.835897063743419, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -959.631079076328\n",
      "Episode: 18, Step: 61\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.873151148797817, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -975.5042302251258\n",
      "Episode: 18, Step: 62\n",
      "Next Action: [-0.852\n",
      "Step reward: -15.895103754814734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -991.3993339799405\n",
      "Episode: 18, Step: 63\n",
      "Next Action: [-0.713\n",
      "Step reward: -15.89417931111285, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1007.2935132910534\n",
      "Episode: 18, Step: 64\n",
      "Next Action: [-0.785\n",
      "Step reward: -15.897053297429313, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1023.1905665884827\n",
      "Episode: 18, Step: 65\n",
      "Next Action: [-0.814\n",
      "Step reward: -15.921387868393625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1039.1119544568764\n",
      "Episode: 18, Step: 66\n",
      "Next Action: [-0.857\n",
      "Step reward: -15.9018539730328, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1055.0138084299092\n",
      "Episode: 18, Step: 67\n",
      "Next Action: [-0.595\n",
      "Step reward: -15.894479586739951, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1070.908288016649\n",
      "Episode: 18, Step: 68\n",
      "Next Action: [-0.607\n",
      "Step reward: -15.89140187152215, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1086.7996898881713\n",
      "Episode: 18, Step: 69\n",
      "Next Action: [-0.464\n",
      "Step reward: -15.887931701419575, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1102.687621589591\n",
      "Episode: 18, Step: 70\n",
      "Next Action: [-0.324\n",
      "Step reward: -15.882282689862937, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1118.5699042794538\n",
      "Episode: 18, Step: 71\n",
      "Next Action: [-0.669\n",
      "Step reward: -15.863283506505937, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1134.4331877859597\n",
      "Episode: 18, Step: 72\n",
      "Next Action: [-0.637\n",
      "Step reward: -15.863100852998349, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1150.296288638958\n",
      "Episode: 18, Step: 73\n",
      "Next Action: [-0.926\n",
      "Step reward: -15.870219117989448, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1166.1665077569473\n",
      "Episode: 18, Step: 74\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.871910117433144, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1182.0384178743805\n",
      "Episode: 18, Step: 75\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.85403505487625, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1197.8924529292567\n",
      "Episode: 18, Step: 76\n",
      "Next Action: [-0.693\n",
      "Step reward: -15.851364488854726, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1213.7438174181113\n",
      "Episode: 18, Step: 77\n",
      "Next Action: [-9.615\n",
      "Step reward: -15.83013356071043, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1229.5739509788218\n",
      "Episode: 18, Step: 78\n",
      "Next Action: [-0.755\n",
      "Step reward: -15.819693681624571, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1245.3936446604464\n",
      "Episode: 18, Step: 79\n",
      "Next Action: [-0.675\n",
      "Step reward: -15.830105168547535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1261.223749828994\n",
      "Episode: 18, Step: 80\n",
      "Next Action: [-0.570\n",
      "Step reward: -15.908172583870803, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1277.1319224128647\n",
      "Episode: 18, Step: 81\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.916127941098773, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1293.0480503539634\n",
      "Episode: 18, Step: 82\n",
      "Next Action: [-0.715\n",
      "Step reward: -15.930051706037549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1308.978102060001\n",
      "Episode: 18, Step: 83\n",
      "Next Action: [-0.621\n",
      "Step reward: -15.972000808010971, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1324.950102868012\n",
      "Episode: 18, Step: 84\n",
      "Next Action: [-0.648\n",
      "Step reward: -15.952661703943432, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1340.9027645719555\n",
      "Episode: 18, Step: 85\n",
      "Next Action: [-0.633\n",
      "Step reward: -15.956238859106428, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1356.8590034310619\n",
      "Episode: 18, Step: 86\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.959964130374855, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1372.8189675614367\n",
      "Episode: 18, Step: 87\n",
      "Next Action: [-0.976\n",
      "Step reward: -15.966665546154816, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1388.7856331075916\n",
      "Episode: 18, Step: 88\n",
      "Next Action: [-1.247\n",
      "Step reward: -15.938339948171603, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1404.7239730557633\n",
      "Episode: 18, Step: 89\n",
      "Next Action: [-1.435\n",
      "Step reward: -15.932744307612245, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1420.6567173633755\n",
      "Episode: 18, Step: 90\n",
      "Next Action: [-1.397\n",
      "Step reward: -15.874255420709709, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1436.5309727840852\n",
      "Episode: 18, Step: 91\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.861315990243574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1452.3922887743288\n",
      "Episode: 18, Step: 92\n",
      "Next Action: [-1.284\n",
      "Step reward: -15.86632175757805, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1468.258610531907\n",
      "Episode: 18, Step: 93\n",
      "Next Action: [-1.396\n",
      "Step reward: -15.868202257484748, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1484.1268127893916\n",
      "Episode: 18, Step: 94\n",
      "Next Action: [-1.151\n",
      "Step reward: -15.901341809219067, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1500.0281545986106\n",
      "Episode: 18, Step: 95\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.893713081805462, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1515.9218676804162\n",
      "Episode: 18, Step: 96\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.906236117214466, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1531.8281037976308\n",
      "Episode: 18, Step: 97\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.909729504738584, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1547.7378333023694\n",
      "Episode: 18, Step: 98\n",
      "Next Action: [-1.228\n",
      "Step reward: -15.936616882946753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1563.6744501853161\n",
      "Episode: 18, Step: 99\n",
      "Next Action: [-1.391\n",
      "Step reward: -15.943057430995465, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1579.6175076163115\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 4.135210037231445\n",
      "Actor loss: 63.0131721496582\n",
      "Critic loss: 6.908352851867676\n",
      "Actor loss: 45.482696533203125\n",
      "Critic loss: 69.02368927001953\n",
      "Actor loss: 48.187171936035156\n",
      "Critic loss: 5.292366981506348\n",
      "Actor loss: 54.6345329284668\n",
      "Critic loss: 38.57566833496094\n",
      "Actor loss: 43.10167694091797\n",
      "Critic loss: 49.448280334472656\n",
      "Actor loss: 29.788009643554688\n",
      "Critic loss: 16.494827270507812\n",
      "Actor loss: 35.959800720214844\n",
      "Critic loss: 19.704851150512695\n",
      "Actor loss: 27.559717178344727\n",
      "Critic loss: 78.96271514892578\n",
      "Actor loss: 21.19194984436035\n",
      "Critic loss: 28.08858871459961\n",
      "Actor loss: 28.134525299072266\n",
      "Episode: 19\n",
      "Episode: 19, Step: 0\n",
      "Next Action: [-0.894\n",
      "Step reward: -11.909829293721977, Next State: [-1.\n",
      "Total episode reward: -11.909829293721977\n",
      "Episode: 19, Step: 1\n",
      "Next Action: [-1.134\n",
      "Step reward: -14.72604178674797, Next State: [-1. \n",
      "Total episode reward: -26.635871080469947\n",
      "Episode: 19, Step: 2\n",
      "Next Action: [-1.234\n",
      "Step reward: -15.440919077462334, Next State: [-1.\n",
      "Total episode reward: -42.07679015793228\n",
      "Episode: 19, Step: 3\n",
      "Next Action: [-1.027\n",
      "Step reward: -15.714492370639174, Next State: [-1.\n",
      "Total episode reward: -57.79128252857146\n",
      "Episode: 19, Step: 4\n",
      "Next Action: [-1.069\n",
      "Step reward: -15.794992518371547, Next State: [-1.\n",
      "Total episode reward: -73.586275046943\n",
      "Episode: 19, Step: 5\n",
      "Next Action: [-0.989\n",
      "Step reward: -15.842285453956201, Next State: [-1.\n",
      "Total episode reward: -89.42856050089921\n",
      "Episode: 19, Step: 6\n",
      "Next Action: [-0.664\n",
      "Step reward: -15.906743938547176, Next State: [-1.\n",
      "Total episode reward: -105.33530443944639\n",
      "Episode: 19, Step: 7\n",
      "Next Action: [-0.590\n",
      "Step reward: -15.91136102963296, Next State: [-1. \n",
      "Total episode reward: -121.24666546907935\n",
      "Episode: 19, Step: 8\n",
      "Next Action: [-0.657\n",
      "Step reward: -15.928926455445184, Next State: [-1.\n",
      "Total episode reward: -137.17559192452453\n",
      "Episode: 19, Step: 9\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.954448816049446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.13004074057397\n",
      "Episode: 19, Step: 10\n",
      "Next Action: [-0.732\n",
      "Step reward: -15.985017002011828, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.1150577425858\n",
      "Episode: 19, Step: 11\n",
      "Next Action: [-0.430\n",
      "Step reward: -15.993109315538563, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.10816705812437\n",
      "Episode: 19, Step: 12\n",
      "Next Action: [-0.661\n",
      "Step reward: -15.99764444025605, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.1058114983804\n",
      "Episode: 19, Step: 13\n",
      "Next Action: [-0.560\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.1058114983804\n",
      "Episode: 19, Step: 14\n",
      "Next Action: [-0.453\n",
      "Step reward: -15.981816661372504, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.0876281597529\n",
      "Episode: 19, Step: 15\n",
      "Next Action: [-0.600\n",
      "Step reward: -15.953247752703378, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.0408759124563\n",
      "Episode: 19, Step: 16\n",
      "Next Action: [-0.686\n",
      "Step reward: -15.902612952823736, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.94348886528\n",
      "Episode: 19, Step: 17\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.853457474014052, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.7969463392941\n",
      "Episode: 19, Step: 18\n",
      "Next Action: [-1.237\n",
      "Step reward: -15.834046229553378, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.6309925688475\n",
      "Episode: 19, Step: 19\n",
      "Next Action: [-1.663\n",
      "Step reward: -15.865726640006583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.4967192088541\n",
      "Episode: 19, Step: 20\n",
      "Next Action: [-1.285\n",
      "Step reward: -15.868666661173542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.36538587002764\n",
      "Episode: 19, Step: 21\n",
      "Next Action: [-1.614\n",
      "Step reward: -15.869837232868415, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.23522310289604\n",
      "Episode: 19, Step: 22\n",
      "Next Action: [-1.560\n",
      "Step reward: -15.90945997403697, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.144683076933\n",
      "Episode: 19, Step: 23\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.91131623354057, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.05599931047357\n",
      "Episode: 19, Step: 24\n",
      "Next Action: [-1.410\n",
      "Step reward: -15.90880071627083, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.9648000267444\n",
      "Episode: 19, Step: 25\n",
      "Next Action: [-1.221\n",
      "Step reward: -15.916318713949257, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.88111874069364\n",
      "Episode: 19, Step: 26\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.937745025718197, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.8188637664118\n",
      "Episode: 19, Step: 27\n",
      "Next Action: [-0.675\n",
      "Step reward: -15.937588567623035, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.75645233403486\n",
      "Episode: 19, Step: 28\n",
      "Next Action: [-0.793\n",
      "Step reward: -15.939529415819035, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.6959817498539\n",
      "Episode: 19, Step: 29\n",
      "Next Action: [-0.853\n",
      "Step reward: -15.934010247541352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.6299919973952\n",
      "Episode: 19, Step: 30\n",
      "Next Action: [-0.865\n",
      "Step reward: -15.94577134509021, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.57576334248546\n",
      "Episode: 19, Step: 31\n",
      "Next Action: [-1.160\n",
      "Step reward: -15.902386740569662, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.47815008305514\n",
      "Episode: 19, Step: 32\n",
      "Next Action: [-0.981\n",
      "Step reward: -15.889010494998717, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.3671605780538\n",
      "Episode: 19, Step: 33\n",
      "Next Action: [-0.842\n",
      "Step reward: -15.876761931973418, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.2439225100272\n",
      "Episode: 19, Step: 34\n",
      "Next Action: [-1.052\n",
      "Step reward: -15.867919154623308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.1118416646506\n",
      "Episode: 19, Step: 35\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.844208046807069, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -566.9560497114576\n",
      "Episode: 19, Step: 36\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.827176664842105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -582.7832263762997\n",
      "Episode: 19, Step: 37\n",
      "Next Action: [-0.462\n",
      "Step reward: -15.846995286465196, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -598.6302216627649\n",
      "Episode: 19, Step: 38\n",
      "Next Action: [-5.092\n",
      "Step reward: -15.874890305483493, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -614.5051119682485\n",
      "Episode: 19, Step: 39\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.88706497322939, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -630.3921769414778\n",
      "Episode: 19, Step: 40\n",
      "Next Action: [-0.984\n",
      "Step reward: -15.917839320118189, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -646.310016261596\n",
      "Episode: 19, Step: 41\n",
      "Next Action: [-0.844\n",
      "Step reward: -15.955257803588545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -662.2652740651846\n",
      "Episode: 19, Step: 42\n",
      "Next Action: [-0.450\n",
      "Step reward: -15.906210476843103, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -678.1714845420277\n",
      "Episode: 19, Step: 43\n",
      "Next Action: [-0.497\n",
      "Step reward: -15.887824616506569, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -694.0593091585342\n",
      "Episode: 19, Step: 44\n",
      "Next Action: [-0.670\n",
      "Step reward: -15.908582578576091, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -709.9678917371103\n",
      "Episode: 19, Step: 45\n",
      "Next Action: [-0.953\n",
      "Step reward: -15.9254343425464, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -725.8933260796567\n",
      "Episode: 19, Step: 46\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.932636763287723, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -741.8259628429445\n",
      "Episode: 19, Step: 47\n",
      "Next Action: [-0.625\n",
      "Step reward: -15.949779442652495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -757.775742285597\n",
      "Episode: 19, Step: 48\n",
      "Next Action: [-0.614\n",
      "Step reward: -15.95048410337502, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -773.726226388972\n",
      "Episode: 19, Step: 49\n",
      "Next Action: [-0.636\n",
      "Step reward: -15.971984933650708, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -789.6982113226227\n",
      "Episode: 19, Step: 50\n",
      "Next Action: [-0.929\n",
      "Step reward: -15.983251976831145, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -805.6814632994539\n",
      "Episode: 19, Step: 51\n",
      "Next Action: [-1.517\n",
      "Step reward: -15.987773551418876, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -821.6692368508727\n",
      "Episode: 19, Step: 52\n",
      "Next Action: [-1.497\n",
      "Step reward: -15.992423529413132, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -837.6616603802859\n",
      "Episode: 19, Step: 53\n",
      "Next Action: [-1.496\n",
      "Step reward: -15.969747385385668, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -853.6314077656715\n",
      "Episode: 19, Step: 54\n",
      "Next Action: [-1.107\n",
      "Step reward: -15.9642901447677, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -869.5956979104392\n",
      "Episode: 19, Step: 55\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.962947894240836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -885.55864580468\n",
      "Episode: 19, Step: 56\n",
      "Next Action: [-1.034\n",
      "Step reward: -15.97904898381505, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -901.5376947884951\n",
      "Episode: 19, Step: 57\n",
      "Next Action: [-1.000\n",
      "Step reward: -15.956358379940625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -917.4940531684357\n",
      "Episode: 19, Step: 58\n",
      "Next Action: [-1.175\n",
      "Step reward: -15.92776594621311, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -933.4218191146488\n",
      "Episode: 19, Step: 59\n",
      "Next Action: [-1.388\n",
      "Step reward: -15.921014650769653, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -949.3428337654184\n",
      "Episode: 19, Step: 60\n",
      "Next Action: [-1.517\n",
      "Step reward: -15.910723895743022, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -965.2535576611614\n",
      "Episode: 19, Step: 61\n",
      "Next Action: [-1.332\n",
      "Step reward: -15.896773388225135, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -981.1503310493865\n",
      "Episode: 19, Step: 62\n",
      "Next Action: [-1.213\n",
      "Step reward: -15.89271310349217, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -997.0430441528787\n",
      "Episode: 19, Step: 63\n",
      "Next Action: [-1.052\n",
      "Step reward: -15.889039643654915, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1012.9320837965336\n",
      "Episode: 19, Step: 64\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.907784280482334, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1028.8398680770158\n",
      "Episode: 19, Step: 65\n",
      "Next Action: [-1.022\n",
      "Step reward: -15.90825157509782, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1044.7481196521137\n",
      "Episode: 19, Step: 66\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.934894528759804, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1060.6830141808734\n",
      "Episode: 19, Step: 67\n",
      "Next Action: [-0.937\n",
      "Step reward: -15.938622478806908, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1076.6216366596805\n",
      "Episode: 19, Step: 68\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.950466997504728, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1092.5721036571852\n",
      "Episode: 19, Step: 69\n",
      "Next Action: [-0.885\n",
      "Step reward: -15.951077724666602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1108.5231813818518\n",
      "Episode: 19, Step: 70\n",
      "Next Action: [-1.026\n",
      "Step reward: -15.94238689670592, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1124.4655682785578\n",
      "Episode: 19, Step: 71\n",
      "Next Action: [-1.011\n",
      "Step reward: -15.936820509991662, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1140.4023887885496\n",
      "Episode: 19, Step: 72\n",
      "Next Action: [-0.802\n",
      "Step reward: -15.975701893007585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1156.3780906815573\n",
      "Episode: 19, Step: 73\n",
      "Next Action: [-0.688\n",
      "Step reward: -15.94913146459891, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1172.3272221461561\n",
      "Episode: 19, Step: 74\n",
      "Next Action: [-0.926\n",
      "Step reward: -15.91337177116479, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1188.240593917321\n",
      "Episode: 19, Step: 75\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.887263113242254, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1204.127857030563\n",
      "Episode: 19, Step: 76\n",
      "Next Action: [-1.288\n",
      "Step reward: -15.891958771413265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1220.0198158019764\n",
      "Episode: 19, Step: 77\n",
      "Next Action: [-1.034\n",
      "Step reward: -15.862700120912397, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1235.8825159228888\n",
      "Episode: 19, Step: 78\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.88112676914359, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1251.7636426920324\n",
      "Episode: 19, Step: 79\n",
      "Next Action: [-0.698\n",
      "Step reward: -15.90079077218395, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1267.6644334642162\n",
      "Episode: 19, Step: 80\n",
      "Next Action: [-1.050\n",
      "Step reward: -15.90774714799983, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1283.572180612216\n",
      "Episode: 19, Step: 81\n",
      "Next Action: [-1.203\n",
      "Step reward: -15.930090541594815, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1299.5022711538109\n",
      "Episode: 19, Step: 82\n",
      "Next Action: [-1.657\n",
      "Step reward: -15.9690549683646, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1315.4713261221755\n",
      "Episode: 19, Step: 83\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.981243019109252, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1331.4525691412848\n",
      "Episode: 19, Step: 84\n",
      "Next Action: [-1.478\n",
      "Step reward: -15.998543026986841, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1347.4511121682717\n",
      "Episode: 19, Step: 85\n",
      "Next Action: [-1.467\n",
      "Step reward: -15.988415490962772, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1363.4395276592345\n",
      "Episode: 19, Step: 86\n",
      "Next Action: [-1.421\n",
      "Step reward: -15.932883068947328, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1379.3724107281819\n",
      "Episode: 19, Step: 87\n",
      "Next Action: [-1.743\n",
      "Step reward: -15.943706566575395, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1395.3161172947573\n",
      "Episode: 19, Step: 88\n",
      "Next Action: [-1.860\n",
      "Step reward: -15.953002934536592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1411.269120229294\n",
      "Episode: 19, Step: 89\n",
      "Next Action: [-1.612\n",
      "Step reward: -15.915117837834735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1427.1842380671287\n",
      "Episode: 19, Step: 90\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.929951416834543, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1443.1141894839632\n",
      "Episode: 19, Step: 91\n",
      "Next Action: [-0.982\n",
      "Step reward: -15.905024825461792, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1459.019214309425\n",
      "Episode: 19, Step: 92\n",
      "Next Action: [-1.208\n",
      "Step reward: -15.885280171110768, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1474.9044944805357\n",
      "Episode: 19, Step: 93\n",
      "Next Action: [-1.476\n",
      "Step reward: -15.868721903523788, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1490.7732163840594\n",
      "Episode: 19, Step: 94\n",
      "Next Action: [-1.545\n",
      "Step reward: -15.842661729579625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1506.6158781136392\n",
      "Episode: 19, Step: 95\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.866837758953269, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1522.4827158725925\n",
      "Episode: 19, Step: 96\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.865418609447293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1538.3481344820398\n",
      "Episode: 19, Step: 97\n",
      "Next Action: [-1.372\n",
      "Step reward: -15.887587725692452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1554.2357222077321\n",
      "Episode: 19, Step: 98\n",
      "Next Action: [-1.161\n",
      "Step reward: -15.886327987179099, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1570.1220501949113\n",
      "Episode: 19, Step: 99\n",
      "Next Action: [-1.397\n",
      "Step reward: -15.88250175170998, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1586.0045519466212\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 17.416698455810547\n",
      "Actor loss: 37.47568893432617\n",
      "Critic loss: 30.226896286010742\n",
      "Actor loss: 29.028833389282227\n",
      "Critic loss: 14.806522369384766\n",
      "Actor loss: 41.792449951171875\n",
      "Critic loss: 7.361145496368408\n",
      "Actor loss: 35.28837966918945\n",
      "Critic loss: 3.407607078552246\n",
      "Actor loss: 40.46535873413086\n",
      "Critic loss: 69.22344970703125\n",
      "Actor loss: 27.45846939086914\n",
      "Critic loss: 4.182962894439697\n",
      "Actor loss: 52.93646240234375\n",
      "Critic loss: 3.445462703704834\n",
      "Actor loss: 39.735252380371094\n",
      "Critic loss: 3.5426135063171387\n",
      "Actor loss: 58.11322784423828\n",
      "Critic loss: 6.7435479164123535\n",
      "Actor loss: 53.98663330078125\n",
      "Episode: 20\n",
      "Episode: 20, Step: 0\n",
      "Next Action: [-0.841\n",
      "Step reward: -12.02323038954193, Next State: [-1. \n",
      "Total episode reward: -12.02323038954193\n",
      "Episode: 20, Step: 1\n",
      "Next Action: [-0.940\n",
      "Step reward: -14.892624844672785, Next State: [-1.\n",
      "Total episode reward: -26.915855234214714\n",
      "Episode: 20, Step: 2\n",
      "Next Action: [-0.730\n",
      "Step reward: -15.568061753150854, Next State: [-1.\n",
      "Total episode reward: -42.48391698736557\n",
      "Episode: 20, Step: 3\n",
      "Next Action: [-0.552\n",
      "Step reward: -15.725561089574523, Next State: [-1.\n",
      "Total episode reward: -58.20947807694009\n",
      "Episode: 20, Step: 4\n",
      "Next Action: [-0.857\n",
      "Step reward: -15.83139611950686, Next State: [-1. \n",
      "Total episode reward: -74.04087419644695\n",
      "Episode: 20, Step: 5\n",
      "Next Action: [-0.270\n",
      "Step reward: -15.887434477289869, Next State: [-1.\n",
      "Total episode reward: -89.92830867373682\n",
      "Episode: 20, Step: 6\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.930674472785377, Next State: [-1.\n",
      "Total episode reward: -105.8589831465222\n",
      "Episode: 20, Step: 7\n",
      "Next Action: [-0.643\n",
      "Step reward: -15.908947756767363, Next State: [-1.\n",
      "Total episode reward: -121.76793090328957\n",
      "Episode: 20, Step: 8\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.865326240829205, Next State: [-1.\n",
      "Total episode reward: -137.63325714411877\n",
      "Episode: 20, Step: 9\n",
      "Next Action: [-1.111\n",
      "Step reward: -15.829468331845932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.4627254759647\n",
      "Episode: 20, Step: 10\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.84575000441742, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.30847548038213\n",
      "Episode: 20, Step: 11\n",
      "Next Action: [-1.113\n",
      "Step reward: -15.866449046980113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.17492452736224\n",
      "Episode: 20, Step: 12\n",
      "Next Action: [-0.822\n",
      "Step reward: -15.900566050832545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.07549057819477\n",
      "Episode: 20, Step: 13\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.936487206495727, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.0119777846905\n",
      "Episode: 20, Step: 14\n",
      "Next Action: [-1.083\n",
      "Step reward: -15.911297060445907, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.9232748451364\n",
      "Episode: 20, Step: 15\n",
      "Next Action: [-0.997\n",
      "Step reward: -15.888985972517448, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.81226081765385\n",
      "Episode: 20, Step: 16\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.918908521073908, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.73116933872774\n",
      "Episode: 20, Step: 17\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.906018941477852, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.6371882802056\n",
      "Episode: 20, Step: 18\n",
      "Next Action: [-0.920\n",
      "Step reward: -15.902828890754476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.54001717096\n",
      "Episode: 20, Step: 19\n",
      "Next Action: [-0.439\n",
      "Step reward: -15.913917421265468, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.4539345922255\n",
      "Episode: 20, Step: 20\n",
      "Next Action: [-0.201\n",
      "Step reward: -15.878013767421189, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.3319483596467\n",
      "Episode: 20, Step: 21\n",
      "Next Action: [-0.435\n",
      "Step reward: -15.865938790893066, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.1978871505397\n",
      "Episode: 20, Step: 22\n",
      "Next Action: [-0.404\n",
      "Step reward: -15.849458767565332, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.04734591810507\n",
      "Episode: 20, Step: 23\n",
      "Next Action: [-0.524\n",
      "Step reward: -15.867294140146647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.91464005825173\n",
      "Episode: 20, Step: 24\n",
      "Next Action: [-0.542\n",
      "Step reward: -15.89285645313786, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.8074965113896\n",
      "Episode: 20, Step: 25\n",
      "Next Action: [-0.697\n",
      "Step reward: -15.923907320157777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.73140383154737\n",
      "Episode: 20, Step: 26\n",
      "Next Action: [-0.509\n",
      "Step reward: -15.94506585910784, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.6764696906552\n",
      "Episode: 20, Step: 27\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.950908897712653, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.6273785883679\n",
      "Episode: 20, Step: 28\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.958297275680833, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.5856758640487\n",
      "Episode: 20, Step: 29\n",
      "Next Action: [-1.041\n",
      "Step reward: -15.970508182530322, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.556184046579\n",
      "Episode: 20, Step: 30\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.95463144037645, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.51081548695544\n",
      "Episode: 20, Step: 31\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.956755053623366, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.4675705405788\n",
      "Episode: 20, Step: 32\n",
      "Next Action: [-1.111\n",
      "Step reward: -15.97509856901224, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.4426691095911\n",
      "Episode: 20, Step: 33\n",
      "Next Action: [-1.031\n",
      "Step reward: -15.95130509040323, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.3939741999943\n",
      "Episode: 20, Step: 34\n",
      "Next Action: [-1.267\n",
      "Step reward: -15.932894045127139, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.3268682451214\n",
      "Episode: 20, Step: 35\n",
      "Next Action: [-1.483\n",
      "Step reward: -15.942313944864665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -567.269182189986\n",
      "Episode: 20, Step: 36\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.960015515800254, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -583.2291977057863\n",
      "Episode: 20, Step: 37\n",
      "Next Action: [-1.067\n",
      "Step reward: -15.96625071984328, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -599.1954484256296\n",
      "Episode: 20, Step: 38\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.982673406438083, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -615.1781218320676\n",
      "Episode: 20, Step: 39\n",
      "Next Action: [-1.337\n",
      "Step reward: -15.98421974393881, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -631.1623415760065\n",
      "Episode: 20, Step: 40\n",
      "Next Action: [-1.455\n",
      "Step reward: -15.989390167925212, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -647.1517317439317\n",
      "Episode: 20, Step: 41\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.974455948984977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -663.1261876929167\n",
      "Episode: 20, Step: 42\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.949770417812124, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -679.0759581107288\n",
      "Episode: 20, Step: 43\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.934455273557274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -695.0104133842862\n",
      "Episode: 20, Step: 44\n",
      "Next Action: [-0.985\n",
      "Step reward: -15.955935798374174, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -710.9663491826603\n",
      "Episode: 20, Step: 45\n",
      "Next Action: [-0.964\n",
      "Step reward: -15.97996383306864, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -726.946313015729\n",
      "Episode: 20, Step: 46\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.986099571786264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -742.9324125875152\n",
      "Episode: 20, Step: 47\n",
      "Next Action: [-1.302\n",
      "Step reward: -15.977718099098382, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -758.9101306866136\n",
      "Episode: 20, Step: 48\n",
      "Next Action: [-1.069\n",
      "Step reward: -15.9735379305825, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -774.8836686171961\n",
      "Episode: 20, Step: 49\n",
      "Next Action: [-1.197\n",
      "Step reward: -15.952002819816878, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -790.835671437013\n",
      "Episode: 20, Step: 50\n",
      "Next Action: [-1.060\n",
      "Step reward: -15.925409496571644, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -806.7610809335847\n",
      "Episode: 20, Step: 51\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.923434071831755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -822.6845150054164\n",
      "Episode: 20, Step: 52\n",
      "Next Action: [-1.061\n",
      "Step reward: -15.944641309652278, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -838.6291563150687\n",
      "Episode: 20, Step: 53\n",
      "Next Action: [-1.167\n",
      "Step reward: -15.95238629853102, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -854.5815426135997\n",
      "Episode: 20, Step: 54\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.968571377199414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -870.5501139907991\n",
      "Episode: 20, Step: 55\n",
      "Next Action: [-1.058\n",
      "Step reward: -15.985506582931938, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -886.5356205737311\n",
      "Episode: 20, Step: 56\n",
      "Next Action: [-0.872\n",
      "Step reward: -15.994452024394803, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -902.5300725981259\n",
      "Episode: 20, Step: 57\n",
      "Next Action: [-1.075\n",
      "Step reward: -15.982532425743225, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -918.5126050238691\n",
      "Episode: 20, Step: 58\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.948639039597243, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -934.4612440634663\n",
      "Episode: 20, Step: 59\n",
      "Next Action: [-0.927\n",
      "Step reward: -15.914620420059833, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -950.3758644835261\n",
      "Episode: 20, Step: 60\n",
      "Next Action: [-1.015\n",
      "Step reward: -15.916281309085734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -966.2921457926119\n",
      "Episode: 20, Step: 61\n",
      "Next Action: [-1.114\n",
      "Step reward: -15.944065807292187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -982.2362115999041\n",
      "Episode: 20, Step: 62\n",
      "Next Action: [-1.209\n",
      "Step reward: -15.949944867213768, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -998.1861564671178\n",
      "Episode: 20, Step: 63\n",
      "Next Action: [-1.159\n",
      "Step reward: -15.940933071444851, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1014.1270895385627\n",
      "Episode: 20, Step: 64\n",
      "Next Action: [-1.506\n",
      "Step reward: -15.946679801708036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1030.0737693402707\n",
      "Episode: 20, Step: 65\n",
      "Next Action: [-1.502\n",
      "Step reward: -15.944815538304702, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.0185848785754\n",
      "Episode: 20, Step: 66\n",
      "Next Action: [-1.289\n",
      "Step reward: -15.932180367630757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1061.9507652462062\n",
      "Episode: 20, Step: 67\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.9072159208405, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1077.8579811670468\n",
      "Episode: 20, Step: 68\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.897204161183064, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1093.7551853282298\n",
      "Episode: 20, Step: 69\n",
      "Next Action: [-1.550\n",
      "Step reward: -15.92146943585268, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1109.6766547640825\n",
      "Episode: 20, Step: 70\n",
      "Next Action: [-1.447\n",
      "Step reward: -15.927013419076875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1125.6036681831595\n",
      "Episode: 20, Step: 71\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.934780558113927, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1141.5384487412734\n",
      "Episode: 20, Step: 72\n",
      "Next Action: [-1.319\n",
      "Step reward: -15.938475355243305, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1157.4769240965168\n",
      "Episode: 20, Step: 73\n",
      "Next Action: [-1.257\n",
      "Step reward: -15.943145575180495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1173.4200696716973\n",
      "Episode: 20, Step: 74\n",
      "Next Action: [-1.208\n",
      "Step reward: -15.941753785161529, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1189.361823456859\n",
      "Episode: 20, Step: 75\n",
      "Next Action: [-0.904\n",
      "Step reward: -15.95197632365874, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1205.3137997805177\n",
      "Episode: 20, Step: 76\n",
      "Next Action: [-1.239\n",
      "Step reward: -15.944317906364713, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1221.2581176868823\n",
      "Episode: 20, Step: 77\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.948373893624298, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1237.2064915805065\n",
      "Episode: 20, Step: 78\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.941437537233911, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1253.1479291177404\n",
      "Episode: 20, Step: 79\n",
      "Next Action: [-1.467\n",
      "Step reward: -15.956958860035096, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1269.1048879777754\n",
      "Episode: 20, Step: 80\n",
      "Next Action: [-1.527\n",
      "Step reward: -15.959796520794782, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1285.0646844985702\n",
      "Episode: 20, Step: 81\n",
      "Next Action: [-1.391\n",
      "Step reward: -15.96403514408373, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1301.0287196426539\n",
      "Episode: 20, Step: 82\n",
      "Next Action: [-1.441\n",
      "Step reward: -15.953063364291635, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1316.9817830069455\n",
      "Episode: 20, Step: 83\n",
      "Next Action: [-1.405\n",
      "Step reward: -15.948358447907008, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1332.9301414548524\n",
      "Episode: 20, Step: 84\n",
      "Next Action: [-1.239\n",
      "Step reward: -15.956791063536963, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1348.8869325183894\n",
      "Episode: 20, Step: 85\n",
      "Next Action: [-1.362\n",
      "Step reward: -15.948644662367807, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1364.8355771807571\n",
      "Episode: 20, Step: 86\n",
      "Next Action: [-1.492\n",
      "Step reward: -15.946610571032792, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1380.78218775179\n",
      "Episode: 20, Step: 87\n",
      "Next Action: [-1.532\n",
      "Step reward: -15.938819910990464, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1396.7210076627805\n",
      "Episode: 20, Step: 88\n",
      "Next Action: [-1.412\n",
      "Step reward: -15.925907944464832, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1412.6469156072453\n",
      "Episode: 20, Step: 89\n",
      "Next Action: [-1.227\n",
      "Step reward: -15.92785261108639, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1428.5747682183317\n",
      "Episode: 20, Step: 90\n",
      "Next Action: [-1.182\n",
      "Step reward: -15.944899637598127, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1444.5196678559298\n",
      "Episode: 20, Step: 91\n",
      "Next Action: [-0.978\n",
      "Step reward: -15.959560446411023, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1460.4792283023407\n",
      "Episode: 20, Step: 92\n",
      "Next Action: [-1.033\n",
      "Step reward: -15.976100177943614, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1476.4553284802844\n",
      "Episode: 20, Step: 93\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.975351492039595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1492.430679972324\n",
      "Episode: 20, Step: 94\n",
      "Next Action: [-1.481\n",
      "Step reward: -15.977121515696926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1508.407801488021\n",
      "Episode: 20, Step: 95\n",
      "Next Action: [-1.297\n",
      "Step reward: -15.982522914161267, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1524.3903244021824\n",
      "Episode: 20, Step: 96\n",
      "Next Action: [-1.493\n",
      "Step reward: -15.994773790394477, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1540.3850981925768\n",
      "Episode: 20, Step: 97\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.966648175568883, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1556.3517463681458\n",
      "Episode: 20, Step: 98\n",
      "Next Action: [-1.459\n",
      "Step reward: -15.93825457699307, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1572.290000945139\n",
      "Episode: 20, Step: 99\n",
      "Next Action: [-1.612\n",
      "Step reward: -15.940867249453008, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1588.230868194592\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 10.779892921447754\n",
      "Actor loss: 43.12525177001953\n",
      "Critic loss: 15.094491004943848\n",
      "Actor loss: 34.945716857910156\n",
      "Critic loss: 8.57762622833252\n",
      "Actor loss: 41.00513458251953\n",
      "Critic loss: 7.227587699890137\n",
      "Actor loss: 41.390106201171875\n",
      "Critic loss: 1.7969512939453125\n",
      "Actor loss: 55.1162223815918\n",
      "Critic loss: 19.733469009399414\n",
      "Actor loss: 64.57752227783203\n",
      "Critic loss: 20.29822540283203\n",
      "Actor loss: 43.86821746826172\n",
      "Critic loss: 16.905933380126953\n",
      "Actor loss: 54.846778869628906\n",
      "Critic loss: 13.239662170410156\n",
      "Actor loss: 59.142303466796875\n",
      "Critic loss: 37.909542083740234\n",
      "Actor loss: 42.470176696777344\n",
      "Episode: 21\n",
      "Episode: 21, Step: 0\n",
      "Next Action: [-1.070\n",
      "Step reward: -11.964615810780309, Next State: [-1.\n",
      "Total episode reward: -11.964615810780309\n",
      "Episode: 21, Step: 1\n",
      "Next Action: [-1.153\n",
      "Step reward: -14.697893277773051, Next State: [-1.\n",
      "Total episode reward: -26.66250908855336\n",
      "Episode: 21, Step: 2\n",
      "Next Action: [-1.420\n",
      "Step reward: -15.296769833042676, Next State: [-1.\n",
      "Total episode reward: -41.95927892159604\n",
      "Episode: 21, Step: 3\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.68404844232969, Next State: [-1. \n",
      "Total episode reward: -57.64332736392573\n",
      "Episode: 21, Step: 4\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.796058498069167, Next State: [-1.\n",
      "Total episode reward: -73.4393858619949\n",
      "Episode: 21, Step: 5\n",
      "Next Action: [-1.527\n",
      "Step reward: -15.807331585885628, Next State: [-1.\n",
      "Total episode reward: -89.24671744788051\n",
      "Episode: 21, Step: 6\n",
      "Next Action: [-1.411\n",
      "Step reward: -15.802625226072957, Next State: [-1.\n",
      "Total episode reward: -105.04934267395348\n",
      "Episode: 21, Step: 7\n",
      "Next Action: [-1.282\n",
      "Step reward: -15.820287015269058, Next State: [-1.\n",
      "Total episode reward: -120.86962968922253\n",
      "Episode: 21, Step: 8\n",
      "Next Action: [-1.237\n",
      "Step reward: -15.850968831378568, Next State: [-1.\n",
      "Total episode reward: -136.7205985206011\n",
      "Episode: 21, Step: 9\n",
      "Next Action: [-1.664\n",
      "Step reward: -15.879280391806969, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.59987891240806\n",
      "Episode: 21, Step: 10\n",
      "Next Action: [-1.581\n",
      "Step reward: -15.894521571132405, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.49440048354046\n",
      "Episode: 21, Step: 11\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.900671469640354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.3950719531808\n",
      "Episode: 21, Step: 12\n",
      "Next Action: [-1.123\n",
      "Step reward: -15.919457481042183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.31452943422298\n",
      "Episode: 21, Step: 13\n",
      "Next Action: [-1.091\n",
      "Step reward: -15.895439370232667, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.20996880445566\n",
      "Episode: 21, Step: 14\n",
      "Next Action: [-1.153\n",
      "Step reward: -15.862316671601967, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.07228547605763\n",
      "Episode: 21, Step: 15\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.830614018478608, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -247.90289949453623\n",
      "Episode: 21, Step: 16\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.821371172289226, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -263.7242706668255\n",
      "Episode: 21, Step: 17\n",
      "Next Action: [-1.293\n",
      "Step reward: -15.854654905742487, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -279.578925572568\n",
      "Episode: 21, Step: 18\n",
      "Next Action: [-1.366\n",
      "Step reward: -15.8646213094822, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -295.44354688205016\n",
      "Episode: 21, Step: 19\n",
      "Next Action: [-1.264\n",
      "Step reward: -15.873861999517388, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -311.31740888156753\n",
      "Episode: 21, Step: 20\n",
      "Next Action: [-1.255\n",
      "Step reward: -15.870868702800308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -327.1882775843678\n",
      "Episode: 21, Step: 21\n",
      "Next Action: [-1.616\n",
      "Step reward: -15.891628698990147, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -343.07990628335796\n",
      "Episode: 21, Step: 22\n",
      "Next Action: [-1.534\n",
      "Step reward: -15.8922481368162, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -358.97215442017415\n",
      "Episode: 21, Step: 23\n",
      "Next Action: [-1.749\n",
      "Step reward: -15.885822411634685, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -374.85797683180886\n",
      "Episode: 21, Step: 24\n",
      "Next Action: [-1.606\n",
      "Step reward: -15.872420975160173, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -390.73039780696905\n",
      "Episode: 21, Step: 25\n",
      "Next Action: [-1.479\n",
      "Step reward: -15.88300127668699, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -406.613399083656\n",
      "Episode: 21, Step: 26\n",
      "Next Action: [-1.454\n",
      "Step reward: -15.896956842585256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -422.5103559262413\n",
      "Episode: 21, Step: 27\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.919457740841242, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -438.42981366708256\n",
      "Episode: 21, Step: 28\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.94388426121676, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -454.3736979282993\n",
      "Episode: 21, Step: 29\n",
      "Next Action: [-1.157\n",
      "Step reward: -15.954208688568944, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -470.32790661686823\n",
      "Episode: 21, Step: 30\n",
      "Next Action: [-1.056\n",
      "Step reward: -15.934560937896293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -486.2624675547645\n",
      "Episode: 21, Step: 31\n",
      "Next Action: [-0.923\n",
      "Step reward: -15.899990781949091, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -502.1624583367136\n",
      "Episode: 21, Step: 32\n",
      "Next Action: [-0.969\n",
      "Step reward: -15.890846305507113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -518.0533046422207\n",
      "Episode: 21, Step: 33\n",
      "Next Action: [-0.800\n",
      "Step reward: -15.875801260563843, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -533.9291059027846\n",
      "Episode: 21, Step: 34\n",
      "Next Action: [-0.641\n",
      "Step reward: -15.889872648519354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -549.818978551304\n",
      "Episode: 21, Step: 35\n",
      "Next Action: [-0.900\n",
      "Step reward: -15.861198406062272, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -565.6801769573663\n",
      "Episode: 21, Step: 36\n",
      "Next Action: [-0.926\n",
      "Step reward: -15.845261456755136, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -581.5254384141214\n",
      "Episode: 21, Step: 37\n",
      "Next Action: [-0.621\n",
      "Step reward: -15.851085575732279, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -597.3765239898537\n",
      "Episode: 21, Step: 38\n",
      "Next Action: [-0.576\n",
      "Step reward: -15.849591513034309, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -613.226115502888\n",
      "Episode: 21, Step: 39\n",
      "Next Action: [-0.720\n",
      "Step reward: -15.857352064610863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -629.0834675674988\n",
      "Episode: 21, Step: 40\n",
      "Next Action: [-0.185\n",
      "Step reward: -15.865745110575354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -644.9492126780742\n",
      "Episode: 21, Step: 41\n",
      "Next Action: [-0.044\n",
      "Step reward: -15.87338815478159, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -660.8226008328558\n",
      "Episode: 21, Step: 42\n",
      "Next Action: [-0.021\n",
      "Step reward: -15.85960022606488, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -676.6822010589207\n",
      "Episode: 21, Step: 43\n",
      "Next Action: [-0.144\n",
      "Step reward: -15.890702576494343, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -692.5729036354151\n",
      "Episode: 21, Step: 44\n",
      "Next Action: [-0.204\n",
      "Step reward: -15.929732495517406, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -708.5026361309325\n",
      "Episode: 21, Step: 45\n",
      "Next Action: [-0.160\n",
      "Step reward: -15.97790124315401, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -724.4805373740866\n",
      "Episode: 21, Step: 46\n",
      "Next Action: [-0.544\n",
      "Step reward: -15.961465153450202, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -740.4420025275367\n",
      "Episode: 21, Step: 47\n",
      "Next Action: [-0.498\n",
      "Step reward: -15.963882125678387, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -756.4058846532151\n",
      "Episode: 21, Step: 48\n",
      "Next Action: [-0.449\n",
      "Step reward: -15.961029589854277, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -772.3669142430695\n",
      "Episode: 21, Step: 49\n",
      "Next Action: [-0.308\n",
      "Step reward: -15.955907586669536, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -788.322821829739\n",
      "Episode: 21, Step: 50\n",
      "Next Action: [-0.321\n",
      "Step reward: -15.957116862685092, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -804.2799386924241\n",
      "Episode: 21, Step: 51\n",
      "Next Action: [-0.610\n",
      "Step reward: -15.957335676328235, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -820.2372743687524\n",
      "Episode: 21, Step: 52\n",
      "Next Action: [-0.696\n",
      "Step reward: -15.955203368122163, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -836.1924777368746\n",
      "Episode: 21, Step: 53\n",
      "Next Action: [-0.778\n",
      "Step reward: -15.963492222452164, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -852.1559699593267\n",
      "Episode: 21, Step: 54\n",
      "Next Action: [-0.674\n",
      "Step reward: -15.961099997209597, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -868.1170699565363\n",
      "Episode: 21, Step: 55\n",
      "Next Action: [-0.469\n",
      "Step reward: -15.936465371616748, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -884.053535328153\n",
      "Episode: 21, Step: 56\n",
      "Next Action: [-0.821\n",
      "Step reward: -15.903796686667224, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -899.9573320148202\n",
      "Episode: 21, Step: 57\n",
      "Next Action: [-0.864\n",
      "Step reward: -15.91255665048298, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -915.8698886653032\n",
      "Episode: 21, Step: 58\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.938610677101185, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -931.8084993424044\n",
      "Episode: 21, Step: 59\n",
      "Next Action: [-1.143\n",
      "Step reward: -15.968334332446391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -947.7768336748508\n",
      "Episode: 21, Step: 60\n",
      "Next Action: [-1.168\n",
      "Step reward: -15.952359833583781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -963.7291935084345\n",
      "Episode: 21, Step: 61\n",
      "Next Action: [-1.049\n",
      "Step reward: -15.94724233223809, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -979.6764358406726\n",
      "Episode: 21, Step: 62\n",
      "Next Action: [-0.776\n",
      "Step reward: -15.938307752919137, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -995.6147435935918\n",
      "Episode: 21, Step: 63\n",
      "Next Action: [-0.594\n",
      "Step reward: -15.958492801718215, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1011.57323639531\n",
      "Episode: 21, Step: 64\n",
      "Next Action: [-0.272\n",
      "Step reward: -15.949834003122731, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1027.5230703984328\n",
      "Episode: 21, Step: 65\n",
      "Next Action: [-0.261\n",
      "Step reward: -15.94204620953647, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1043.4651166079693\n",
      "Episode: 21, Step: 66\n",
      "Next Action: [-0.194\n",
      "Step reward: -15.936556149968183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1059.4016727579376\n",
      "Episode: 21, Step: 67\n",
      "Next Action: [-0.283\n",
      "Step reward: -15.947497402629981, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1075.3491701605676\n",
      "Episode: 21, Step: 68\n",
      "Next Action: [-0.425\n",
      "Step reward: -15.972720858535839, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1091.3218910191035\n",
      "Episode: 21, Step: 69\n",
      "Next Action: [-0.428\n",
      "Step reward: -15.977608640558403, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1107.299499659662\n",
      "Episode: 21, Step: 70\n",
      "Next Action: [-0.238\n",
      "Step reward: -15.977082120744242, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1123.276581780406\n",
      "Episode: 21, Step: 71\n",
      "Next Action: [-0.458\n",
      "Step reward: -15.97820999660434, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1139.2547917770105\n",
      "Episode: 21, Step: 72\n",
      "Next Action: [-5.193\n",
      "Step reward: -15.961226298006336, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1155.2160180750168\n",
      "Episode: 21, Step: 73\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.933195576870911, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1171.1492136518877\n",
      "Episode: 21, Step: 74\n",
      "Next Action: [-0.772\n",
      "Step reward: -15.924720195257324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1187.073933847145\n",
      "Episode: 21, Step: 75\n",
      "Next Action: [-0.570\n",
      "Step reward: -15.902716657273526, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1202.9766505044186\n",
      "Episode: 21, Step: 76\n",
      "Next Action: [-6.138\n",
      "Step reward: -15.888607209010678, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1218.8652577134292\n",
      "Episode: 21, Step: 77\n",
      "Next Action: [-0.694\n",
      "Step reward: -15.899033083803229, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1234.7642907972324\n",
      "Episode: 21, Step: 78\n",
      "Next Action: [-1.115\n",
      "Step reward: -15.907912206992135, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1250.6722030042245\n",
      "Episode: 21, Step: 79\n",
      "Next Action: [-1.110\n",
      "Step reward: -15.92264412514951, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1266.5948471293739\n",
      "Episode: 21, Step: 80\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.896515470697286, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1282.4913626000712\n",
      "Episode: 21, Step: 81\n",
      "Next Action: [-1.246\n",
      "Step reward: -15.881595365415823, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1298.372957965487\n",
      "Episode: 21, Step: 82\n",
      "Next Action: [-1.242\n",
      "Step reward: -15.89735123184704, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1314.2703091973342\n",
      "Episode: 21, Step: 83\n",
      "Next Action: [-1.207\n",
      "Step reward: -15.865359383926323, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1330.1356685812605\n",
      "Episode: 21, Step: 84\n",
      "Next Action: [-1.380\n",
      "Step reward: -15.877017195888682, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1346.012685777149\n",
      "Episode: 21, Step: 85\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.879391953607762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1361.892077730757\n",
      "Episode: 21, Step: 86\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.916230947721795, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1377.8083086784786\n",
      "Episode: 21, Step: 87\n",
      "Next Action: [-1.198\n",
      "Step reward: -15.910710965360371, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1393.719019643839\n",
      "Episode: 21, Step: 88\n",
      "Next Action: [-1.097\n",
      "Step reward: -15.920590575483791, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1409.6396102193228\n",
      "Episode: 21, Step: 89\n",
      "Next Action: [-1.358\n",
      "Step reward: -15.912865373424294, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1425.552475592747\n",
      "Episode: 21, Step: 90\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.894911401505192, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1441.4473869942522\n",
      "Episode: 21, Step: 91\n",
      "Next Action: [-1.426\n",
      "Step reward: -15.881062997574082, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1457.3284499918263\n",
      "Episode: 21, Step: 92\n",
      "Next Action: [-1.596\n",
      "Step reward: -15.875233007549582, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1473.2036829993758\n",
      "Episode: 21, Step: 93\n",
      "Next Action: [-1.515\n",
      "Step reward: -15.864500278800033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1489.0681832781759\n",
      "Episode: 21, Step: 94\n",
      "Next Action: [-1.631\n",
      "Step reward: -15.869177540987987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1504.9373608191638\n",
      "Episode: 21, Step: 95\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.914434480849357, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1520.8517953000132\n",
      "Episode: 21, Step: 96\n",
      "Next Action: [-1.596\n",
      "Step reward: -15.9388609040696, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1536.7906562040828\n",
      "Episode: 21, Step: 97\n",
      "Next Action: [-1.731\n",
      "Step reward: -15.968663738244354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1552.7593199423272\n",
      "Episode: 21, Step: 98\n",
      "Next Action: [-1.746\n",
      "Step reward: -15.981812620470963, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1568.7411325627982\n",
      "Episode: 21, Step: 99\n",
      "Next Action: [-1.833\n",
      "Step reward: -15.985565397918931, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1584.726697960717\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 4.374251842498779\n",
      "Actor loss: 52.51606369018555\n",
      "Critic loss: 6.642805099487305\n",
      "Actor loss: 56.931846618652344\n",
      "Critic loss: 2.796013832092285\n",
      "Actor loss: 46.326026916503906\n",
      "Critic loss: 22.21415138244629\n",
      "Actor loss: 36.77610397338867\n",
      "Critic loss: 3.4021992683410645\n",
      "Actor loss: 39.64513397216797\n",
      "Critic loss: 10.901269912719727\n",
      "Actor loss: 51.820777893066406\n",
      "Critic loss: 20.532703399658203\n",
      "Actor loss: 25.38322639465332\n",
      "Critic loss: 10.791967391967773\n",
      "Actor loss: 40.660621643066406\n",
      "Critic loss: 12.674200057983398\n",
      "Actor loss: 40.166297912597656\n",
      "Critic loss: 12.688371658325195\n",
      "Actor loss: 20.691240310668945\n",
      "Episode: 22\n",
      "Episode: 22, Step: 0\n",
      "Next Action: [-1.750\n",
      "Step reward: -11.960184143166899, Next State: [-0.\n",
      "Total episode reward: -11.960184143166899\n",
      "Episode: 22, Step: 1\n",
      "Next Action: [-1.943\n",
      "Step reward: -14.972190037543683, Next State: [-1.\n",
      "Total episode reward: -26.932374180710582\n",
      "Episode: 22, Step: 2\n",
      "Next Action: [-1.896\n",
      "Step reward: -15.573496128832083, Next State: [-1.\n",
      "Total episode reward: -42.505870309542665\n",
      "Episode: 22, Step: 3\n",
      "Next Action: [-1.690\n",
      "Step reward: -15.801668774975797, Next State: [-1.\n",
      "Total episode reward: -58.30753908451846\n",
      "Episode: 22, Step: 4\n",
      "Next Action: [-1.332\n",
      "Step reward: -15.875604663287278, Next State: [-1.\n",
      "Total episode reward: -74.18314374780574\n",
      "Episode: 22, Step: 5\n",
      "Next Action: [-1.220\n",
      "Step reward: -15.893763674162296, Next State: [-1.\n",
      "Total episode reward: -90.07690742196804\n",
      "Episode: 22, Step: 6\n",
      "Next Action: [-1.057\n",
      "Step reward: -15.914805798735811, Next State: [-1.\n",
      "Total episode reward: -105.99171322070386\n",
      "Episode: 22, Step: 7\n",
      "Next Action: [-1.146\n",
      "Step reward: -15.947139473841805, Next State: [-1.\n",
      "Total episode reward: -121.93885269454566\n",
      "Episode: 22, Step: 8\n",
      "Next Action: [-1.047\n",
      "Step reward: -15.948456205939069, Next State: [-1.\n",
      "Total episode reward: -137.88730890048473\n",
      "Episode: 22, Step: 9\n",
      "Next Action: [-1.019\n",
      "Step reward: -15.955956060600261, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.843264961085\n",
      "Episode: 22, Step: 10\n",
      "Next Action: [-0.769\n",
      "Step reward: -15.943389006334346, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.78665396741934\n",
      "Episode: 22, Step: 11\n",
      "Next Action: [-0.767\n",
      "Step reward: -15.930846760396664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.717500727816\n",
      "Episode: 22, Step: 12\n",
      "Next Action: [-0.708\n",
      "Step reward: -15.911676398866925, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.62917712668292\n",
      "Episode: 22, Step: 13\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.887675011123266, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.51685213780618\n",
      "Episode: 22, Step: 14\n",
      "Next Action: [-0.788\n",
      "Step reward: -15.879850515821751, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.39670265362793\n",
      "Episode: 22, Step: 15\n",
      "Next Action: [-0.671\n",
      "Step reward: -15.88088118658493, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.27758384021286\n",
      "Episode: 22, Step: 16\n",
      "Next Action: [-0.829\n",
      "Step reward: -15.903662586499847, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.1812464267127\n",
      "Episode: 22, Step: 17\n",
      "Next Action: [-0.994\n",
      "Step reward: -15.904469045772789, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.08571547248545\n",
      "Episode: 22, Step: 18\n",
      "Next Action: [-0.960\n",
      "Step reward: -15.900095981534761, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.9858114540202\n",
      "Episode: 22, Step: 19\n",
      "Next Action: [-0.891\n",
      "Step reward: -15.901480890598485, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.8872923446187\n",
      "Episode: 22, Step: 20\n",
      "Next Action: [-1.082\n",
      "Step reward: -15.921102008565095, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.80839435318376\n",
      "Episode: 22, Step: 21\n",
      "Next Action: [-1.161\n",
      "Step reward: -15.932697157478318, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.7410915106621\n",
      "Episode: 22, Step: 22\n",
      "Next Action: [-0.908\n",
      "Step reward: -15.934320732253461, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.67541224291557\n",
      "Episode: 22, Step: 23\n",
      "Next Action: [-0.733\n",
      "Step reward: -15.952720836477134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.6281330793927\n",
      "Episode: 22, Step: 24\n",
      "Next Action: [-0.852\n",
      "Step reward: -15.985402824140706, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.6135359035334\n",
      "Episode: 22, Step: 25\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.988884416225613, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.602420319759\n",
      "Episode: 22, Step: 26\n",
      "Next Action: [-0.893\n",
      "Step reward: -15.997670660037619, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.6000909797966\n",
      "Episode: 22, Step: 27\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.99772046317277, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.5978114429694\n",
      "Episode: 22, Step: 28\n",
      "Next Action: [-1.444\n",
      "Step reward: -15.994286278904402, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.59209772187376\n",
      "Episode: 22, Step: 29\n",
      "Next Action: [-1.374\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.59209772187376\n",
      "Episode: 22, Step: 30\n",
      "Next Action: [-1.442\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.59209772187376\n",
      "Episode: 22, Step: 31\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.9852916345407, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.57738935641447\n",
      "Episode: 22, Step: 32\n",
      "Next Action: [-1.578\n",
      "Step reward: -15.973135483381558, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.550524839796\n",
      "Episode: 22, Step: 33\n",
      "Next Action: [-1.523\n",
      "Step reward: -15.963953515997376, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.5144783557934\n",
      "Episode: 22, Step: 34\n",
      "Next Action: [-1.564\n",
      "Step reward: -15.960533983682556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.4750123394759\n",
      "Episode: 22, Step: 35\n",
      "Next Action: [-1.524\n",
      "Step reward: -15.970682546499967, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.4456948859759\n",
      "Episode: 22, Step: 36\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.975337081838576, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.4210319678144\n",
      "Episode: 22, Step: 37\n",
      "Next Action: [-0.938\n",
      "Step reward: -15.98741583649294, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.4084478043073\n",
      "Episode: 22, Step: 38\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.998005799191874, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.4064536034992\n",
      "Episode: 22, Step: 39\n",
      "Next Action: [-0.891\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.4064536034992\n",
      "Episode: 22, Step: 40\n",
      "Next Action: [-0.999\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.4064536034992\n",
      "Episode: 22, Step: 41\n",
      "Next Action: [-1.273\n",
      "Step reward: -16.0, Next State: [-1.  1.  1.  1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.4064536034992\n",
      "Episode: 22, Step: 42\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.98665462751383, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.3931082310131\n",
      "Episode: 22, Step: 43\n",
      "Next Action: [-0.731\n",
      "Step reward: -15.9842572404318, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.3773654714449\n",
      "Episode: 22, Step: 44\n",
      "Next Action: [-1.036\n",
      "Step reward: -15.99659053191859, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.3739560033636\n",
      "Episode: 22, Step: 45\n",
      "Next Action: [-1.453\n",
      "Step reward: -15.975643271117262, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.3495992744809\n",
      "Episode: 22, Step: 46\n",
      "Next Action: [-1.328\n",
      "Step reward: -15.961417617810342, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.3110168922911\n",
      "Episode: 22, Step: 47\n",
      "Next Action: [-1.300\n",
      "Step reward: -15.934679032712587, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.2456959250037\n",
      "Episode: 22, Step: 48\n",
      "Next Action: [-1.043\n",
      "Step reward: -15.927718555976954, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.1734144809807\n",
      "Episode: 22, Step: 49\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.935900482365724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.1093149633464\n",
      "Episode: 22, Step: 50\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.970313906313498, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.0796288696599\n",
      "Episode: 22, Step: 51\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.993023122887887, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.0726519925478\n",
      "Episode: 22, Step: 52\n",
      "Next Action: [-1.328\n",
      "Step reward: -15.996945497919013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.0695974904668\n",
      "Episode: 22, Step: 53\n",
      "Next Action: [-1.182\n",
      "Step reward: -15.990957812547151, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.0605553030139\n",
      "Episode: 22, Step: 54\n",
      "Next Action: [-1.195\n",
      "Step reward: -15.979526401469725, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.0400817044836\n",
      "Episode: 22, Step: 55\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.956025583868804, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.9961072883524\n",
      "Episode: 22, Step: 56\n",
      "Next Action: [-0.945\n",
      "Step reward: -15.953684628974724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.9497919173272\n",
      "Episode: 22, Step: 57\n",
      "Next Action: [-0.680\n",
      "Step reward: -15.978734874432662, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.9285267917599\n",
      "Episode: 22, Step: 58\n",
      "Next Action: [-0.149\n",
      "Step reward: -15.994910603545465, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.9234373953053\n",
      "Episode: 22, Step: 59\n",
      "Next Action: [-0.449\n",
      "Step reward: -15.987557328436294, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.9109947237416\n",
      "Episode: 22, Step: 60\n",
      "Next Action: [-0.516\n",
      "Step reward: -15.964709848410806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.8757045721525\n",
      "Episode: 22, Step: 61\n",
      "Next Action: [-0.768\n",
      "Step reward: -15.942445572814501, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.8181501449669\n",
      "Episode: 22, Step: 62\n",
      "Next Action: [-0.489\n",
      "Step reward: -15.92818444622424, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.7463345911912\n",
      "Episode: 22, Step: 63\n",
      "Next Action: [-7.801\n",
      "Step reward: -15.915383684760066, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.6617182759512\n",
      "Episode: 22, Step: 64\n",
      "Next Action: [-1.056\n",
      "Step reward: -15.905353482058128, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.5670717580094\n",
      "Episode: 22, Step: 65\n",
      "Next Action: [-8.771\n",
      "Step reward: -15.916108646783533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.483180404793\n",
      "Episode: 22, Step: 66\n",
      "Next Action: [-0.897\n",
      "Step reward: -15.91813633034424, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.4013167351372\n",
      "Episode: 22, Step: 67\n",
      "Next Action: [-0.692\n",
      "Step reward: -15.914234233939128, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.3155509690764\n",
      "Episode: 22, Step: 68\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.926510603939718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.2420615730161\n",
      "Episode: 22, Step: 69\n",
      "Next Action: [-0.877\n",
      "Step reward: -15.928356744224917, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.170418317241\n",
      "Episode: 22, Step: 70\n",
      "Next Action: [-0.732\n",
      "Step reward: -15.917015546199405, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.0874338634403\n",
      "Episode: 22, Step: 71\n",
      "Next Action: [-0.764\n",
      "Step reward: -15.911608585973676, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.999042449414\n",
      "Episode: 22, Step: 72\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.892317608020125, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.8913600574342\n",
      "Episode: 22, Step: 73\n",
      "Next Action: [-0.950\n",
      "Step reward: -15.917344048667545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.8087041061017\n",
      "Episode: 22, Step: 74\n",
      "Next Action: [-1.194\n",
      "Step reward: -15.93159653663857, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1190.7403006427403\n",
      "Episode: 22, Step: 75\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.927449580783374, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1206.6677502235236\n",
      "Episode: 22, Step: 76\n",
      "Next Action: [-1.199\n",
      "Step reward: -15.935040605051642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1222.6027908285753\n",
      "Episode: 22, Step: 77\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.962836395874184, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1238.5656272244494\n",
      "Episode: 22, Step: 78\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.975104834699838, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1254.5407320591491\n",
      "Episode: 22, Step: 79\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.973521660667808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1270.514253719817\n",
      "Episode: 22, Step: 80\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.968871357523847, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.4831250773407\n",
      "Episode: 22, Step: 81\n",
      "Next Action: [-1.101\n",
      "Step reward: -15.966206765201408, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.449331842542\n",
      "Episode: 22, Step: 82\n",
      "Next Action: [-1.146\n",
      "Step reward: -15.966935861406848, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.416267703949\n",
      "Episode: 22, Step: 83\n",
      "Next Action: [-1.487\n",
      "Step reward: -15.968298227454204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.3845659314031\n",
      "Episode: 22, Step: 84\n",
      "Next Action: [-1.501\n",
      "Step reward: -15.93979088961336, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.3243568210164\n",
      "Episode: 22, Step: 85\n",
      "Next Action: [-1.452\n",
      "Step reward: -15.935599274467775, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.259956095484\n",
      "Episode: 22, Step: 86\n",
      "Next Action: [-1.511\n",
      "Step reward: -15.956747329909101, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.2167034253932\n",
      "Episode: 22, Step: 87\n",
      "Next Action: [-1.486\n",
      "Step reward: -15.949364079106951, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.1660675045002\n",
      "Episode: 22, Step: 88\n",
      "Next Action: [-1.427\n",
      "Step reward: -15.91820643911104, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.0842739436114\n",
      "Episode: 22, Step: 89\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.900726854886976, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1429.9850007984983\n",
      "Episode: 22, Step: 90\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.893764251669829, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1445.878765050168\n",
      "Episode: 22, Step: 91\n",
      "Next Action: [-0.982\n",
      "Step reward: -15.919414650139663, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1461.7981797003079\n",
      "Episode: 22, Step: 92\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.925261065757033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1477.7234407660649\n",
      "Episode: 22, Step: 93\n",
      "Next Action: [-1.316\n",
      "Step reward: -15.952803198847372, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.6762439649121\n",
      "Episode: 22, Step: 94\n",
      "Next Action: [-1.146\n",
      "Step reward: -15.956796552603977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.633040517516\n",
      "Episode: 22, Step: 95\n",
      "Next Action: [-1.231\n",
      "Step reward: -15.964977974069821, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.598018491586\n",
      "Episode: 22, Step: 96\n",
      "Next Action: [-1.573\n",
      "Step reward: -15.945625673437432, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1541.5436441650234\n",
      "Episode: 22, Step: 97\n",
      "Next Action: [-1.207\n",
      "Step reward: -15.935179465649654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1557.478823630673\n",
      "Episode: 22, Step: 98\n",
      "Next Action: [-1.507\n",
      "Step reward: -15.942594652338835, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1573.421418283012\n",
      "Episode: 22, Step: 99\n",
      "Next Action: [-1.338\n",
      "Step reward: -15.965717515461574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1589.3871357984735\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 39.85577392578125\n",
      "Actor loss: 27.850072860717773\n",
      "Critic loss: 42.91502380371094\n",
      "Actor loss: 30.589879989624023\n",
      "Critic loss: 35.25533676147461\n",
      "Actor loss: 33.715232849121094\n",
      "Critic loss: 15.041439056396484\n",
      "Actor loss: 28.60440444946289\n",
      "Critic loss: 13.585356712341309\n",
      "Actor loss: 40.23672103881836\n",
      "Critic loss: 5.34902286529541\n",
      "Actor loss: 30.95416831970215\n",
      "Critic loss: 31.17876434326172\n",
      "Actor loss: 26.985952377319336\n",
      "Critic loss: 1.710744857788086\n",
      "Actor loss: 59.43809127807617\n",
      "Critic loss: 2.0510706901550293\n",
      "Actor loss: 56.55989456176758\n",
      "Critic loss: 5.714850425720215\n",
      "Actor loss: 46.178375244140625\n",
      "Episode: 23\n",
      "Episode: 23, Step: 0\n",
      "Next Action: [-1.117\n",
      "Step reward: -12.204610512260064, Next State: [-0.\n",
      "Total episode reward: -12.204610512260064\n",
      "Episode: 23, Step: 1\n",
      "Next Action: [-1.160\n",
      "Step reward: -15.088228764410832, Next State: [-1.\n",
      "Total episode reward: -27.292839276670897\n",
      "Episode: 23, Step: 2\n",
      "Next Action: [-1.290\n",
      "Step reward: -15.558051961005813, Next State: [-1.\n",
      "Total episode reward: -42.850891237676706\n",
      "Episode: 23, Step: 3\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.844159363311084, Next State: [-1.\n",
      "Total episode reward: -58.69505060098779\n",
      "Episode: 23, Step: 4\n",
      "Next Action: [-1.129\n",
      "Step reward: -15.895964131711589, Next State: [-1.\n",
      "Total episode reward: -74.59101473269938\n",
      "Episode: 23, Step: 5\n",
      "Next Action: [-0.928\n",
      "Step reward: -15.918786644180965, Next State: [-1.\n",
      "Total episode reward: -90.50980137688035\n",
      "Episode: 23, Step: 6\n",
      "Next Action: [-0.811\n",
      "Step reward: -15.96156422432265, Next State: [-1. \n",
      "Total episode reward: -106.47136560120299\n",
      "Episode: 23, Step: 7\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.974796460240782, Next State: [-1.\n",
      "Total episode reward: -122.44616206144377\n",
      "Episode: 23, Step: 8\n",
      "Next Action: [-1.430\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Total episode reward: -138.44616206144377\n",
      "Episode: 23, Step: 9\n",
      "Next Action: [-1.474\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.44616206144377\n",
      "Episode: 23, Step: 10\n",
      "Next Action: [-1.452\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.44616206144377\n",
      "Episode: 23, Step: 11\n",
      "Next Action: [-1.689\n",
      "Step reward: -15.99592041564539, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.44208247708917\n",
      "Episode: 23, Step: 12\n",
      "Next Action: [-1.678\n",
      "Step reward: -15.969697520979725, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.4117799980689\n",
      "Episode: 23, Step: 13\n",
      "Next Action: [-1.366\n",
      "Step reward: -15.962683981614594, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.3744639796835\n",
      "Episode: 23, Step: 14\n",
      "Next Action: [-1.122\n",
      "Step reward: -15.971054623403877, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.34551860308738\n",
      "Episode: 23, Step: 15\n",
      "Next Action: [-1.283\n",
      "Step reward: -15.955999476768193, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.30151807985558\n",
      "Episode: 23, Step: 16\n",
      "Next Action: [-1.148\n",
      "Step reward: -15.947822533260698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.24934061311626\n",
      "Episode: 23, Step: 17\n",
      "Next Action: [-1.217\n",
      "Step reward: -15.950365886967578, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.19970650008383\n",
      "Episode: 23, Step: 18\n",
      "Next Action: [-1.041\n",
      "Step reward: -15.934303696223923, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.13401019630777\n",
      "Episode: 23, Step: 19\n",
      "Next Action: [-0.924\n",
      "Step reward: -15.915675605646442, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.0496858019542\n",
      "Episode: 23, Step: 20\n",
      "Next Action: [-0.815\n",
      "Step reward: -15.927099714767655, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.97678551672186\n",
      "Episode: 23, Step: 21\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.95455382574811, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.93133934247\n",
      "Episode: 23, Step: 22\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.943090492144533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.8744298346145\n",
      "Episode: 23, Step: 23\n",
      "Next Action: [-0.713\n",
      "Step reward: -15.941748247662535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.816178082277\n",
      "Episode: 23, Step: 24\n",
      "Next Action: [-0.617\n",
      "Step reward: -15.95699378907341, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.7731718713504\n",
      "Episode: 23, Step: 25\n",
      "Next Action: [-0.954\n",
      "Step reward: -15.926094478711716, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.69926635006215\n",
      "Episode: 23, Step: 26\n",
      "Next Action: [-0.907\n",
      "Step reward: -15.904531899607173, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.6037982496693\n",
      "Episode: 23, Step: 27\n",
      "Next Action: [-0.719\n",
      "Step reward: -15.895729815965428, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.4995280656347\n",
      "Episode: 23, Step: 28\n",
      "Next Action: [-1.003\n",
      "Step reward: -15.90431398737989, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.4038420530146\n",
      "Episode: 23, Step: 29\n",
      "Next Action: [-0.985\n",
      "Step reward: -15.903861192073409, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.307703245088\n",
      "Episode: 23, Step: 30\n",
      "Next Action: [-1.171\n",
      "Step reward: -15.892621390716322, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.20032463580435\n",
      "Episode: 23, Step: 31\n",
      "Next Action: [-0.971\n",
      "Step reward: -15.92035828674974, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.12068292255407\n",
      "Episode: 23, Step: 32\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.944769317504374, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.0654522400584\n",
      "Episode: 23, Step: 33\n",
      "Next Action: [-1.066\n",
      "Step reward: -15.956856622039293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.0223088620977\n",
      "Episode: 23, Step: 34\n",
      "Next Action: [-1.027\n",
      "Step reward: -15.976346159369, Next State: [-1.   \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.9986550214667\n",
      "Episode: 23, Step: 35\n",
      "Next Action: [-0.613\n",
      "Step reward: -15.970223979809896, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.9688790012766\n",
      "Episode: 23, Step: 36\n",
      "Next Action: [-0.900\n",
      "Step reward: -15.944546367724985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.9134253690016\n",
      "Episode: 23, Step: 37\n",
      "Next Action: [-0.893\n",
      "Step reward: -15.956671554131526, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.8700969231331\n",
      "Episode: 23, Step: 38\n",
      "Next Action: [-0.757\n",
      "Step reward: -15.943094957003817, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.813191880137\n",
      "Episode: 23, Step: 39\n",
      "Next Action: [-0.526\n",
      "Step reward: -15.96343477085315, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.7766266509901\n",
      "Episode: 23, Step: 40\n",
      "Next Action: [-0.354\n",
      "Step reward: -15.993011336684203, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.7696379876743\n",
      "Episode: 23, Step: 41\n",
      "Next Action: [-4.136\n",
      "Step reward: -15.976268148345879, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.7459061360202\n",
      "Episode: 23, Step: 42\n",
      "Next Action: [-0.352\n",
      "Step reward: -15.960215322804318, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.7061214588244\n",
      "Episode: 23, Step: 43\n",
      "Next Action: [-0.649\n",
      "Step reward: -15.945457951178593, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.651579410003\n",
      "Episode: 23, Step: 44\n",
      "Next Action: [-0.882\n",
      "Step reward: -15.966471265755322, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.6180506757584\n",
      "Episode: 23, Step: 45\n",
      "Next Action: [-0.971\n",
      "Step reward: -15.976123108164138, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.5941737839225\n",
      "Episode: 23, Step: 46\n",
      "Next Action: [-0.600\n",
      "Step reward: -15.982626596387037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.5768003803096\n",
      "Episode: 23, Step: 47\n",
      "Next Action: [-0.928\n",
      "Step reward: -15.998530218076192, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.5753305983858\n",
      "Episode: 23, Step: 48\n",
      "Next Action: [-1.127\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.5753305983858\n",
      "Episode: 23, Step: 49\n",
      "Next Action: [-1.026\n",
      "Step reward: -15.974042734069629, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.5493733324554\n",
      "Episode: 23, Step: 50\n",
      "Next Action: [-1.161\n",
      "Step reward: -15.977937346621959, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.5273106790773\n",
      "Episode: 23, Step: 51\n",
      "Next Action: [-1.355\n",
      "Step reward: -15.975749294289374, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.5030599733667\n",
      "Episode: 23, Step: 52\n",
      "Next Action: [-1.486\n",
      "Step reward: -15.973093518424028, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.4761534917907\n",
      "Episode: 23, Step: 53\n",
      "Next Action: [-1.396\n",
      "Step reward: -15.963364972745275, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.4395184645359\n",
      "Episode: 23, Step: 54\n",
      "Next Action: [-1.316\n",
      "Step reward: -15.959031063419769, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.3985495279557\n",
      "Episode: 23, Step: 55\n",
      "Next Action: [-1.445\n",
      "Step reward: -15.96767553871347, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.3662250666691\n",
      "Episode: 23, Step: 56\n",
      "Next Action: [-1.807\n",
      "Step reward: -15.969103972729947, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.3353290393991\n",
      "Episode: 23, Step: 57\n",
      "Next Action: [-1.695\n",
      "Step reward: -15.976094441489979, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.3114234808891\n",
      "Episode: 23, Step: 58\n",
      "Next Action: [-1.660\n",
      "Step reward: -15.983582916973592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.2950063978626\n",
      "Episode: 23, Step: 59\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.983053121700115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.2780595195627\n",
      "Episode: 23, Step: 60\n",
      "Next Action: [-1.250\n",
      "Step reward: -15.986114631014603, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.2641741505773\n",
      "Episode: 23, Step: 61\n",
      "Next Action: [-1.170\n",
      "Step reward: -15.995182711838222, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.2593568624155\n",
      "Episode: 23, Step: 62\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.995121222169104, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.2544780845847\n",
      "Episode: 23, Step: 63\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.992252798818647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.2467308834033\n",
      "Episode: 23, Step: 64\n",
      "Next Action: [-1.329\n",
      "Step reward: -15.995535347538903, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.2422662309423\n",
      "Episode: 23, Step: 65\n",
      "Next Action: [-1.291\n",
      "Step reward: -15.988202842359906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.2304690733022\n",
      "Episode: 23, Step: 66\n",
      "Next Action: [-1.119\n",
      "Step reward: -15.990548185595465, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.2210172588977\n",
      "Episode: 23, Step: 67\n",
      "Next Action: [-0.952\n",
      "Step reward: -15.971984251174229, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.193001510072\n",
      "Episode: 23, Step: 68\n",
      "Next Action: [-0.738\n",
      "Step reward: -15.958963246798522, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.1519647568705\n",
      "Episode: 23, Step: 69\n",
      "Next Action: [-0.508\n",
      "Step reward: -15.972399057820242, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.1243638146907\n",
      "Episode: 23, Step: 70\n",
      "Next Action: [-0.551\n",
      "Step reward: -15.996643740945686, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.1210075556364\n",
      "Episode: 23, Step: 71\n",
      "Next Action: [-0.673\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.1210075556364\n",
      "Episode: 23, Step: 72\n",
      "Next Action: [-0.772\n",
      "Step reward: -15.980844676275218, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.1018522319116\n",
      "Episode: 23, Step: 73\n",
      "Next Action: [-0.694\n",
      "Step reward: -15.97329435745335, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.075146589365\n",
      "Episode: 23, Step: 74\n",
      "Next Action: [-0.680\n",
      "Step reward: -15.97049701660093, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.045643605966\n",
      "Episode: 23, Step: 75\n",
      "Next Action: [-0.500\n",
      "Step reward: -15.966891538989655, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.0125351449556\n",
      "Episode: 23, Step: 76\n",
      "Next Action: [-0.841\n",
      "Step reward: -15.962054288943884, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.9745894338994\n",
      "Episode: 23, Step: 77\n",
      "Next Action: [-0.855\n",
      "Step reward: -15.962753746973654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.937343180873\n",
      "Episode: 23, Step: 78\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.993807492574165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.9311506734473\n",
      "Episode: 23, Step: 79\n",
      "Next Action: [-0.879\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.9311506734473\n",
      "Episode: 23, Step: 80\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.998832601370124, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.9299832748175\n",
      "Episode: 23, Step: 81\n",
      "Next Action: [-0.949\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.9299832748175\n",
      "Episode: 23, Step: 82\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.99331344873056, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.923296723548\n",
      "Episode: 23, Step: 83\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.97891657896042, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.9022133025085\n",
      "Episode: 23, Step: 84\n",
      "Next Action: [-1.263\n",
      "Step reward: -15.957394440504176, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.8596077430127\n",
      "Episode: 23, Step: 85\n",
      "Next Action: [-1.022\n",
      "Step reward: -15.959789765830248, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.819397508843\n",
      "Episode: 23, Step: 86\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.946510018503544, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.7659075273464\n",
      "Episode: 23, Step: 87\n",
      "Next Action: [-1.233\n",
      "Step reward: -15.971052134930863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.7369596622773\n",
      "Episode: 23, Step: 88\n",
      "Next Action: [-1.335\n",
      "Step reward: -15.97959651644307, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.7165561787203\n",
      "Episode: 23, Step: 89\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.99942989245879, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.715986071179\n",
      "Episode: 23, Step: 90\n",
      "Next Action: [-1.148\n",
      "Step reward: -15.991832169154264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.7078182403334\n",
      "Episode: 23, Step: 91\n",
      "Next Action: [-1.005\n",
      "Step reward: -15.990970827760389, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.6987890680937\n",
      "Episode: 23, Step: 92\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.969005349319977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.6677944174137\n",
      "Episode: 23, Step: 93\n",
      "Next Action: [-0.750\n",
      "Step reward: -15.96113078543066, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.6289252028444\n",
      "Episode: 23, Step: 94\n",
      "Next Action: [-6.832\n",
      "Step reward: -15.970746182256345, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.5996713851007\n",
      "Episode: 23, Step: 95\n",
      "Next Action: [-0.347\n",
      "Step reward: -15.974664137337989, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.5743355224388\n",
      "Episode: 23, Step: 96\n",
      "Next Action: [-4.080\n",
      "Step reward: -15.98727082990541, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.5616063523441\n",
      "Episode: 23, Step: 97\n",
      "Next Action: [-0.282\n",
      "Step reward: -15.987503648821113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.5491100011652\n",
      "Episode: 23, Step: 98\n",
      "Next Action: [-0.508\n",
      "Step reward: -15.983629278423582, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.532739279589\n",
      "Episode: 23, Step: 99\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.963144475736238, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.4958837553252\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 39.119483947753906\n",
      "Actor loss: 65.6011734008789\n",
      "Critic loss: 1.7366762161254883\n",
      "Actor loss: 50.22580337524414\n",
      "Critic loss: 3.5456888675689697\n",
      "Actor loss: 49.05295944213867\n",
      "Critic loss: 2.3353686332702637\n",
      "Actor loss: 51.795928955078125\n",
      "Critic loss: 8.832695007324219\n",
      "Actor loss: 40.794769287109375\n",
      "Critic loss: 2.006270408630371\n",
      "Actor loss: 52.914306640625\n",
      "Critic loss: 1.526663064956665\n",
      "Actor loss: 55.2886962890625\n",
      "Critic loss: 2.2622811794281006\n",
      "Actor loss: 55.086509704589844\n",
      "Critic loss: 2.776067018508911\n",
      "Actor loss: 57.30808639526367\n",
      "Critic loss: 2.688584089279175\n",
      "Actor loss: 54.841094970703125\n",
      "Episode: 24\n",
      "Episode: 24, Step: 0\n",
      "Next Action: [-0.788\n",
      "Step reward: -11.91468051956494, Next State: [-1. \n",
      "Total episode reward: -11.91468051956494\n",
      "Episode: 24, Step: 1\n",
      "Next Action: [-1.185\n",
      "Step reward: -14.937478433127943, Next State: [-1.\n",
      "Total episode reward: -26.852158952692882\n",
      "Episode: 24, Step: 2\n",
      "Next Action: [-0.685\n",
      "Step reward: -15.602768671150873, Next State: [-1.\n",
      "Total episode reward: -42.45492762384376\n",
      "Episode: 24, Step: 3\n",
      "Next Action: [-0.462\n",
      "Step reward: -15.828713849399417, Next State: [-1.\n",
      "Total episode reward: -58.28364147324318\n",
      "Episode: 24, Step: 4\n",
      "Next Action: [-0.739\n",
      "Step reward: -15.892161577225865, Next State: [-1.\n",
      "Total episode reward: -74.17580305046904\n",
      "Episode: 24, Step: 5\n",
      "Next Action: [-0.503\n",
      "Step reward: -15.936691584447134, Next State: [-1.\n",
      "Total episode reward: -90.11249463491617\n",
      "Episode: 24, Step: 6\n",
      "Next Action: [-0.378\n",
      "Step reward: -15.95635039852554, Next State: [-1. \n",
      "Total episode reward: -106.06884503344172\n",
      "Episode: 24, Step: 7\n",
      "Next Action: [-0.729\n",
      "Step reward: -15.969074533227312, Next State: [-1.\n",
      "Total episode reward: -122.03791956666903\n",
      "Episode: 24, Step: 8\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.987460254839581, Next State: [-1.\n",
      "Total episode reward: -138.0253798215086\n",
      "Episode: 24, Step: 9\n",
      "Next Action: [-0.337\n",
      "Step reward: -15.975723282890906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.0011031043995\n",
      "Episode: 24, Step: 10\n",
      "Next Action: [-0.214\n",
      "Step reward: -15.967613795325857, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.96871689972537\n",
      "Episode: 24, Step: 11\n",
      "Next Action: [-0.366\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.96871689972537\n",
      "Episode: 24, Step: 12\n",
      "Next Action: [-0.417\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.96871689972537\n",
      "Episode: 24, Step: 13\n",
      "Next Action: [-0.498\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.96871689972537\n",
      "Episode: 24, Step: 14\n",
      "Next Action: [-0.556\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.96871689972537\n",
      "Episode: 24, Step: 15\n",
      "Next Action: [-0.493\n",
      "Step reward: -15.99865220315203, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.9673691028774\n",
      "Episode: 24, Step: 16\n",
      "Next Action: [-0.882\n",
      "Step reward: -15.997261404298895, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.9646305071763\n",
      "Episode: 24, Step: 17\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.990685680658585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.9553161878349\n",
      "Episode: 24, Step: 18\n",
      "Next Action: [-1.013\n",
      "Step reward: -15.960689724931445, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.91600591276637\n",
      "Episode: 24, Step: 19\n",
      "Next Action: [-1.340\n",
      "Step reward: -15.948028094429283, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.86403400719564\n",
      "Episode: 24, Step: 20\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.941402865288053, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.8054368724837\n",
      "Episode: 24, Step: 21\n",
      "Next Action: [-1.257\n",
      "Step reward: -15.932041583832174, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.7374784563159\n",
      "Episode: 24, Step: 22\n",
      "Next Action: [-1.532\n",
      "Step reward: -15.938852145642395, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.6763306019583\n",
      "Episode: 24, Step: 23\n",
      "Next Action: [-1.818\n",
      "Step reward: -15.935277463521073, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.61160806547934\n",
      "Episode: 24, Step: 24\n",
      "Next Action: [-1.708\n",
      "Step reward: -15.929809062116892, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.5414171275962\n",
      "Episode: 24, Step: 25\n",
      "Next Action: [-1.794\n",
      "Step reward: -15.922155806833405, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.46357293442964\n",
      "Episode: 24, Step: 26\n",
      "Next Action: [-1.844\n",
      "Step reward: -15.925052775152885, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.3886257095825\n",
      "Episode: 24, Step: 27\n",
      "Next Action: [-1.710\n",
      "Step reward: -15.939353335469374, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.3279790450519\n",
      "Episode: 24, Step: 28\n",
      "Next Action: [-1.966\n",
      "Step reward: -15.935187517768252, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.2631665628202\n",
      "Episode: 24, Step: 29\n",
      "Next Action: [-1.954\n",
      "Step reward: -15.929362965485595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.1925295283058\n",
      "Episode: 24, Step: 30\n",
      "Next Action: [-1.848\n",
      "Step reward: -15.935817803427568, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.1283473317334\n",
      "Episode: 24, Step: 31\n",
      "Next Action: [-1.855\n",
      "Step reward: -15.94455758647487, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.0729049182082\n",
      "Episode: 24, Step: 32\n",
      "Next Action: [-1.919\n",
      "Step reward: -15.947068428703018, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.0199733469112\n",
      "Episode: 24, Step: 33\n",
      "Next Action: [-1.338\n",
      "Step reward: -15.943253837298535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.9632271842098\n",
      "Episode: 24, Step: 34\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.94473484528993, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.9079620294997\n",
      "Episode: 24, Step: 35\n",
      "Next Action: [-1.032\n",
      "Step reward: -15.954171965311925, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.8621339948116\n",
      "Episode: 24, Step: 36\n",
      "Next Action: [-9.028\n",
      "Step reward: -15.967862440152889, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.8299964349645\n",
      "Episode: 24, Step: 37\n",
      "Next Action: [-0.769\n",
      "Step reward: -15.995983604140575, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.825980039105\n",
      "Episode: 24, Step: 38\n",
      "Next Action: [-0.940\n",
      "Step reward: -15.989292316447244, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.8152723555523\n",
      "Episode: 24, Step: 39\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.986974777849719, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.802247133402\n",
      "Episode: 24, Step: 40\n",
      "Next Action: [-1.051\n",
      "Step reward: -15.991553820116858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.7938009535189\n",
      "Episode: 24, Step: 41\n",
      "Next Action: [-1.014\n",
      "Step reward: -15.997649864634424, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.7914508181534\n",
      "Episode: 24, Step: 42\n",
      "Next Action: [-0.922\n",
      "Step reward: -15.98873633404108, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.7801871521945\n",
      "Episode: 24, Step: 43\n",
      "Next Action: [-5.000\n",
      "Step reward: -15.962543754921478, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.742730907116\n",
      "Episode: 24, Step: 44\n",
      "Next Action: [-0.676\n",
      "Step reward: -15.953140187427447, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.6958710945435\n",
      "Episode: 24, Step: 45\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.926100239303011, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.6219713338464\n",
      "Episode: 24, Step: 46\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.933788483948634, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.5557598177951\n",
      "Episode: 24, Step: 47\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.91669943616863, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.4724592539637\n",
      "Episode: 24, Step: 48\n",
      "Next Action: [-1.047\n",
      "Step reward: -15.914818936948272, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.387278190912\n",
      "Episode: 24, Step: 49\n",
      "Next Action: [-0.932\n",
      "Step reward: -15.907206294446302, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.2944844853582\n",
      "Episode: 24, Step: 50\n",
      "Next Action: [-1.092\n",
      "Step reward: -15.905194247884447, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.1996787332426\n",
      "Episode: 24, Step: 51\n",
      "Next Action: [-1.157\n",
      "Step reward: -15.91109829670339, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.110777029946\n",
      "Episode: 24, Step: 52\n",
      "Next Action: [-1.006\n",
      "Step reward: -15.916989697976868, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.0277667279229\n",
      "Episode: 24, Step: 53\n",
      "Next Action: [-1.049\n",
      "Step reward: -15.935149278762092, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -855.962916006685\n",
      "Episode: 24, Step: 54\n",
      "Next Action: [-1.055\n",
      "Step reward: -15.95515463914176, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -871.9180706458267\n",
      "Episode: 24, Step: 55\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.94168835048262, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.8597589963093\n",
      "Episode: 24, Step: 56\n",
      "Next Action: [-1.123\n",
      "Step reward: -15.923564612208201, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.7833236085175\n",
      "Episode: 24, Step: 57\n",
      "Next Action: [-0.786\n",
      "Step reward: -15.93397446104427, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.7172980695618\n",
      "Episode: 24, Step: 58\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.926930274184251, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.644228343746\n",
      "Episode: 24, Step: 59\n",
      "Next Action: [-1.292\n",
      "Step reward: -15.895083140717672, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.5393114844637\n",
      "Episode: 24, Step: 60\n",
      "Next Action: [-1.023\n",
      "Step reward: -15.896050321562683, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.4353618060264\n",
      "Episode: 24, Step: 61\n",
      "Next Action: [-1.256\n",
      "Step reward: -15.927384917667403, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.3627467236938\n",
      "Episode: 24, Step: 62\n",
      "Next Action: [-1.273\n",
      "Step reward: -15.93735733388525, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.3001040575791\n",
      "Episode: 24, Step: 63\n",
      "Next Action: [-0.967\n",
      "Step reward: -15.92082322605905, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.2209272836382\n",
      "Episode: 24, Step: 64\n",
      "Next Action: [-0.947\n",
      "Step reward: -15.888461650742165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.1093889343804\n",
      "Episode: 24, Step: 65\n",
      "Next Action: [-1.050\n",
      "Step reward: -15.888131023469857, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.9975199578503\n",
      "Episode: 24, Step: 66\n",
      "Next Action: [-1.059\n",
      "Step reward: -15.902235408774024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1062.8997553666243\n",
      "Episode: 24, Step: 67\n",
      "Next Action: [-1.206\n",
      "Step reward: -15.919558639120932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1078.8193140057451\n",
      "Episode: 24, Step: 68\n",
      "Next Action: [-1.240\n",
      "Step reward: -15.917113522059969, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1094.736427527805\n",
      "Episode: 24, Step: 69\n",
      "Next Action: [-1.076\n",
      "Step reward: -15.917894777513535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1110.6543223053186\n",
      "Episode: 24, Step: 70\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.940289824672533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.5946121299912\n",
      "Episode: 24, Step: 71\n",
      "Next Action: [-0.955\n",
      "Step reward: -15.953529219466272, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.5481413494574\n",
      "Episode: 24, Step: 72\n",
      "Next Action: [-1.108\n",
      "Step reward: -15.955940837971552, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.504082187429\n",
      "Episode: 24, Step: 73\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.960264043212925, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.464346230642\n",
      "Episode: 24, Step: 74\n",
      "Next Action: [-0.836\n",
      "Step reward: -15.943682585739404, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1190.4080288163814\n",
      "Episode: 24, Step: 75\n",
      "Next Action: [-0.830\n",
      "Step reward: -15.955732942008344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1206.3637617583897\n",
      "Episode: 24, Step: 76\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.969710775643794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1222.3334725340335\n",
      "Episode: 24, Step: 77\n",
      "Next Action: [-0.802\n",
      "Step reward: -15.968920346940568, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1238.3023928809741\n",
      "Episode: 24, Step: 78\n",
      "Next Action: [-0.884\n",
      "Step reward: -15.9758756886301, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1254.2782685696043\n",
      "Episode: 24, Step: 79\n",
      "Next Action: [-0.533\n",
      "Step reward: -15.980594089996439, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1270.2588626596007\n",
      "Episode: 24, Step: 80\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.98777958413568, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.2466422437365\n",
      "Episode: 24, Step: 81\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.994779577346872, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.2414218210833\n",
      "Episode: 24, Step: 82\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.99229239831661, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.2337142193999\n",
      "Episode: 24, Step: 83\n",
      "Next Action: [-1.031\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.2337142193999\n",
      "Episode: 24, Step: 84\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.996104906938115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.229819126338\n",
      "Episode: 24, Step: 85\n",
      "Next Action: [-1.245\n",
      "Step reward: -15.987535842270024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.217354968608\n",
      "Episode: 24, Step: 86\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.977224978225099, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.1945799468333\n",
      "Episode: 24, Step: 87\n",
      "Next Action: [-1.099\n",
      "Step reward: -15.964841474605004, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.1594214214383\n",
      "Episode: 24, Step: 88\n",
      "Next Action: [-1.307\n",
      "Step reward: -15.968987169383132, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.1284085908214\n",
      "Episode: 24, Step: 89\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.985031093065595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.113439683887\n",
      "Episode: 24, Step: 90\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.999508223021824, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.1129479069089\n",
      "Episode: 24, Step: 91\n",
      "Next Action: [-1.147\n",
      "Step reward: -15.980776842074075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.0937247489828\n",
      "Episode: 24, Step: 92\n",
      "Next Action: [-0.747\n",
      "Step reward: -15.954613280957577, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.0483380299404\n",
      "Episode: 24, Step: 93\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.935471780008507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.983809809949\n",
      "Episode: 24, Step: 94\n",
      "Next Action: [-0.926\n",
      "Step reward: -15.94498555999209, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.928795369941\n",
      "Episode: 24, Step: 95\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.976937783811621, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.9057331537526\n",
      "Episode: 24, Step: 96\n",
      "Next Action: [-1.031\n",
      "Step reward: -15.968485813402086, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1541.8742189671548\n",
      "Episode: 24, Step: 97\n",
      "Next Action: [-0.932\n",
      "Step reward: -15.967855697135978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1557.8420746642907\n",
      "Episode: 24, Step: 98\n",
      "Next Action: [-1.101\n",
      "Step reward: -15.976619467138221, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1573.8186941314289\n",
      "Episode: 24, Step: 99\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.957902655196817, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1589.7765967866258\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.7916324138641357\n",
      "Actor loss: 38.38219451904297\n",
      "Critic loss: 3.3467471599578857\n",
      "Actor loss: 56.723289489746094\n",
      "Critic loss: 26.906003952026367\n",
      "Actor loss: 71.51458740234375\n",
      "Critic loss: 24.958192825317383\n",
      "Actor loss: 54.287986755371094\n",
      "Critic loss: 2.5617387294769287\n",
      "Actor loss: 55.29408645629883\n",
      "Critic loss: 20.081483840942383\n",
      "Actor loss: 55.010467529296875\n",
      "Critic loss: 1.6773681640625\n",
      "Actor loss: 45.4514045715332\n",
      "Critic loss: 3.7605199813842773\n",
      "Actor loss: 39.6180305480957\n",
      "Critic loss: 10.53919506072998\n",
      "Actor loss: 28.8439998626709\n",
      "Critic loss: 3.382903814315796\n",
      "Actor loss: 46.667510986328125\n",
      "Episode: 25\n",
      "Episode: 25, Step: 0\n",
      "Next Action: [-0.750\n",
      "Step reward: -10.98842086461842, Next State: [-7.4\n",
      "Total episode reward: -10.98842086461842\n",
      "Episode: 25, Step: 1\n",
      "Next Action: [-0.829\n",
      "Step reward: -14.536035490980234, Next State: [-1.\n",
      "Total episode reward: -25.52445635559865\n",
      "Episode: 25, Step: 2\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.583223295042105, Next State: [-1.\n",
      "Total episode reward: -41.107679650640755\n",
      "Episode: 25, Step: 3\n",
      "Next Action: [-0.700\n",
      "Step reward: -15.826856491923106, Next State: [-1.\n",
      "Total episode reward: -56.93453614256386\n",
      "Episode: 25, Step: 4\n",
      "Next Action: [-0.847\n",
      "Step reward: -15.900782452219657, Next State: [-1.\n",
      "Total episode reward: -72.83531859478352\n",
      "Episode: 25, Step: 5\n",
      "Next Action: [-0.922\n",
      "Step reward: -15.932596939262831, Next State: [-1.\n",
      "Total episode reward: -88.76791553404635\n",
      "Episode: 25, Step: 6\n",
      "Next Action: [-0.756\n",
      "Step reward: -15.928736177482987, Next State: [-1.\n",
      "Total episode reward: -104.69665171152934\n",
      "Episode: 25, Step: 7\n",
      "Next Action: [-0.820\n",
      "Step reward: -15.918432762281242, Next State: [-1.\n",
      "Total episode reward: -120.61508447381058\n",
      "Episode: 25, Step: 8\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.925989865556911, Next State: [-1.\n",
      "Total episode reward: -136.5410743393675\n",
      "Episode: 25, Step: 9\n",
      "Next Action: [-0.653\n",
      "Step reward: -15.938339187525804, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.4794135268933\n",
      "Episode: 25, Step: 10\n",
      "Next Action: [-4.297\n",
      "Step reward: -15.938810003287562, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.41822353018085\n",
      "Episode: 25, Step: 11\n",
      "Next Action: [-0.499\n",
      "Step reward: -15.940374671517391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.35859820169824\n",
      "Episode: 25, Step: 12\n",
      "Next Action: [-0.747\n",
      "Step reward: -15.971939438477108, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.33053764017535\n",
      "Episode: 25, Step: 13\n",
      "Next Action: [-0.643\n",
      "Step reward: -15.968893088307006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.29943072848235\n",
      "Episode: 25, Step: 14\n",
      "Next Action: [-3.155\n",
      "Step reward: -15.955443147006662, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.254873875489\n",
      "Episode: 25, Step: 15\n",
      "Next Action: [-0.651\n",
      "Step reward: -15.946433901485518, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.20130777697452\n",
      "Episode: 25, Step: 16\n",
      "Next Action: [-0.322\n",
      "Step reward: -15.939786169291752, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.14109394626627\n",
      "Episode: 25, Step: 17\n",
      "Next Action: [-0.266\n",
      "Step reward: -15.964999445420021, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.1060933916863\n",
      "Episode: 25, Step: 18\n",
      "Next Action: [-0.291\n",
      "Step reward: -15.969833873129753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.075927264816\n",
      "Episode: 25, Step: 19\n",
      "Next Action: [-0.450\n",
      "Step reward: -15.958686279092452, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.03461354390845\n",
      "Episode: 25, Step: 20\n",
      "Next Action: [-0.497\n",
      "Step reward: -15.950189549625795, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -327.98480309353425\n",
      "Episode: 25, Step: 21\n",
      "Next Action: [-4.460\n",
      "Step reward: -15.936471711975091, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -343.92127480550937\n",
      "Episode: 25, Step: 22\n",
      "Next Action: [-0.609\n",
      "Step reward: -15.923469784394547, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -359.84474458990394\n",
      "Episode: 25, Step: 23\n",
      "Next Action: [-6.856\n",
      "Step reward: -15.932932521385153, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -375.7776771112891\n",
      "Episode: 25, Step: 24\n",
      "Next Action: [-9.248\n",
      "Step reward: -15.947363757537502, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -391.7250408688266\n",
      "Episode: 25, Step: 25\n",
      "Next Action: [-8.004\n",
      "Step reward: -15.93049836925347, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -407.6555392380801\n",
      "Episode: 25, Step: 26\n",
      "Next Action: [-0.670\n",
      "Step reward: -15.905665900187165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.56120513826727\n",
      "Episode: 25, Step: 27\n",
      "Next Action: [-0.663\n",
      "Step reward: -15.905142726613423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.4663478648807\n",
      "Episode: 25, Step: 28\n",
      "Next Action: [-0.582\n",
      "Step reward: -15.903013163753778, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.36936102863444\n",
      "Episode: 25, Step: 29\n",
      "Next Action: [-1.077\n",
      "Step reward: -15.907716879812899, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.2770779084473\n",
      "Episode: 25, Step: 30\n",
      "Next Action: [-1.091\n",
      "Step reward: -15.941539477189277, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.2186173856366\n",
      "Episode: 25, Step: 31\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.970133951466973, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.18875133710355\n",
      "Episode: 25, Step: 32\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.97507855801287, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.1638298951165\n",
      "Episode: 25, Step: 33\n",
      "Next Action: [-1.335\n",
      "Step reward: -15.980177233522756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.1440071286393\n",
      "Episode: 25, Step: 34\n",
      "Next Action: [-1.521\n",
      "Step reward: -15.970190305448796, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.114197434088\n",
      "Episode: 25, Step: 35\n",
      "Next Action: [-1.674\n",
      "Step reward: -15.969848934797216, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -567.0840463688853\n",
      "Episode: 25, Step: 36\n",
      "Next Action: [-1.473\n",
      "Step reward: -15.97999600085571, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -583.0640423697411\n",
      "Episode: 25, Step: 37\n",
      "Next Action: [-1.396\n",
      "Step reward: -15.993560784769228, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -599.0576031545103\n",
      "Episode: 25, Step: 38\n",
      "Next Action: [-1.329\n",
      "Step reward: -15.986916348138562, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -615.0445195026489\n",
      "Episode: 25, Step: 39\n",
      "Next Action: [-1.198\n",
      "Step reward: -15.984241532803662, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -631.0287610354526\n",
      "Episode: 25, Step: 40\n",
      "Next Action: [-0.986\n",
      "Step reward: -15.970512012897608, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -646.9992730483502\n",
      "Episode: 25, Step: 41\n",
      "Next Action: [-1.415\n",
      "Step reward: -15.971308790855828, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -662.970581839206\n",
      "Episode: 25, Step: 42\n",
      "Next Action: [-1.783\n",
      "Step reward: -15.988358911934565, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -678.9589407511406\n",
      "Episode: 25, Step: 43\n",
      "Next Action: [-1.835\n",
      "Step reward: -15.992960062555111, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -694.9519008136957\n",
      "Episode: 25, Step: 44\n",
      "Next Action: [-1.551\n",
      "Step reward: -15.972930196335826, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -710.9248310100315\n",
      "Episode: 25, Step: 45\n",
      "Next Action: [-1.388\n",
      "Step reward: -15.9668042929297, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -726.8916353029613\n",
      "Episode: 25, Step: 46\n",
      "Next Action: [-1.420\n",
      "Step reward: -15.951766400834583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -742.8434017037958\n",
      "Episode: 25, Step: 47\n",
      "Next Action: [-1.731\n",
      "Step reward: -15.967994299691561, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -758.8113960034874\n",
      "Episode: 25, Step: 48\n",
      "Next Action: [-1.995\n",
      "Step reward: -15.99423240491721, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -774.8056284084046\n",
      "Episode: 25, Step: 49\n",
      "Next Action: [-2.013\n",
      "Step reward: -15.994705101575377, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -790.80033350998\n",
      "Episode: 25, Step: 50\n",
      "Next Action: [-1.822\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -806.80033350998\n",
      "Episode: 25, Step: 51\n",
      "Next Action: [-1.660\n",
      "Step reward: -15.984947950093574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -822.7852814600735\n",
      "Episode: 25, Step: 52\n",
      "Next Action: [-1.585\n",
      "Step reward: -15.984732984715158, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -838.7700144447887\n",
      "Episode: 25, Step: 53\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.97874198509501, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -854.7487564298837\n",
      "Episode: 25, Step: 54\n",
      "Next Action: [-1.632\n",
      "Step reward: -15.977335971213932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -870.7260924010976\n",
      "Episode: 25, Step: 55\n",
      "Next Action: [-1.625\n",
      "Step reward: -15.988463347634529, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -886.7145557487321\n",
      "Episode: 25, Step: 56\n",
      "Next Action: [-1.550\n",
      "Step reward: -15.976268161469765, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -902.6908239102019\n",
      "Episode: 25, Step: 57\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.98747008810306, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -918.6782939983049\n",
      "Episode: 25, Step: 58\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.981146258664479, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -934.6594402569693\n",
      "Episode: 25, Step: 59\n",
      "Next Action: [-0.956\n",
      "Step reward: -15.96483083642527, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -950.6242710933946\n",
      "Episode: 25, Step: 60\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.951415228589477, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -966.575686321984\n",
      "Episode: 25, Step: 61\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.952560290169904, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -982.5282466121539\n",
      "Episode: 25, Step: 62\n",
      "Next Action: [-0.952\n",
      "Step reward: -15.959717800658384, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -998.4879644128123\n",
      "Episode: 25, Step: 63\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.987745629854755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1014.4757100426671\n",
      "Episode: 25, Step: 64\n",
      "Next Action: [-1.257\n",
      "Step reward: -15.948981158117887, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1030.424691200785\n",
      "Episode: 25, Step: 65\n",
      "Next Action: [-1.163\n",
      "Step reward: -15.927863518191117, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.352554718976\n",
      "Episode: 25, Step: 66\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.915195170180585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1062.2677498891567\n",
      "Episode: 25, Step: 67\n",
      "Next Action: [-0.759\n",
      "Step reward: -15.930402817312817, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1078.1981527064695\n",
      "Episode: 25, Step: 68\n",
      "Next Action: [-0.726\n",
      "Step reward: -15.952094694056896, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1094.1502474005265\n",
      "Episode: 25, Step: 69\n",
      "Next Action: [-0.897\n",
      "Step reward: -15.954133264772217, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1110.1043806652988\n",
      "Episode: 25, Step: 70\n",
      "Next Action: [-0.973\n",
      "Step reward: -15.936712848093439, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.0410935133923\n",
      "Episode: 25, Step: 71\n",
      "Next Action: [-1.012\n",
      "Step reward: -15.939415385917183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1141.9805088993094\n",
      "Episode: 25, Step: 72\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.935871952439024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1157.9163808517485\n",
      "Episode: 25, Step: 73\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.939332231865395, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1173.8557130836139\n",
      "Episode: 25, Step: 74\n",
      "Next Action: [-1.080\n",
      "Step reward: -15.9412003693267, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1189.7969134529405\n",
      "Episode: 25, Step: 75\n",
      "Next Action: [-1.090\n",
      "Step reward: -15.954493355367806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1205.7514068083083\n",
      "Episode: 25, Step: 76\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.981606305000794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1221.733013113309\n",
      "Episode: 25, Step: 77\n",
      "Next Action: [-1.281\n",
      "Step reward: -15.993217489286964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1237.726230602596\n",
      "Episode: 25, Step: 78\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.996535875160815, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1253.7227664777567\n",
      "Episode: 25, Step: 79\n",
      "Next Action: [-0.826\n",
      "Step reward: -15.986407334981088, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1269.7091738127378\n",
      "Episode: 25, Step: 80\n",
      "Next Action: [-0.825\n",
      "Step reward: -15.97625602241195, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1285.6854298351498\n",
      "Episode: 25, Step: 81\n",
      "Next Action: [-0.658\n",
      "Step reward: -15.970364092917858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1301.6557939280676\n",
      "Episode: 25, Step: 82\n",
      "Next Action: [-0.857\n",
      "Step reward: -15.963540636505638, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1317.6193345645734\n",
      "Episode: 25, Step: 83\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.945170560940719, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1333.5645051255142\n",
      "Episode: 25, Step: 84\n",
      "Next Action: [-8.513\n",
      "Step reward: -15.951376213766602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1349.515881339281\n",
      "Episode: 25, Step: 85\n",
      "Next Action: [-1.205\n",
      "Step reward: -15.964999304245223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1365.480880643526\n",
      "Episode: 25, Step: 86\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.971634738174412, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1381.4525153817003\n",
      "Episode: 25, Step: 87\n",
      "Next Action: [-1.079\n",
      "Step reward: -15.950063140424996, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1397.4025785221254\n",
      "Episode: 25, Step: 88\n",
      "Next Action: [-1.096\n",
      "Step reward: -15.94608524147044, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1413.3486637635958\n",
      "Episode: 25, Step: 89\n",
      "Next Action: [-0.925\n",
      "Step reward: -15.94938690726718, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1429.298050670863\n",
      "Episode: 25, Step: 90\n",
      "Next Action: [-1.115\n",
      "Step reward: -15.95684402662066, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1445.2548946974837\n",
      "Episode: 25, Step: 91\n",
      "Next Action: [-0.952\n",
      "Step reward: -15.953252628400442, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1461.208147325884\n",
      "Episode: 25, Step: 92\n",
      "Next Action: [-1.230\n",
      "Step reward: -15.958669695504424, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1477.1668170213884\n",
      "Episode: 25, Step: 93\n",
      "Next Action: [-1.101\n",
      "Step reward: -15.958538914702256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.1253559360907\n",
      "Episode: 25, Step: 94\n",
      "Next Action: [-0.960\n",
      "Step reward: -15.960733720072476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.0860896561633\n",
      "Episode: 25, Step: 95\n",
      "Next Action: [-1.053\n",
      "Step reward: -15.946567822707054, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.0326574788703\n",
      "Episode: 25, Step: 96\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.957818189651736, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1540.990475668522\n",
      "Episode: 25, Step: 97\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.953273638317667, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1556.9437493068397\n",
      "Episode: 25, Step: 98\n",
      "Next Action: [-1.566\n",
      "Step reward: -15.945053643039115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1572.8888029498787\n",
      "Episode: 25, Step: 99\n",
      "Next Action: [-1.304\n",
      "Step reward: -15.96131870363415, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1588.8501216535128\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 10.486702919006348\n",
      "Actor loss: 29.18292236328125\n",
      "Critic loss: 8.190112113952637\n",
      "Actor loss: 27.402393341064453\n",
      "Critic loss: 10.27199935913086\n",
      "Actor loss: 42.92999267578125\n",
      "Critic loss: 13.500322341918945\n",
      "Actor loss: 38.69988250732422\n",
      "Critic loss: 34.96780776977539\n",
      "Actor loss: 33.23640060424805\n",
      "Critic loss: 21.9200496673584\n",
      "Actor loss: 38.937686920166016\n",
      "Critic loss: 12.622843742370605\n",
      "Actor loss: 45.928646087646484\n",
      "Critic loss: 12.657629013061523\n",
      "Actor loss: 44.216739654541016\n",
      "Critic loss: 4.943406105041504\n",
      "Actor loss: 52.858551025390625\n",
      "Critic loss: 6.547502517700195\n",
      "Actor loss: 57.14863204956055\n",
      "Episode: 26\n",
      "Episode: 26, Step: 0\n",
      "Next Action: [-1.336\n",
      "Step reward: -12.36527797144243, Next State: [-1. \n",
      "Total episode reward: -12.36527797144243\n",
      "Episode: 26, Step: 1\n",
      "Next Action: [-1.421\n",
      "Step reward: -15.228970390540448, Next State: [-1.\n",
      "Total episode reward: -27.594248361982878\n",
      "Episode: 26, Step: 2\n",
      "Next Action: [-1.332\n",
      "Step reward: -15.816618610208149, Next State: [-1.\n",
      "Total episode reward: -43.41086697219103\n",
      "Episode: 26, Step: 3\n",
      "Next Action: [-1.093\n",
      "Step reward: -15.966717669005103, Next State: [-1.\n",
      "Total episode reward: -59.37758464119614\n",
      "Episode: 26, Step: 4\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.967594658289944, Next State: [-1.\n",
      "Total episode reward: -75.34517929948608\n",
      "Episode: 26, Step: 5\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.958585507479366, Next State: [-1.\n",
      "Total episode reward: -91.30376480696545\n",
      "Episode: 26, Step: 6\n",
      "Next Action: [-0.770\n",
      "Step reward: -15.96580177911342, Next State: [-1. \n",
      "Total episode reward: -107.26956658607887\n",
      "Episode: 26, Step: 7\n",
      "Next Action: [-0.735\n",
      "Step reward: -15.960728659027833, Next State: [-1.\n",
      "Total episode reward: -123.2302952451067\n",
      "Episode: 26, Step: 8\n",
      "Next Action: [-0.854\n",
      "Step reward: -15.950177788252637, Next State: [-1.\n",
      "Total episode reward: -139.18047303335933\n",
      "Episode: 26, Step: 9\n",
      "Next Action: [-0.802\n",
      "Step reward: -15.943487225510113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -155.12396025886946\n",
      "Episode: 26, Step: 10\n",
      "Next Action: [-0.766\n",
      "Step reward: -15.94568086035411, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -171.06964111922358\n",
      "Episode: 26, Step: 11\n",
      "Next Action: [-0.541\n",
      "Step reward: -15.959879944528842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -187.0295210637524\n",
      "Episode: 26, Step: 12\n",
      "Next Action: [-0.777\n",
      "Step reward: -15.944623097984659, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.97414416173706\n",
      "Episode: 26, Step: 13\n",
      "Next Action: [-0.788\n",
      "Step reward: -15.971498817953082, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.94564297969015\n",
      "Episode: 26, Step: 14\n",
      "Next Action: [-0.693\n",
      "Step reward: -15.996481670626235, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.94212465031637\n",
      "Episode: 26, Step: 15\n",
      "Next Action: [-0.644\n",
      "Step reward: -15.981775208939922, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.9238998592563\n",
      "Episode: 26, Step: 16\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.943542475328382, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.86744233458467\n",
      "Episode: 26, Step: 17\n",
      "Next Action: [-1.186\n",
      "Step reward: -15.948778052689507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.8162203872742\n",
      "Episode: 26, Step: 18\n",
      "Next Action: [-1.024\n",
      "Step reward: -15.94880204291002, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.7650224301842\n",
      "Episode: 26, Step: 19\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.941568617796639, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.70659104798085\n",
      "Episode: 26, Step: 20\n",
      "Next Action: [-1.520\n",
      "Step reward: -15.948363961812436, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.6549550097933\n",
      "Episode: 26, Step: 21\n",
      "Next Action: [-1.344\n",
      "Step reward: -15.993079752474907, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.6480347622682\n",
      "Episode: 26, Step: 22\n",
      "Next Action: [-1.254\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.6480347622682\n",
      "Episode: 26, Step: 23\n",
      "Next Action: [-1.386\n",
      "Step reward: -15.997544005942698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.6455787682109\n",
      "Episode: 26, Step: 24\n",
      "Next Action: [-1.771\n",
      "Step reward: -15.986230963313961, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.63180973152487\n",
      "Episode: 26, Step: 25\n",
      "Next Action: [-1.566\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.63180973152487\n",
      "Episode: 26, Step: 26\n",
      "Next Action: [-1.462\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.63180973152487\n",
      "Episode: 26, Step: 27\n",
      "Next Action: [-1.797\n",
      "Step reward: -15.990160243827532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.6219699753524\n",
      "Episode: 26, Step: 28\n",
      "Next Action: [-1.483\n",
      "Step reward: -15.976044456765019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.5980144321174\n",
      "Episode: 26, Step: 29\n",
      "Next Action: [-1.315\n",
      "Step reward: -15.970459393807461, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.56847382592485\n",
      "Episode: 26, Step: 30\n",
      "Next Action: [-1.505\n",
      "Step reward: -15.941599742243483, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.5100735681683\n",
      "Episode: 26, Step: 31\n",
      "Next Action: [-1.434\n",
      "Step reward: -15.953135512710217, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.4632090808785\n",
      "Episode: 26, Step: 32\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.929708154501355, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.3929172353799\n",
      "Episode: 26, Step: 33\n",
      "Next Action: [-1.207\n",
      "Step reward: -15.921187324903453, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.3141045602833\n",
      "Episode: 26, Step: 34\n",
      "Next Action: [-0.976\n",
      "Step reward: -15.973080949707072, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.2871855099904\n",
      "Episode: 26, Step: 35\n",
      "Next Action: [-0.889\n",
      "Step reward: -15.981002951827405, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.2681884618178\n",
      "Episode: 26, Step: 36\n",
      "Next Action: [-0.818\n",
      "Step reward: -15.991742027256917, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.2599304890747\n",
      "Episode: 26, Step: 37\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.988043052005358, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.24797354108\n",
      "Episode: 26, Step: 38\n",
      "Next Action: [-1.040\n",
      "Step reward: -15.9878042592, Next State: [-1.     \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -618.23577780028\n",
      "Episode: 26, Step: 39\n",
      "Next Action: [-1.164\n",
      "Step reward: -15.99911831140522, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -634.2348961116852\n",
      "Episode: 26, Step: 40\n",
      "Next Action: [-1.271\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -650.2348961116852\n",
      "Episode: 26, Step: 41\n",
      "Next Action: [-1.274\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -666.2348961116852\n",
      "Episode: 26, Step: 42\n",
      "Next Action: [-1.013\n",
      "Step reward: -15.991335869349081, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -682.2262319810343\n",
      "Episode: 26, Step: 43\n",
      "Next Action: [-0.877\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -698.2262319810343\n",
      "Episode: 26, Step: 44\n",
      "Next Action: [-1.057\n",
      "Step reward: -15.993466481988277, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -714.2196984630226\n",
      "Episode: 26, Step: 45\n",
      "Next Action: [-1.394\n",
      "Step reward: -15.989120925683324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -730.2088193887059\n",
      "Episode: 26, Step: 46\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.975543390056009, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -746.184362778762\n",
      "Episode: 26, Step: 47\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.97274941692102, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -762.157112195683\n",
      "Episode: 26, Step: 48\n",
      "Next Action: [-1.212\n",
      "Step reward: -15.980653900349518, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -778.1377660960325\n",
      "Episode: 26, Step: 49\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.988534410170953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -794.1263005062034\n",
      "Episode: 26, Step: 50\n",
      "Next Action: [-1.747\n",
      "Step reward: -15.983841627072392, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -810.1101421332758\n",
      "Episode: 26, Step: 51\n",
      "Next Action: [-1.473\n",
      "Step reward: -15.983457991342155, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -826.0936001246179\n",
      "Episode: 26, Step: 52\n",
      "Next Action: [-1.529\n",
      "Step reward: -15.987019368191115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -842.080619492809\n",
      "Episode: 26, Step: 53\n",
      "Next Action: [-1.188\n",
      "Step reward: -15.995397287615617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -858.0760167804247\n",
      "Episode: 26, Step: 54\n",
      "Next Action: [-1.297\n",
      "Step reward: -15.980348918573178, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -874.0563656989979\n",
      "Episode: 26, Step: 55\n",
      "Next Action: [-1.626\n",
      "Step reward: -15.994218008930664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -890.0505837079286\n",
      "Episode: 26, Step: 56\n",
      "Next Action: [-1.368\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -906.0505837079286\n",
      "Episode: 26, Step: 57\n",
      "Next Action: [-1.038\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -922.0505837079286\n",
      "Episode: 26, Step: 58\n",
      "Next Action: [-0.940\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -938.0505837079286\n",
      "Episode: 26, Step: 59\n",
      "Next Action: [-1.258\n",
      "Step reward: -15.994468389198863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -954.0450520971275\n",
      "Episode: 26, Step: 60\n",
      "Next Action: [-1.453\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -970.0450520971275\n",
      "Episode: 26, Step: 61\n",
      "Next Action: [-1.606\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -986.0450520971275\n",
      "Episode: 26, Step: 62\n",
      "Next Action: [-1.193\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1002.0450520971275\n",
      "Episode: 26, Step: 63\n",
      "Next Action: [-1.481\n",
      "Step reward: -15.998916799732964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1018.0439688968604\n",
      "Episode: 26, Step: 64\n",
      "Next Action: [-1.153\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1034.0439688968604\n",
      "Episode: 26, Step: 65\n",
      "Next Action: [-1.084\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1050.0439688968604\n",
      "Episode: 26, Step: 66\n",
      "Next Action: [-0.963\n",
      "Step reward: -15.980944150105799, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1066.0249130469663\n",
      "Episode: 26, Step: 67\n",
      "Next Action: [-0.955\n",
      "Step reward: -15.969408249012382, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.9943212959786\n",
      "Episode: 26, Step: 68\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.968725857924365, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.9630471539028\n",
      "Episode: 26, Step: 69\n",
      "Next Action: [-0.972\n",
      "Step reward: -15.968859653399907, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.9319068073028\n",
      "Episode: 26, Step: 70\n",
      "Next Action: [-0.979\n",
      "Step reward: -15.963500241145525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.8954070484483\n",
      "Episode: 26, Step: 71\n",
      "Next Action: [-0.806\n",
      "Step reward: -15.952201382950111, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.8476084313984\n",
      "Episode: 26, Step: 72\n",
      "Next Action: [-0.751\n",
      "Step reward: -15.967090548904677, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.814698980303\n",
      "Episode: 26, Step: 73\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.97636897332887, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.791067953632\n",
      "Episode: 26, Step: 74\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.984416409096756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.7754843627288\n",
      "Episode: 26, Step: 75\n",
      "Next Action: [-0.822\n",
      "Step reward: -15.96764971184287, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.7431340745716\n",
      "Episode: 26, Step: 76\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.982246440555537, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.7253805151272\n",
      "Episode: 26, Step: 77\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.976087939745245, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.7014684548724\n",
      "Episode: 26, Step: 78\n",
      "Next Action: [-0.922\n",
      "Step reward: -15.964369058651698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.6658375135241\n",
      "Episode: 26, Step: 79\n",
      "Next Action: [-1.113\n",
      "Step reward: -15.950469485485938, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.61630699901\n",
      "Episode: 26, Step: 80\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.941807973641398, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.5581149726513\n",
      "Episode: 26, Step: 81\n",
      "Next Action: [-1.376\n",
      "Step reward: -15.938772364367928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.4968873370192\n",
      "Episode: 26, Step: 82\n",
      "Next Action: [-1.542\n",
      "Step reward: -15.970295298852287, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.4671826358715\n",
      "Episode: 26, Step: 83\n",
      "Next Action: [-1.385\n",
      "Step reward: -15.996170888690001, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.4633535245614\n",
      "Episode: 26, Step: 84\n",
      "Next Action: [-1.465\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.4633535245614\n",
      "Episode: 26, Step: 85\n",
      "Next Action: [-1.401\n",
      "Step reward: -15.999689348780127, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.4630428733415\n",
      "Episode: 26, Step: 86\n",
      "Next Action: [-1.553\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.4630428733415\n",
      "Episode: 26, Step: 87\n",
      "Next Action: [-1.469\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.4630428733415\n",
      "Episode: 26, Step: 88\n",
      "Next Action: [-1.371\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1417.4630428733415\n",
      "Episode: 26, Step: 89\n",
      "Next Action: [-1.053\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1433.4630428733415\n",
      "Episode: 26, Step: 90\n",
      "Next Action: [-0.827\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1449.4630428733415\n",
      "Episode: 26, Step: 91\n",
      "Next Action: [-1.004\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1465.4630428733415\n",
      "Episode: 26, Step: 92\n",
      "Next Action: [-1.440\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1481.4630428733415\n",
      "Episode: 26, Step: 93\n",
      "Next Action: [-1.616\n",
      "Step reward: -15.99412918358465, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1497.457172056926\n",
      "Episode: 26, Step: 94\n",
      "Next Action: [-1.444\n",
      "Step reward: -15.988581457691195, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1513.4457535146173\n",
      "Episode: 26, Step: 95\n",
      "Next Action: [-1.433\n",
      "Step reward: -15.986729299267651, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1529.432482813885\n",
      "Episode: 26, Step: 96\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.976291111744969, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1545.40877392563\n",
      "Episode: 26, Step: 97\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.981764967545615, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1561.3905388931755\n",
      "Episode: 26, Step: 98\n",
      "Next Action: [-1.021\n",
      "Step reward: -15.976395877622712, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1577.3669347707983\n",
      "Episode: 26, Step: 99\n",
      "Next Action: [-0.709\n",
      "Step reward: -15.974153728135793, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1593.3410884989341\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.633772373199463\n",
      "Actor loss: 51.65411376953125\n",
      "Critic loss: 4.652230262756348\n",
      "Actor loss: 50.16204833984375\n",
      "Critic loss: 15.311111450195312\n",
      "Actor loss: 70.16658020019531\n",
      "Critic loss: 1.9651522636413574\n",
      "Actor loss: 58.3859977722168\n",
      "Critic loss: 20.379745483398438\n",
      "Actor loss: 48.32334899902344\n",
      "Critic loss: 16.63188362121582\n",
      "Actor loss: 64.5673828125\n",
      "Critic loss: 2.1964151859283447\n",
      "Actor loss: 39.776649475097656\n",
      "Critic loss: 1.6026538610458374\n",
      "Actor loss: 52.83369064331055\n",
      "Critic loss: 64.03760528564453\n",
      "Actor loss: 49.69179916381836\n",
      "Critic loss: 4.281280040740967\n",
      "Actor loss: 44.025856018066406\n",
      "Episode: 27\n",
      "Episode: 27, Step: 0\n",
      "Next Action: [-0.156\n",
      "Step reward: -11.24288301934193, Next State: [ 0.2\n",
      "Total episode reward: -11.24288301934193\n",
      "Episode: 27, Step: 1\n",
      "Next Action: [-0.703\n",
      "Step reward: -14.489529247905262, Next State: [-0.\n",
      "Total episode reward: -25.73241226724719\n",
      "Episode: 27, Step: 2\n",
      "Next Action: [-0.730\n",
      "Step reward: -15.537173986154137, Next State: [-1.\n",
      "Total episode reward: -41.26958625340133\n",
      "Episode: 27, Step: 3\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.865411669295092, Next State: [-1.\n",
      "Total episode reward: -57.13499792269642\n",
      "Episode: 27, Step: 4\n",
      "Next Action: [-0.815\n",
      "Step reward: -15.881704617708003, Next State: [-1.\n",
      "Total episode reward: -73.01670254040442\n",
      "Episode: 27, Step: 5\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.884634287381116, Next State: [-1.\n",
      "Total episode reward: -88.90133682778554\n",
      "Episode: 27, Step: 6\n",
      "Next Action: [-1.321\n",
      "Step reward: -15.92193416095791, Next State: [-1. \n",
      "Total episode reward: -104.82327098874345\n",
      "Episode: 27, Step: 7\n",
      "Next Action: [-1.453\n",
      "Step reward: -15.955879846670829, Next State: [-1.\n",
      "Total episode reward: -120.77915083541427\n",
      "Episode: 27, Step: 8\n",
      "Next Action: [-1.442\n",
      "Step reward: -15.973603133165458, Next State: [-1.\n",
      "Total episode reward: -136.75275396857973\n",
      "Episode: 27, Step: 9\n",
      "Next Action: [-1.314\n",
      "Step reward: -15.964925760996572, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.7176797295763\n",
      "Episode: 27, Step: 10\n",
      "Next Action: [-1.325\n",
      "Step reward: -15.964288117722033, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.68196784729832\n",
      "Episode: 27, Step: 11\n",
      "Next Action: [-1.506\n",
      "Step reward: -15.972885399525932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.65485324682425\n",
      "Episode: 27, Step: 12\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.9790326286241, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.63388587544836\n",
      "Episode: 27, Step: 13\n",
      "Next Action: [-1.365\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.63388587544836\n",
      "Episode: 27, Step: 14\n",
      "Next Action: [-1.434\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.63388587544836\n",
      "Episode: 27, Step: 15\n",
      "Next Action: [-1.576\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.63388587544836\n",
      "Episode: 27, Step: 16\n",
      "Next Action: [-1.506\n",
      "Step reward: -15.997028037888908, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.6309139133373\n",
      "Episode: 27, Step: 17\n",
      "Next Action: [-1.261\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.6309139133373\n",
      "Episode: 27, Step: 18\n",
      "Next Action: [-1.422\n",
      "Step reward: -15.995511706180737, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.62642561951805\n",
      "Episode: 27, Step: 19\n",
      "Next Action: [-1.229\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.62642561951805\n",
      "Episode: 27, Step: 20\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.997088157279439, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.6235137767975\n",
      "Episode: 27, Step: 21\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.988632156644139, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.61214593344164\n",
      "Episode: 27, Step: 22\n",
      "Next Action: [-1.696\n",
      "Step reward: -15.986204753032013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.5983506864736\n",
      "Episode: 27, Step: 23\n",
      "Next Action: [-1.879\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.5983506864736\n",
      "Episode: 27, Step: 24\n",
      "Next Action: [-2.139\n",
      "Step reward: -15.993007787050198, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.5913584735238\n",
      "Episode: 27, Step: 25\n",
      "Next Action: [-1.743\n",
      "Step reward: -15.986038517271844, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.57739699079565\n",
      "Episode: 27, Step: 26\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.965394288015258, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.5427912788109\n",
      "Episode: 27, Step: 27\n",
      "Next Action: [-1.649\n",
      "Step reward: -15.977150016446656, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.5199412952576\n",
      "Episode: 27, Step: 28\n",
      "Next Action: [-1.638\n",
      "Step reward: -15.978885631726296, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.4988269269839\n",
      "Episode: 27, Step: 29\n",
      "Next Action: [-1.841\n",
      "Step reward: -15.984230383101924, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.4830573100858\n",
      "Episode: 27, Step: 30\n",
      "Next Action: [-1.351\n",
      "Step reward: -15.995487269690955, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.4785445797768\n",
      "Episode: 27, Step: 31\n",
      "Next Action: [-1.383\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.4785445797768\n",
      "Episode: 27, Step: 32\n",
      "Next Action: [-1.076\n",
      "Step reward: -15.991192127882705, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.4697367076595\n",
      "Episode: 27, Step: 33\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.97683693359103, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.4465736412506\n",
      "Episode: 27, Step: 34\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.937054523376906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.3836281646275\n",
      "Episode: 27, Step: 35\n",
      "Next Action: [-0.806\n",
      "Step reward: -15.9473095103281, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.3309376749555\n",
      "Episode: 27, Step: 36\n",
      "Next Action: [-7.335\n",
      "Step reward: -15.948297566665302, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.2792352416208\n",
      "Episode: 27, Step: 37\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.963241752981117, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.2424769946019\n",
      "Episode: 27, Step: 38\n",
      "Next Action: [-9.293\n",
      "Step reward: -15.966758448492968, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.2092354430949\n",
      "Episode: 27, Step: 39\n",
      "Next Action: [-0.596\n",
      "Step reward: -15.948651815247036, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.1578872583419\n",
      "Episode: 27, Step: 40\n",
      "Next Action: [-0.577\n",
      "Step reward: -15.967649741160013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.125536999502\n",
      "Episode: 27, Step: 41\n",
      "Next Action: [-0.579\n",
      "Step reward: -15.978420437715688, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.1039574372177\n",
      "Episode: 27, Step: 42\n",
      "Next Action: [-0.645\n",
      "Step reward: -15.984008259253734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.0879656964714\n",
      "Episode: 27, Step: 43\n",
      "Next Action: [-0.805\n",
      "Step reward: -15.975980515639128, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.0639462121105\n",
      "Episode: 27, Step: 44\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.968739463252863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.0326856753634\n",
      "Episode: 27, Step: 45\n",
      "Next Action: [-0.929\n",
      "Step reward: -15.971279522980815, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.0039651983442\n",
      "Episode: 27, Step: 46\n",
      "Next Action: [-1.018\n",
      "Step reward: -15.955410962334833, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -743.959376160679\n",
      "Episode: 27, Step: 47\n",
      "Next Action: [-1.288\n",
      "Step reward: -15.948701024117433, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -759.9080771847964\n",
      "Episode: 27, Step: 48\n",
      "Next Action: [-1.372\n",
      "Step reward: -15.92873330432122, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -775.8368104891176\n",
      "Episode: 27, Step: 49\n",
      "Next Action: [-1.420\n",
      "Step reward: -15.927405503565096, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -791.7642159926827\n",
      "Episode: 27, Step: 50\n",
      "Next Action: [-1.353\n",
      "Step reward: -15.94949050490427, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -807.7137064975869\n",
      "Episode: 27, Step: 51\n",
      "Next Action: [-1.240\n",
      "Step reward: -15.966355715179441, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -823.6800622127664\n",
      "Episode: 27, Step: 52\n",
      "Next Action: [-0.798\n",
      "Step reward: -15.979095313637979, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -839.6591575264044\n",
      "Episode: 27, Step: 53\n",
      "Next Action: [-0.834\n",
      "Step reward: -15.963933272173344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -855.6230907985778\n",
      "Episode: 27, Step: 54\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.949342887387287, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -871.572433685965\n",
      "Episode: 27, Step: 55\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.937591032902457, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.5100247188675\n",
      "Episode: 27, Step: 56\n",
      "Next Action: [-0.638\n",
      "Step reward: -15.916923678833118, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.4269483977006\n",
      "Episode: 27, Step: 57\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.906666855059367, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.33361525276\n",
      "Episode: 27, Step: 58\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.908158818486104, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.241774071246\n",
      "Episode: 27, Step: 59\n",
      "Next Action: [-0.581\n",
      "Step reward: -15.91142417275562, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.1531982440016\n",
      "Episode: 27, Step: 60\n",
      "Next Action: [-0.533\n",
      "Step reward: -15.9233529922714, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.076551236273\n",
      "Episode: 27, Step: 61\n",
      "Next Action: [-0.582\n",
      "Step reward: -15.939300189471226, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.0158514257441\n",
      "Episode: 27, Step: 62\n",
      "Next Action: [-0.471\n",
      "Step reward: -15.944818232757923, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -998.9606696585021\n",
      "Episode: 27, Step: 63\n",
      "Next Action: [-0.791\n",
      "Step reward: -15.983114380927486, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1014.9437840394296\n",
      "Episode: 27, Step: 64\n",
      "Next Action: [-0.996\n",
      "Step reward: -15.994222340138968, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1030.9380063795686\n",
      "Episode: 27, Step: 65\n",
      "Next Action: [-1.573\n",
      "Step reward: -15.983948813596507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.921955193165\n",
      "Episode: 27, Step: 66\n",
      "Next Action: [-1.196\n",
      "Step reward: -15.977068590921336, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1062.8990237840865\n",
      "Episode: 27, Step: 67\n",
      "Next Action: [-1.443\n",
      "Step reward: -15.952793154648264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1078.8518169387348\n",
      "Episode: 27, Step: 68\n",
      "Next Action: [-1.801\n",
      "Step reward: -15.933897930845953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1094.7857148695807\n",
      "Episode: 27, Step: 69\n",
      "Next Action: [-1.827\n",
      "Step reward: -15.92837397092593, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1110.7140888405067\n",
      "Episode: 27, Step: 70\n",
      "Next Action: [-1.479\n",
      "Step reward: -15.941142459531903, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.6552313000386\n",
      "Episode: 27, Step: 71\n",
      "Next Action: [-1.590\n",
      "Step reward: -15.98574200807542, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.640973308114\n",
      "Episode: 27, Step: 72\n",
      "Next Action: [-1.475\n",
      "Step reward: -15.978051996871605, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.6190253049856\n",
      "Episode: 27, Step: 73\n",
      "Next Action: [-1.265\n",
      "Step reward: -15.995803043632797, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.6148283486184\n",
      "Episode: 27, Step: 74\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.999863868934495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1190.6146922175528\n",
      "Episode: 27, Step: 75\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.988418258375077, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1206.603110475928\n",
      "Episode: 27, Step: 76\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.991784278045674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1222.5948947539737\n",
      "Episode: 27, Step: 77\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.978464744164457, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1238.5733594981382\n",
      "Episode: 27, Step: 78\n",
      "Next Action: [-1.313\n",
      "Step reward: -15.970170230388751, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1254.543529728527\n",
      "Episode: 27, Step: 79\n",
      "Next Action: [-1.239\n",
      "Step reward: -15.969657156999189, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1270.513186885526\n",
      "Episode: 27, Step: 80\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.985016465245296, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.4982033507713\n",
      "Episode: 27, Step: 81\n",
      "Next Action: [-1.092\n",
      "Step reward: -15.994389859416366, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.4925932101876\n",
      "Episode: 27, Step: 82\n",
      "Next Action: [-1.324\n",
      "Step reward: -15.987247974261988, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.4798411844495\n",
      "Episode: 27, Step: 83\n",
      "Next Action: [-1.258\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.4798411844495\n",
      "Episode: 27, Step: 84\n",
      "Next Action: [-1.441\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.4798411844495\n",
      "Episode: 27, Step: 85\n",
      "Next Action: [-1.252\n",
      "Step reward: -15.990163498791443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.4700046832409\n",
      "Episode: 27, Step: 86\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.969031700345562, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.4390363835864\n",
      "Episode: 27, Step: 87\n",
      "Next Action: [-1.423\n",
      "Step reward: -15.94994636803405, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.3889827516205\n",
      "Episode: 27, Step: 88\n",
      "Next Action: [-1.248\n",
      "Step reward: -15.939174864347683, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.3281576159682\n",
      "Episode: 27, Step: 89\n",
      "Next Action: [-1.450\n",
      "Step reward: -15.936893114099243, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.2650507300675\n",
      "Episode: 27, Step: 90\n",
      "Next Action: [-1.459\n",
      "Step reward: -15.944676525204816, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.2097272552724\n",
      "Episode: 27, Step: 91\n",
      "Next Action: [-1.498\n",
      "Step reward: -15.94578022273362, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.155507478006\n",
      "Episode: 27, Step: 92\n",
      "Next Action: [-1.491\n",
      "Step reward: -15.947354269699602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.1028617477057\n",
      "Episode: 27, Step: 93\n",
      "Next Action: [-1.236\n",
      "Step reward: -15.968898720897538, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1494.0717604686033\n",
      "Episode: 27, Step: 94\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.977667526199118, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1510.0494279948025\n",
      "Episode: 27, Step: 95\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.988928750373667, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1526.038356745176\n",
      "Episode: 27, Step: 96\n",
      "Next Action: [-1.406\n",
      "Step reward: -15.991819943113647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1542.0301766882897\n",
      "Episode: 27, Step: 97\n",
      "Next Action: [-1.552\n",
      "Step reward: -15.979474272156125, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.0096509604457\n",
      "Episode: 27, Step: 98\n",
      "Next Action: [-1.612\n",
      "Step reward: -15.973614659819274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1573.983265620265\n",
      "Episode: 27, Step: 99\n",
      "Next Action: [-1.732\n",
      "Step reward: -15.981908195789284, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1589.9651738160542\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 2.683851718902588\n",
      "Actor loss: 41.54401397705078\n",
      "Critic loss: 10.420385360717773\n",
      "Actor loss: 32.331787109375\n",
      "Critic loss: 1.5793215036392212\n",
      "Actor loss: 48.38402557373047\n",
      "Critic loss: 2.3568625450134277\n",
      "Actor loss: 38.81980895996094\n",
      "Critic loss: 30.465042114257812\n",
      "Actor loss: 27.973072052001953\n",
      "Critic loss: 15.85918140411377\n",
      "Actor loss: 41.113739013671875\n",
      "Critic loss: 6.005196571350098\n",
      "Actor loss: 54.51862716674805\n",
      "Critic loss: 2.2950258255004883\n",
      "Actor loss: 51.39400100708008\n",
      "Critic loss: 2.0346102714538574\n",
      "Actor loss: 44.826637268066406\n",
      "Critic loss: 5.428508281707764\n",
      "Actor loss: 36.44925308227539\n",
      "Episode: 28\n",
      "Episode: 28, Step: 0\n",
      "Next Action: [-1.423\n",
      "Step reward: -12.424169803242064, Next State: [-1.\n",
      "Total episode reward: -12.424169803242064\n",
      "Episode: 28, Step: 1\n",
      "Next Action: [-1.572\n",
      "Step reward: -14.980179023108658, Next State: [-1.\n",
      "Total episode reward: -27.40434882635072\n",
      "Episode: 28, Step: 2\n",
      "Next Action: [-1.519\n",
      "Step reward: -15.64844450512785, Next State: [-1. \n",
      "Total episode reward: -43.05279333147857\n",
      "Episode: 28, Step: 3\n",
      "Next Action: [-1.320\n",
      "Step reward: -15.945741974867605, Next State: [-1.\n",
      "Total episode reward: -58.99853530634617\n",
      "Episode: 28, Step: 4\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.980708871626323, Next State: [-1.\n",
      "Total episode reward: -74.97924417797249\n",
      "Episode: 28, Step: 5\n",
      "Next Action: [-1.258\n",
      "Step reward: -15.965472770330935, Next State: [-1.\n",
      "Total episode reward: -90.94471694830342\n",
      "Episode: 28, Step: 6\n",
      "Next Action: [-0.950\n",
      "Step reward: -15.9766934437129, Next State: [-1.  \n",
      "Total episode reward: -106.92141039201633\n",
      "Episode: 28, Step: 7\n",
      "Next Action: [-0.951\n",
      "Step reward: -15.971845393926891, Next State: [-1.\n",
      "Total episode reward: -122.89325578594321\n",
      "Episode: 28, Step: 8\n",
      "Next Action: [-1.212\n",
      "Step reward: -15.964287586764625, Next State: [-1.\n",
      "Total episode reward: -138.85754337270782\n",
      "Episode: 28, Step: 9\n",
      "Next Action: [-1.225\n",
      "Step reward: -15.961043739835842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.81858711254367\n",
      "Episode: 28, Step: 10\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.969980911413892, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.78856802395757\n",
      "Episode: 28, Step: 11\n",
      "Next Action: [-1.330\n",
      "Step reward: -15.968741856846695, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.75730988080426\n",
      "Episode: 28, Step: 12\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.968829224758165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.72613910556242\n",
      "Episode: 28, Step: 13\n",
      "Next Action: [-1.206\n",
      "Step reward: -15.962971656664548, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.68911076222696\n",
      "Episode: 28, Step: 14\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.972702870301891, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.66181363252886\n",
      "Episode: 28, Step: 15\n",
      "Next Action: [-1.534\n",
      "Step reward: -15.973721099440949, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.6355347319698\n",
      "Episode: 28, Step: 16\n",
      "Next Action: [-1.328\n",
      "Step reward: -15.96781100715235, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.6033457391222\n",
      "Episode: 28, Step: 17\n",
      "Next Action: [-1.040\n",
      "Step reward: -15.97831724626476, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.5816629853869\n",
      "Episode: 28, Step: 18\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.986666491249492, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.5683294766364\n",
      "Episode: 28, Step: 19\n",
      "Next Action: [-1.034\n",
      "Step reward: -15.999295689745814, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.5676251663822\n",
      "Episode: 28, Step: 20\n",
      "Next Action: [-0.811\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.5676251663822\n",
      "Episode: 28, Step: 21\n",
      "Next Action: [-0.674\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.5676251663822\n",
      "Episode: 28, Step: 22\n",
      "Next Action: [-0.734\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.5676251663822\n",
      "Episode: 28, Step: 23\n",
      "Next Action: [-0.341\n",
      "Step reward: -15.991938645545014, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.5595638119272\n",
      "Episode: 28, Step: 24\n",
      "Next Action: [-0.509\n",
      "Step reward: -15.983047612678481, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.5426114246057\n",
      "Episode: 28, Step: 25\n",
      "Next Action: [-0.826\n",
      "Step reward: -15.968557740223837, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.51116916482954\n",
      "Episode: 28, Step: 26\n",
      "Next Action: [-0.824\n",
      "Step reward: -15.948052988669916, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.45922215349947\n",
      "Episode: 28, Step: 27\n",
      "Next Action: [-1.120\n",
      "Step reward: -15.94507307811171, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.4042952316112\n",
      "Episode: 28, Step: 28\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.955485959432123, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.3597811910433\n",
      "Episode: 28, Step: 29\n",
      "Next Action: [-0.894\n",
      "Step reward: -15.94930351394329, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.30908470498656\n",
      "Episode: 28, Step: 30\n",
      "Next Action: [-0.607\n",
      "Step reward: -15.968499409211198, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.27758411419774\n",
      "Episode: 28, Step: 31\n",
      "Next Action: [-0.956\n",
      "Step reward: -15.949969951067013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.22755406526477\n",
      "Episode: 28, Step: 32\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.94554239493204, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.1730964601968\n",
      "Episode: 28, Step: 33\n",
      "Next Action: [-1.285\n",
      "Step reward: -15.951310104852976, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.1244065650498\n",
      "Episode: 28, Step: 34\n",
      "Next Action: [-1.097\n",
      "Step reward: -15.968743375743879, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.0931499407936\n",
      "Episode: 28, Step: 35\n",
      "Next Action: [-1.253\n",
      "Step reward: -15.973267859667352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.066417800461\n",
      "Episode: 28, Step: 36\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.965950285950067, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.032368086411\n",
      "Episode: 28, Step: 37\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.97427471362792, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.0066428000389\n",
      "Episode: 28, Step: 38\n",
      "Next Action: [-1.266\n",
      "Step reward: -15.972401369555579, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.9790441695944\n",
      "Episode: 28, Step: 39\n",
      "Next Action: [-1.462\n",
      "Step reward: -15.969298582801537, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.948342752396\n",
      "Episode: 28, Step: 40\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.969607845366212, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.9179505977622\n",
      "Episode: 28, Step: 41\n",
      "Next Action: [-1.043\n",
      "Step reward: -15.974813275105962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.8927638728682\n",
      "Episode: 28, Step: 42\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.982928299665357, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.8756921725335\n",
      "Episode: 28, Step: 43\n",
      "Next Action: [-0.792\n",
      "Step reward: -15.957393061376786, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.8330852339103\n",
      "Episode: 28, Step: 44\n",
      "Next Action: [-0.925\n",
      "Step reward: -15.925031930736534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.7581171646468\n",
      "Episode: 28, Step: 45\n",
      "Next Action: [-1.018\n",
      "Step reward: -15.914390939069122, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.6725081037159\n",
      "Episode: 28, Step: 46\n",
      "Next Action: [-1.234\n",
      "Step reward: -15.944193228598008, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.6167013323139\n",
      "Episode: 28, Step: 47\n",
      "Next Action: [-1.160\n",
      "Step reward: -15.979789111955357, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.5964904442693\n",
      "Episode: 28, Step: 48\n",
      "Next Action: [-1.331\n",
      "Step reward: -15.954804684289034, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.5512951285583\n",
      "Episode: 28, Step: 49\n",
      "Next Action: [-1.360\n",
      "Step reward: -15.95533299849656, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.5066281270549\n",
      "Episode: 28, Step: 50\n",
      "Next Action: [-1.653\n",
      "Step reward: -15.945714296728667, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.4523424237835\n",
      "Episode: 28, Step: 51\n",
      "Next Action: [-1.179\n",
      "Step reward: -15.92352510249484, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.3758675262784\n",
      "Episode: 28, Step: 52\n",
      "Next Action: [-1.101\n",
      "Step reward: -15.923198311385903, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.2990658376643\n",
      "Episode: 28, Step: 53\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.96624053041754, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.2653063680818\n",
      "Episode: 28, Step: 54\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.9872780353751, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.2525844034569\n",
      "Episode: 28, Step: 55\n",
      "Next Action: [-0.673\n",
      "Step reward: -15.993417463566638, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.2460018670236\n",
      "Episode: 28, Step: 56\n",
      "Next Action: [-0.910\n",
      "Step reward: -15.99350277803237, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.239504645056\n",
      "Episode: 28, Step: 57\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.963384632413593, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.2028892774696\n",
      "Episode: 28, Step: 58\n",
      "Next Action: [-0.572\n",
      "Step reward: -15.968602430364959, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.1714917078345\n",
      "Episode: 28, Step: 59\n",
      "Next Action: [-0.564\n",
      "Step reward: -15.962692525917648, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.1341842337522\n",
      "Episode: 28, Step: 60\n",
      "Next Action: [-0.577\n",
      "Step reward: -15.994368658731767, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.128552892484\n",
      "Episode: 28, Step: 61\n",
      "Next Action: [-0.760\n",
      "Step reward: -15.994923880506231, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.1234767729902\n",
      "Episode: 28, Step: 62\n",
      "Next Action: [-0.409\n",
      "Step reward: -15.989429411852361, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.1129061848426\n",
      "Episode: 28, Step: 63\n",
      "Next Action: [-0.421\n",
      "Step reward: -15.964017882511747, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.0769240673543\n",
      "Episode: 28, Step: 64\n",
      "Next Action: [-0.714\n",
      "Step reward: -15.948136934799953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.0250610021542\n",
      "Episode: 28, Step: 65\n",
      "Next Action: [-1.097\n",
      "Step reward: -15.91293752235737, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.9379985245116\n",
      "Episode: 28, Step: 66\n",
      "Next Action: [-1.295\n",
      "Step reward: -15.946951383927074, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.8849499084388\n",
      "Episode: 28, Step: 67\n",
      "Next Action: [-1.203\n",
      "Step reward: -15.956326545947379, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.8412764543862\n",
      "Episode: 28, Step: 68\n",
      "Next Action: [-1.229\n",
      "Step reward: -15.95963758257915, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.8009140369654\n",
      "Episode: 28, Step: 69\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.956931719866716, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.7578457568322\n",
      "Episode: 28, Step: 70\n",
      "Next Action: [-1.248\n",
      "Step reward: -15.962025930308094, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.7198716871403\n",
      "Episode: 28, Step: 71\n",
      "Next Action: [-1.483\n",
      "Step reward: -15.975279367416265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.6951510545566\n",
      "Episode: 28, Step: 72\n",
      "Next Action: [-1.895\n",
      "Step reward: -15.971399803552133, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.6665508581088\n",
      "Episode: 28, Step: 73\n",
      "Next Action: [-1.772\n",
      "Step reward: -15.9604863072615, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.6270371653704\n",
      "Episode: 28, Step: 74\n",
      "Next Action: [-2.131\n",
      "Step reward: -15.955401912813777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.5824390781843\n",
      "Episode: 28, Step: 75\n",
      "Next Action: [-1.739\n",
      "Step reward: -15.958523397736345, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.5409624759207\n",
      "Episode: 28, Step: 76\n",
      "Next Action: [-1.887\n",
      "Step reward: -15.952429563182323, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.493392039103\n",
      "Episode: 28, Step: 77\n",
      "Next Action: [-1.881\n",
      "Step reward: -15.937129952878902, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.4305219919818\n",
      "Episode: 28, Step: 78\n",
      "Next Action: [-2.018\n",
      "Step reward: -15.91299673534718, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.343518727329\n",
      "Episode: 28, Step: 79\n",
      "Next Action: [-1.769\n",
      "Step reward: -15.912902534206752, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.2564212615357\n",
      "Episode: 28, Step: 80\n",
      "Next Action: [-1.729\n",
      "Step reward: -15.923874506518867, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.1802957680545\n",
      "Episode: 28, Step: 81\n",
      "Next Action: [-1.614\n",
      "Step reward: -15.929638848964585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.109934617019\n",
      "Episode: 28, Step: 82\n",
      "Next Action: [-1.532\n",
      "Step reward: -15.958905665555449, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.0688402825745\n",
      "Episode: 28, Step: 83\n",
      "Next Action: [-1.316\n",
      "Step reward: -15.966674205241846, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.0355144878163\n",
      "Episode: 28, Step: 84\n",
      "Next Action: [-1.205\n",
      "Step reward: -15.976527373340394, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.0120418611566\n",
      "Episode: 28, Step: 85\n",
      "Next Action: [-1.119\n",
      "Step reward: -15.97944417796923, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.991486039126\n",
      "Episode: 28, Step: 86\n",
      "Next Action: [-1.020\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.991486039126\n",
      "Episode: 28, Step: 87\n",
      "Next Action: [-0.836\n",
      "Step reward: -15.995325691457511, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.9868117305834\n",
      "Episode: 28, Step: 88\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.982705572161736, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.969517302745\n",
      "Episode: 28, Step: 89\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.977461140122257, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.9469784428672\n",
      "Episode: 28, Step: 90\n",
      "Next Action: [-0.780\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.9469784428672\n",
      "Episode: 28, Step: 91\n",
      "Next Action: [-0.788\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.9469784428672\n",
      "Episode: 28, Step: 92\n",
      "Next Action: [-0.740\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.9469784428672\n",
      "Episode: 28, Step: 93\n",
      "Next Action: [-0.748\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.9469784428672\n",
      "Episode: 28, Step: 94\n",
      "Next Action: [-0.604\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.9469784428672\n",
      "Episode: 28, Step: 95\n",
      "Next Action: [-0.682\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.9469784428672\n",
      "Episode: 28, Step: 96\n",
      "Next Action: [-0.423\n",
      "Step reward: -15.996452401464971, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.9434308443322\n",
      "Episode: 28, Step: 97\n",
      "Next Action: [-0.924\n",
      "Step reward: -15.99487211740107, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.9383029617331\n",
      "Episode: 28, Step: 98\n",
      "Next Action: [-0.782\n",
      "Step reward: -15.99974417035414, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.9380471320874\n",
      "Episode: 28, Step: 99\n",
      "Next Action: [-0.921\n",
      "Step reward: -15.988054970484132, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.9261021025716\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 30.1718807220459\n",
      "Actor loss: 30.499523162841797\n",
      "Critic loss: 1.9304218292236328\n",
      "Actor loss: 35.85674285888672\n",
      "Critic loss: 1.7200144529342651\n",
      "Actor loss: 33.53593444824219\n",
      "Critic loss: 22.6667537689209\n",
      "Actor loss: 33.741050720214844\n",
      "Critic loss: 8.405373573303223\n",
      "Actor loss: 41.36314010620117\n",
      "Critic loss: 4.322570323944092\n",
      "Actor loss: 48.714290618896484\n",
      "Critic loss: 58.866676330566406\n",
      "Actor loss: 42.831661224365234\n",
      "Critic loss: 2.93200421333313\n",
      "Actor loss: 35.21774673461914\n",
      "Critic loss: 11.987730979919434\n",
      "Actor loss: 49.668479919433594\n",
      "Critic loss: 6.6232523918151855\n",
      "Actor loss: 45.46977996826172\n",
      "Episode: 29\n",
      "Episode: 29, Step: 0\n",
      "Next Action: [-0.802\n",
      "Step reward: -11.3914631643237, Next State: [-0.54\n",
      "Total episode reward: -11.3914631643237\n",
      "Episode: 29, Step: 1\n",
      "Next Action: [-0.413\n",
      "Step reward: -14.765073113757124, Next State: [-0.\n",
      "Total episode reward: -26.15653627808082\n",
      "Episode: 29, Step: 2\n",
      "Next Action: [-0.714\n",
      "Step reward: -15.513251680055031, Next State: [-1.\n",
      "Total episode reward: -41.669787958135856\n",
      "Episode: 29, Step: 3\n",
      "Next Action: [-0.545\n",
      "Step reward: -15.804589427164577, Next State: [-1.\n",
      "Total episode reward: -57.474377385300436\n",
      "Episode: 29, Step: 4\n",
      "Next Action: [-0.635\n",
      "Step reward: -15.885522850698843, Next State: [-1.\n",
      "Total episode reward: -73.35990023599928\n",
      "Episode: 29, Step: 5\n",
      "Next Action: [-0.646\n",
      "Step reward: -15.924235179725432, Next State: [-1.\n",
      "Total episode reward: -89.2841354157247\n",
      "Episode: 29, Step: 6\n",
      "Next Action: [-0.468\n",
      "Step reward: -15.928872145737813, Next State: [-1.\n",
      "Total episode reward: -105.21300756146252\n",
      "Episode: 29, Step: 7\n",
      "Next Action: [-0.523\n",
      "Step reward: -15.932490894282992, Next State: [-1.\n",
      "Total episode reward: -121.14549845574551\n",
      "Episode: 29, Step: 8\n",
      "Next Action: [-1.129\n",
      "Step reward: -15.954613630771865, Next State: [-1.\n",
      "Total episode reward: -137.10011208651738\n",
      "Episode: 29, Step: 9\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.962099465726737, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.06221155224412\n",
      "Episode: 29, Step: 10\n",
      "Next Action: [-0.982\n",
      "Step reward: -15.969433887012991, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.03164543925712\n",
      "Episode: 29, Step: 11\n",
      "Next Action: [-1.144\n",
      "Step reward: -15.946849261939406, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.97849470119652\n",
      "Episode: 29, Step: 12\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.920956195109781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.8994508963063\n",
      "Episode: 29, Step: 13\n",
      "Next Action: [-0.935\n",
      "Step reward: -15.914028263580718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.813479159887\n",
      "Episode: 29, Step: 14\n",
      "Next Action: [-0.937\n",
      "Step reward: -15.930625468026912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.7441046279139\n",
      "Episode: 29, Step: 15\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.954530495965987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.6986351238799\n",
      "Episode: 29, Step: 16\n",
      "Next Action: [-1.027\n",
      "Step reward: -15.958383137922853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.65701826180276\n",
      "Episode: 29, Step: 17\n",
      "Next Action: [-0.985\n",
      "Step reward: -15.964039491612724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.62105775341547\n",
      "Episode: 29, Step: 18\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.956221598879932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.5772793522954\n",
      "Episode: 29, Step: 19\n",
      "Next Action: [-1.298\n",
      "Step reward: -15.968657868295868, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.54593722059127\n",
      "Episode: 29, Step: 20\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.97966753669629, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.52560475728757\n",
      "Episode: 29, Step: 21\n",
      "Next Action: [-1.651\n",
      "Step reward: -15.972391021945793, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.49799577923335\n",
      "Episode: 29, Step: 22\n",
      "Next Action: [-1.417\n",
      "Step reward: -15.971607966895007, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.46960374612837\n",
      "Episode: 29, Step: 23\n",
      "Next Action: [-1.449\n",
      "Step reward: -15.97460420959062, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.444207955719\n",
      "Episode: 29, Step: 24\n",
      "Next Action: [-1.261\n",
      "Step reward: -15.981480627820202, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.42568858353917\n",
      "Episode: 29, Step: 25\n",
      "Next Action: [-1.339\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.42568858353917\n",
      "Episode: 29, Step: 26\n",
      "Next Action: [-1.151\n",
      "Step reward: -15.985569067173886, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.4112576507131\n",
      "Episode: 29, Step: 27\n",
      "Next Action: [-1.008\n",
      "Step reward: -15.988771963831722, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.4000296145448\n",
      "Episode: 29, Step: 28\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.987918363844798, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.3879479783896\n",
      "Episode: 29, Step: 29\n",
      "Next Action: [-0.533\n",
      "Step reward: -15.975533433481745, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.36348141187136\n",
      "Episode: 29, Step: 30\n",
      "Next Action: [-0.643\n",
      "Step reward: -15.972539135533902, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.33602054740527\n",
      "Episode: 29, Step: 31\n",
      "Next Action: [-0.910\n",
      "Step reward: -15.96846807695719, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.3044886243625\n",
      "Episode: 29, Step: 32\n",
      "Next Action: [-0.940\n",
      "Step reward: -15.998432705176272, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.3029213295388\n",
      "Episode: 29, Step: 33\n",
      "Next Action: [-0.863\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.3029213295388\n",
      "Episode: 29, Step: 34\n",
      "Next Action: [-0.568\n",
      "Step reward: -15.975836577303392, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.2787579068422\n",
      "Episode: 29, Step: 35\n",
      "Next Action: [-0.655\n",
      "Step reward: -15.935431609810342, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.2141895166526\n",
      "Episode: 29, Step: 36\n",
      "Next Action: [-0.780\n",
      "Step reward: -15.905176923244465, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.119366439897\n",
      "Episode: 29, Step: 37\n",
      "Next Action: [-0.947\n",
      "Step reward: -15.917663348479506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.0370297883766\n",
      "Episode: 29, Step: 38\n",
      "Next Action: [-1.168\n",
      "Step reward: -15.911887513577934, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -615.9489173019545\n",
      "Episode: 29, Step: 39\n",
      "Next Action: [-1.444\n",
      "Step reward: -15.922906504454232, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -631.8718238064087\n",
      "Episode: 29, Step: 40\n",
      "Next Action: [-1.579\n",
      "Step reward: -15.92017644643518, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -647.7920002528439\n",
      "Episode: 29, Step: 41\n",
      "Next Action: [-1.215\n",
      "Step reward: -15.931799108403913, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -663.7237993612478\n",
      "Episode: 29, Step: 42\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.93290404762679, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -679.6567034088746\n",
      "Episode: 29, Step: 43\n",
      "Next Action: [-1.320\n",
      "Step reward: -15.923576906187161, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -695.5802803150617\n",
      "Episode: 29, Step: 44\n",
      "Next Action: [-1.098\n",
      "Step reward: -15.927179428182232, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -711.507459743244\n",
      "Episode: 29, Step: 45\n",
      "Next Action: [-1.389\n",
      "Step reward: -15.946897863615316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -727.4543576068593\n",
      "Episode: 29, Step: 46\n",
      "Next Action: [-1.531\n",
      "Step reward: -15.946997465921191, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -743.4013550727805\n",
      "Episode: 29, Step: 47\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.947264905478724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -759.3486199782592\n",
      "Episode: 29, Step: 48\n",
      "Next Action: [-0.961\n",
      "Step reward: -15.939388201052193, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -775.2880081793114\n",
      "Episode: 29, Step: 49\n",
      "Next Action: [-0.870\n",
      "Step reward: -15.941638253149849, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -791.2296464324613\n",
      "Episode: 29, Step: 50\n",
      "Next Action: [-1.007\n",
      "Step reward: -15.958399509096845, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -807.1880459415581\n",
      "Episode: 29, Step: 51\n",
      "Next Action: [-0.851\n",
      "Step reward: -15.971988307133023, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -823.1600342486912\n",
      "Episode: 29, Step: 52\n",
      "Next Action: [-0.856\n",
      "Step reward: -15.959279499783987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -839.1193137484752\n",
      "Episode: 29, Step: 53\n",
      "Next Action: [-0.888\n",
      "Step reward: -15.96630764522121, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -855.0856213936964\n",
      "Episode: 29, Step: 54\n",
      "Next Action: [-0.958\n",
      "Step reward: -15.972481262424639, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -871.058102656121\n",
      "Episode: 29, Step: 55\n",
      "Next Action: [-0.822\n",
      "Step reward: -15.988420434777966, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.046523090899\n",
      "Episode: 29, Step: 56\n",
      "Next Action: [-0.740\n",
      "Step reward: -15.993127592365497, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.0396506832644\n",
      "Episode: 29, Step: 57\n",
      "Next Action: [-0.607\n",
      "Step reward: -15.989260865675899, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.0289115489403\n",
      "Episode: 29, Step: 58\n",
      "Next Action: [-0.834\n",
      "Step reward: -15.964686549816184, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -934.9935980987565\n",
      "Episode: 29, Step: 59\n",
      "Next Action: [-0.788\n",
      "Step reward: -15.963113683121906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -950.9567117818784\n",
      "Episode: 29, Step: 60\n",
      "Next Action: [-1.011\n",
      "Step reward: -15.956816569814073, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -966.9135283516925\n",
      "Episode: 29, Step: 61\n",
      "Next Action: [-1.074\n",
      "Step reward: -15.969704621789031, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -982.8832329734815\n",
      "Episode: 29, Step: 62\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.968459789952536, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -998.8516927634341\n",
      "Episode: 29, Step: 63\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.968727714530974, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1014.8204204779651\n",
      "Episode: 29, Step: 64\n",
      "Next Action: [-1.119\n",
      "Step reward: -15.968756734442623, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1030.7891772124076\n",
      "Episode: 29, Step: 65\n",
      "Next Action: [-1.385\n",
      "Step reward: -15.97063858846893, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.7598158008766\n",
      "Episode: 29, Step: 66\n",
      "Next Action: [-1.503\n",
      "Step reward: -15.98165643531073, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1062.7414722361873\n",
      "Episode: 29, Step: 67\n",
      "Next Action: [-1.289\n",
      "Step reward: -15.99104404530039, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1078.7325162814877\n",
      "Episode: 29, Step: 68\n",
      "Next Action: [-1.233\n",
      "Step reward: -15.995932910577016, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1094.7284491920648\n",
      "Episode: 29, Step: 69\n",
      "Next Action: [-1.507\n",
      "Step reward: -15.991188651149772, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1110.7196378432145\n",
      "Episode: 29, Step: 70\n",
      "Next Action: [-1.470\n",
      "Step reward: -15.981338162022388, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.700976005237\n",
      "Episode: 29, Step: 71\n",
      "Next Action: [-1.487\n",
      "Step reward: -15.956615195157308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.6575912003943\n",
      "Episode: 29, Step: 72\n",
      "Next Action: [-1.321\n",
      "Step reward: -15.96904171565327, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.6266329160476\n",
      "Episode: 29, Step: 73\n",
      "Next Action: [-1.342\n",
      "Step reward: -15.939233608143443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.565866524191\n",
      "Episode: 29, Step: 74\n",
      "Next Action: [-1.559\n",
      "Step reward: -15.925007678429049, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1190.49087420262\n",
      "Episode: 29, Step: 75\n",
      "Next Action: [-1.358\n",
      "Step reward: -15.92131204058899, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1206.4121862432091\n",
      "Episode: 29, Step: 76\n",
      "Next Action: [-1.462\n",
      "Step reward: -15.923308239377812, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1222.335494482587\n",
      "Episode: 29, Step: 77\n",
      "Next Action: [-1.990\n",
      "Step reward: -15.94149146670514, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1238.276985949292\n",
      "Episode: 29, Step: 78\n",
      "Next Action: [-2.174\n",
      "Step reward: -15.952480504334087, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1254.2294664536262\n",
      "Episode: 29, Step: 79\n",
      "Next Action: [-2.037\n",
      "Step reward: -15.9683092735287, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1270.197775727155\n",
      "Episode: 29, Step: 80\n",
      "Next Action: [-1.790\n",
      "Step reward: -15.998504602020223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.1962803291751\n",
      "Episode: 29, Step: 81\n",
      "Next Action: [-1.608\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.1962803291751\n",
      "Episode: 29, Step: 82\n",
      "Next Action: [-1.311\n",
      "Step reward: -15.997277999217735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.1935583283928\n",
      "Episode: 29, Step: 83\n",
      "Next Action: [-1.199\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.1935583283928\n",
      "Episode: 29, Step: 84\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.985230120488042, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.1787884488808\n",
      "Episode: 29, Step: 85\n",
      "Next Action: [-1.571\n",
      "Step reward: -15.983934433500705, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.1627228823816\n",
      "Episode: 29, Step: 86\n",
      "Next Action: [-1.931\n",
      "Step reward: -15.981853403472916, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.1445762858546\n",
      "Episode: 29, Step: 87\n",
      "Next Action: [-1.959\n",
      "Step reward: -15.986557379056519, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.131133664911\n",
      "Episode: 29, Step: 88\n",
      "Next Action: [-1.606\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.131133664911\n",
      "Episode: 29, Step: 89\n",
      "Next Action: [-1.608\n",
      "Step reward: -15.98222832001006, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.113361984921\n",
      "Episode: 29, Step: 90\n",
      "Next Action: [-1.480\n",
      "Step reward: -15.975208663930756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.0885706488518\n",
      "Episode: 29, Step: 91\n",
      "Next Action: [-1.462\n",
      "Step reward: -15.964926711634549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.0534973604863\n",
      "Episode: 29, Step: 92\n",
      "Next Action: [-1.491\n",
      "Step reward: -15.963868372282926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.0173657327691\n",
      "Episode: 29, Step: 93\n",
      "Next Action: [-1.182\n",
      "Step reward: -15.975090509960676, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.9924562427298\n",
      "Episode: 29, Step: 94\n",
      "Next Action: [-1.380\n",
      "Step reward: -15.95817734649548, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.9506335892252\n",
      "Episode: 29, Step: 95\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.933086017554942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.88371960678\n",
      "Episode: 29, Step: 96\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.941970640838063, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1541.825690247618\n",
      "Episode: 29, Step: 97\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.950956198084201, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1557.7766464457022\n",
      "Episode: 29, Step: 98\n",
      "Next Action: [-0.316\n",
      "Step reward: -15.968307708325193, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1573.7449541540275\n",
      "Episode: 29, Step: 99\n",
      "Next Action: [-0.162\n",
      "Step reward: -15.97010384785465, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1589.7150580018822\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 46.926170349121094\n",
      "Actor loss: 34.58509063720703\n",
      "Critic loss: 3.9849510192871094\n",
      "Actor loss: 54.4263801574707\n",
      "Critic loss: 22.589284896850586\n",
      "Actor loss: 34.47114181518555\n",
      "Critic loss: 74.54978942871094\n",
      "Actor loss: 56.293678283691406\n",
      "Critic loss: 2.626974105834961\n",
      "Actor loss: 53.535400390625\n",
      "Critic loss: 4.5133442878723145\n",
      "Actor loss: 56.527244567871094\n",
      "Critic loss: 22.337804794311523\n",
      "Actor loss: 70.41963195800781\n",
      "Critic loss: 20.074138641357422\n",
      "Actor loss: 39.66783905029297\n",
      "Critic loss: 25.660890579223633\n",
      "Actor loss: 63.797088623046875\n",
      "Critic loss: 54.294315338134766\n",
      "Actor loss: 40.435546875\n",
      "Episode: 30\n",
      "Episode: 30, Step: 0\n",
      "Next Action: [-0.023\n",
      "Step reward: -12.049794265661602, Next State: [ 0.\n",
      "Total episode reward: -12.049794265661602\n",
      "Episode: 30, Step: 1\n",
      "Next Action: [-0.060\n",
      "Step reward: -14.854031191613505, Next State: [ 0.\n",
      "Total episode reward: -26.903825457275104\n",
      "Episode: 30, Step: 2\n",
      "Next Action: [-0.286\n",
      "Step reward: -15.626853570753534, Next State: [ 8.\n",
      "Total episode reward: -42.530679028028636\n",
      "Episode: 30, Step: 3\n",
      "Next Action: [-4.134\n",
      "Step reward: -15.742140924678115, Next State: [-0.\n",
      "Total episode reward: -58.272819952706755\n",
      "Episode: 30, Step: 4\n",
      "Next Action: [-0.623\n",
      "Step reward: -15.823812269618688, Next State: [-0.\n",
      "Total episode reward: -74.09663222232544\n",
      "Episode: 30, Step: 5\n",
      "Next Action: [-0.711\n",
      "Step reward: -15.863067657300952, Next State: [-1.\n",
      "Total episode reward: -89.95969987962638\n",
      "Episode: 30, Step: 6\n",
      "Next Action: [-7.073\n",
      "Step reward: -15.892181194922292, Next State: [-1.\n",
      "Total episode reward: -105.85188107454867\n",
      "Episode: 30, Step: 7\n",
      "Next Action: [-0.978\n",
      "Step reward: -15.904366959863568, Next State: [-1.\n",
      "Total episode reward: -121.75624803441224\n",
      "Episode: 30, Step: 8\n",
      "Next Action: [-0.944\n",
      "Step reward: -15.888343830089443, Next State: [-1.\n",
      "Total episode reward: -137.64459186450168\n",
      "Episode: 30, Step: 9\n",
      "Next Action: [-1.055\n",
      "Step reward: -15.90402000624986, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.54861187075153\n",
      "Episode: 30, Step: 10\n",
      "Next Action: [-1.316\n",
      "Step reward: -15.95607706893136, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.5046889396829\n",
      "Episode: 30, Step: 11\n",
      "Next Action: [-1.067\n",
      "Step reward: -15.964078831410195, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.4687677710931\n",
      "Episode: 30, Step: 12\n",
      "Next Action: [-1.175\n",
      "Step reward: -15.977001432137824, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.44576920323092\n",
      "Episode: 30, Step: 13\n",
      "Next Action: [-1.122\n",
      "Step reward: -15.991267955764632, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.43703715899557\n",
      "Episode: 30, Step: 14\n",
      "Next Action: [-0.820\n",
      "Step reward: -15.994893027696355, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.43193018669191\n",
      "Episode: 30, Step: 15\n",
      "Next Action: [-0.934\n",
      "Step reward: -15.996316060601602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.42824624729352\n",
      "Episode: 30, Step: 16\n",
      "Next Action: [-0.721\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.4282462472935\n",
      "Episode: 30, Step: 17\n",
      "Next Action: [-0.572\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.4282462472935\n",
      "Episode: 30, Step: 18\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.999162180758928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.4274084280524\n",
      "Episode: 30, Step: 19\n",
      "Next Action: [-0.609\n",
      "Step reward: -15.99500901981605, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.42241744786844\n",
      "Episode: 30, Step: 20\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.981890678725081, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.40430812659355\n",
      "Episode: 30, Step: 21\n",
      "Next Action: [-4.809\n",
      "Step reward: -15.977555441219861, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.3818635678134\n",
      "Episode: 30, Step: 22\n",
      "Next Action: [-0.504\n",
      "Step reward: -15.947151611631783, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.3290151794452\n",
      "Episode: 30, Step: 23\n",
      "Next Action: [-0.429\n",
      "Step reward: -15.938495330583525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.2675105100287\n",
      "Episode: 30, Step: 24\n",
      "Next Action: [-0.419\n",
      "Step reward: -15.935956572028918, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.20346708205767\n",
      "Episode: 30, Step: 25\n",
      "Next Action: [-0.522\n",
      "Step reward: -15.963266994599527, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.1667340766572\n",
      "Episode: 30, Step: 26\n",
      "Next Action: [-0.476\n",
      "Step reward: -15.969966210657255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.13670028731445\n",
      "Episode: 30, Step: 27\n",
      "Next Action: [-0.616\n",
      "Step reward: -15.979470063504758, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.11617035081923\n",
      "Episode: 30, Step: 28\n",
      "Next Action: [-0.795\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.11617035081923\n",
      "Episode: 30, Step: 29\n",
      "Next Action: [-0.893\n",
      "Step reward: -15.994728673795588, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.1108990246148\n",
      "Episode: 30, Step: 30\n",
      "Next Action: [-0.880\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.1108990246148\n",
      "Episode: 30, Step: 31\n",
      "Next Action: [-0.894\n",
      "Step reward: -15.984702615134758, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.0956016397496\n",
      "Episode: 30, Step: 32\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.984661276385193, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.0802629161348\n",
      "Episode: 30, Step: 33\n",
      "Next Action: [-0.522\n",
      "Step reward: -15.98390851626177, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.0641714323965\n",
      "Episode: 30, Step: 34\n",
      "Next Action: [-1.082\n",
      "Step reward: -15.980001865614948, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.0441732980114\n",
      "Episode: 30, Step: 35\n",
      "Next Action: [-1.336\n",
      "Step reward: -15.981992552040882, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.0261658500523\n",
      "Episode: 30, Step: 36\n",
      "Next Action: [-1.458\n",
      "Step reward: -15.983661795398353, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.0098276454506\n",
      "Episode: 30, Step: 37\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.999451125743457, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.0092787711941\n",
      "Episode: 30, Step: 38\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.999078407466069, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.0083571786602\n",
      "Episode: 30, Step: 39\n",
      "Next Action: [-1.386\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.0083571786602\n",
      "Episode: 30, Step: 40\n",
      "Next Action: [-1.650\n",
      "Step reward: -15.99238644703642, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.0007436256966\n",
      "Episode: 30, Step: 41\n",
      "Next Action: [-1.793\n",
      "Step reward: -15.984556897389492, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.985300523086\n",
      "Episode: 30, Step: 42\n",
      "Next Action: [-1.473\n",
      "Step reward: -15.99103742705515, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.9763379501412\n",
      "Episode: 30, Step: 43\n",
      "Next Action: [-1.028\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.9763379501412\n",
      "Episode: 30, Step: 44\n",
      "Next Action: [-1.340\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.9763379501412\n",
      "Episode: 30, Step: 45\n",
      "Next Action: [-1.238\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.9763379501412\n",
      "Episode: 30, Step: 46\n",
      "Next Action: [-1.257\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.9763379501412\n",
      "Episode: 30, Step: 47\n",
      "Next Action: [-0.904\n",
      "Step reward: -15.989859667780134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.9661976179214\n",
      "Episode: 30, Step: 48\n",
      "Next Action: [-0.622\n",
      "Step reward: -15.986497870702932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.9526954886243\n",
      "Episode: 30, Step: 49\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.972885112747088, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.9255806013714\n",
      "Episode: 30, Step: 50\n",
      "Next Action: [-0.899\n",
      "Step reward: -15.971100598068997, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.8966811994404\n",
      "Episode: 30, Step: 51\n",
      "Next Action: [-0.670\n",
      "Step reward: -15.970526270030545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.8672074694709\n",
      "Episode: 30, Step: 52\n",
      "Next Action: [-0.900\n",
      "Step reward: -15.972404892658615, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.8396123621295\n",
      "Episode: 30, Step: 53\n",
      "Next Action: [-0.806\n",
      "Step reward: -15.977327127053954, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.8169394891835\n",
      "Episode: 30, Step: 54\n",
      "Next Action: [-1.211\n",
      "Step reward: -15.967373276850207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.7843127660337\n",
      "Episode: 30, Step: 55\n",
      "Next Action: [-1.087\n",
      "Step reward: -15.964314990975081, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.7486277570088\n",
      "Episode: 30, Step: 56\n",
      "Next Action: [-1.612\n",
      "Step reward: -15.972444545966018, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.7210723029748\n",
      "Episode: 30, Step: 57\n",
      "Next Action: [-1.560\n",
      "Step reward: -15.98113031657597, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.7022026195508\n",
      "Episode: 30, Step: 58\n",
      "Next Action: [-1.825\n",
      "Step reward: -15.96696138002335, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.6691639995742\n",
      "Episode: 30, Step: 59\n",
      "Next Action: [-1.681\n",
      "Step reward: -15.967057744059405, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.6362217436335\n",
      "Episode: 30, Step: 60\n",
      "Next Action: [-1.707\n",
      "Step reward: -15.956008853849228, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.5922305974827\n",
      "Episode: 30, Step: 61\n",
      "Next Action: [-1.660\n",
      "Step reward: -15.941505630887006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.5337362283698\n",
      "Episode: 30, Step: 62\n",
      "Next Action: [-1.428\n",
      "Step reward: -15.939574840394616, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.4733110687644\n",
      "Episode: 30, Step: 63\n",
      "Next Action: [-1.082\n",
      "Step reward: -15.93863657132572, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.41194764009\n",
      "Episode: 30, Step: 64\n",
      "Next Action: [-1.081\n",
      "Step reward: -15.950321801983213, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.3622694420733\n",
      "Episode: 30, Step: 65\n",
      "Next Action: [-0.953\n",
      "Step reward: -15.944824019926633, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.307093462\n",
      "Episode: 30, Step: 66\n",
      "Next Action: [-1.181\n",
      "Step reward: -15.914939300417686, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.2220327624177\n",
      "Episode: 30, Step: 67\n",
      "Next Action: [-1.566\n",
      "Step reward: -15.906858973807982, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.1288917362258\n",
      "Episode: 30, Step: 68\n",
      "Next Action: [-1.426\n",
      "Step reward: -15.898200153576683, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.0270918898025\n",
      "Episode: 30, Step: 69\n",
      "Next Action: [-1.426\n",
      "Step reward: -15.906632972278274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.9337248620807\n",
      "Episode: 30, Step: 70\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.922582647163992, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.8563075092447\n",
      "Episode: 30, Step: 71\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.939163530967127, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1143.7954710402119\n",
      "Episode: 30, Step: 72\n",
      "Next Action: [-1.023\n",
      "Step reward: -15.942862264070099, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.738333304282\n",
      "Episode: 30, Step: 73\n",
      "Next Action: [-0.921\n",
      "Step reward: -15.953114089357161, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.691447393639\n",
      "Episode: 30, Step: 74\n",
      "Next Action: [-1.166\n",
      "Step reward: -15.976216414128853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.667663807768\n",
      "Episode: 30, Step: 75\n",
      "Next Action: [-0.715\n",
      "Step reward: -15.9941260909332, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.661789898701\n",
      "Episode: 30, Step: 76\n",
      "Next Action: [-0.977\n",
      "Step reward: -15.984904495592403, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.6466943942935\n",
      "Episode: 30, Step: 77\n",
      "Next Action: [-1.185\n",
      "Step reward: -15.982656879729255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.6293512740228\n",
      "Episode: 30, Step: 78\n",
      "Next Action: [-0.725\n",
      "Step reward: -15.974294726851506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.6036460008743\n",
      "Episode: 30, Step: 79\n",
      "Next Action: [-0.795\n",
      "Step reward: -15.966941057397682, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.570587058272\n",
      "Episode: 30, Step: 80\n",
      "Next Action: [-0.753\n",
      "Step reward: -15.985614449553863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.5562015078258\n",
      "Episode: 30, Step: 81\n",
      "Next Action: [-0.868\n",
      "Step reward: -15.995473428395764, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.5516749362216\n",
      "Episode: 30, Step: 82\n",
      "Next Action: [-0.564\n",
      "Step reward: -15.976560954837824, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.5282358910595\n",
      "Episode: 30, Step: 83\n",
      "Next Action: [-0.262\n",
      "Step reward: -15.977785268178897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.5060211592383\n",
      "Episode: 30, Step: 84\n",
      "Next Action: [-0.186\n",
      "Step reward: -15.988664701294908, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.4946858605333\n",
      "Episode: 30, Step: 85\n",
      "Next Action: [-0.251\n",
      "Step reward: -15.997582756789818, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.4922686173231\n",
      "Episode: 30, Step: 86\n",
      "Next Action: [-0.441\n",
      "Step reward: -15.986499737710403, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.4787683550335\n",
      "Episode: 30, Step: 87\n",
      "Next Action: [-0.394\n",
      "Step reward: -15.961722120076796, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.4404904751102\n",
      "Episode: 30, Step: 88\n",
      "Next Action: [-0.518\n",
      "Step reward: -15.94919526358129, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.3896857386915\n",
      "Episode: 30, Step: 89\n",
      "Next Action: [-0.754\n",
      "Step reward: -15.939585541379094, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.3292712800705\n",
      "Episode: 30, Step: 90\n",
      "Next Action: [-1.153\n",
      "Step reward: -15.944746768096127, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.2740180481667\n",
      "Episode: 30, Step: 91\n",
      "Next Action: [-1.515\n",
      "Step reward: -15.955102977584383, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.229121025751\n",
      "Episode: 30, Step: 92\n",
      "Next Action: [-1.446\n",
      "Step reward: -15.96671174734985, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.1958327731008\n",
      "Episode: 30, Step: 93\n",
      "Next Action: [-1.650\n",
      "Step reward: -15.9993063981915, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.1951391712923\n",
      "Episode: 30, Step: 94\n",
      "Next Action: [-1.736\n",
      "Step reward: -15.996538474080369, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.1916776453727\n",
      "Episode: 30, Step: 95\n",
      "Next Action: [-1.773\n",
      "Step reward: -15.99116969044399, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.1828473358166\n",
      "Episode: 30, Step: 96\n",
      "Next Action: [-1.654\n",
      "Step reward: -15.99164865081638, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.174495986633\n",
      "Episode: 30, Step: 97\n",
      "Next Action: [-1.677\n",
      "Step reward: -15.97617358681477, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.1506695734479\n",
      "Episode: 30, Step: 98\n",
      "Next Action: [-1.416\n",
      "Step reward: -15.963359695468593, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.1140292689165\n",
      "Episode: 30, Step: 99\n",
      "Next Action: [-1.680\n",
      "Step reward: -15.955648333854574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.069677602771\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 5.848230838775635\n",
      "Actor loss: 54.88831329345703\n",
      "Critic loss: 1.766062617301941\n",
      "Actor loss: 44.542137145996094\n",
      "Critic loss: 1.9754406213760376\n",
      "Actor loss: 48.39374542236328\n",
      "Critic loss: 2.0017333030700684\n",
      "Actor loss: 29.838903427124023\n",
      "Critic loss: 17.849512100219727\n",
      "Actor loss: 38.95307159423828\n",
      "Critic loss: 3.3772573471069336\n",
      "Actor loss: 28.1816349029541\n",
      "Critic loss: 13.243637084960938\n",
      "Actor loss: 29.738332748413086\n",
      "Critic loss: 2.2001914978027344\n",
      "Actor loss: 29.261791229248047\n",
      "Critic loss: 40.282772064208984\n",
      "Actor loss: 34.508522033691406\n",
      "Critic loss: 25.05733299255371\n",
      "Actor loss: 27.828720092773438\n",
      "Episode: 31\n",
      "Episode: 31, Step: 0\n",
      "Next Action: [-1.407\n",
      "Step reward: -11.713686215685893, Next State: [-1.\n",
      "Total episode reward: -11.713686215685893\n",
      "Episode: 31, Step: 1\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.000152482399184, Next State: [-1.\n",
      "Total episode reward: -26.713838698085077\n",
      "Episode: 31, Step: 2\n",
      "Next Action: [-1.453\n",
      "Step reward: -15.626224601802974, Next State: [-1.\n",
      "Total episode reward: -42.34006329988805\n",
      "Episode: 31, Step: 3\n",
      "Next Action: [-1.384\n",
      "Step reward: -15.843621732210199, Next State: [-1.\n",
      "Total episode reward: -58.18368503209825\n",
      "Episode: 31, Step: 4\n",
      "Next Action: [-1.621\n",
      "Step reward: -15.947155969856256, Next State: [-1.\n",
      "Total episode reward: -74.13084100195451\n",
      "Episode: 31, Step: 5\n",
      "Next Action: [-1.553\n",
      "Step reward: -15.993876997465623, Next State: [-1.\n",
      "Total episode reward: -90.12471799942013\n",
      "Episode: 31, Step: 6\n",
      "Next Action: [-1.632\n",
      "Step reward: -15.99387263762492, Next State: [-1. \n",
      "Total episode reward: -106.11859063704505\n",
      "Episode: 31, Step: 7\n",
      "Next Action: [-1.760\n",
      "Step reward: -15.998478193031067, Next State: [-1.\n",
      "Total episode reward: -122.11706883007611\n",
      "Episode: 31, Step: 8\n",
      "Next Action: [-1.541\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Total episode reward: -138.1170688300761\n",
      "Episode: 31, Step: 9\n",
      "Next Action: [-1.822\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.1170688300761\n",
      "Episode: 31, Step: 10\n",
      "Next Action: [-1.646\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.1170688300761\n",
      "Episode: 31, Step: 11\n",
      "Next Action: [-1.562\n",
      "Step reward: -15.983593378189855, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.10066220826596\n",
      "Episode: 31, Step: 12\n",
      "Next Action: [-1.528\n",
      "Step reward: -15.968751306652607, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.06941351491855\n",
      "Episode: 31, Step: 13\n",
      "Next Action: [-1.396\n",
      "Step reward: -15.969204813994638, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.0386183289132\n",
      "Episode: 31, Step: 14\n",
      "Next Action: [-1.252\n",
      "Step reward: -15.982131714111427, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.02075004302463\n",
      "Episode: 31, Step: 15\n",
      "Next Action: [-1.318\n",
      "Step reward: -15.992881933548894, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.01363197657352\n",
      "Episode: 31, Step: 16\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.996948816257476, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.010580792831\n",
      "Episode: 31, Step: 17\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.985647779913315, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.9962285727443\n",
      "Episode: 31, Step: 18\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.955961174395757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.9521897471401\n",
      "Episode: 31, Step: 19\n",
      "Next Action: [-7.997\n",
      "Step reward: -15.95935277201587, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.91154251915594\n",
      "Episode: 31, Step: 20\n",
      "Next Action: [-0.658\n",
      "Step reward: -15.975710752296479, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.88725327145244\n",
      "Episode: 31, Step: 21\n",
      "Next Action: [-0.626\n",
      "Step reward: -15.948832121991197, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.8360853934436\n",
      "Episode: 31, Step: 22\n",
      "Next Action: [-0.539\n",
      "Step reward: -15.939392249289616, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.7754776427332\n",
      "Episode: 31, Step: 23\n",
      "Next Action: [-0.152\n",
      "Step reward: -15.952679435771753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.728157078505\n",
      "Episode: 31, Step: 24\n",
      "Next Action: [-0.388\n",
      "Step reward: -15.965881537675928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.69403861618093\n",
      "Episode: 31, Step: 25\n",
      "Next Action: [-0.511\n",
      "Step reward: -15.987886245524047, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.681924861705\n",
      "Episode: 31, Step: 26\n",
      "Next Action: [-0.323\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.681924861705\n",
      "Episode: 31, Step: 27\n",
      "Next Action: [-0.454\n",
      "Step reward: -15.987484889881085, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.6694097515861\n",
      "Episode: 31, Step: 28\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.98897109450051, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.65838084608663\n",
      "Episode: 31, Step: 29\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.997809269249753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.65619011533636\n",
      "Episode: 31, Step: 30\n",
      "Next Action: [-1.037\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.65619011533636\n",
      "Episode: 31, Step: 31\n",
      "Next Action: [-1.398\n",
      "Step reward: -15.99333100077577, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.6495211161121\n",
      "Episode: 31, Step: 32\n",
      "Next Action: [-1.122\n",
      "Step reward: -15.9947457486782, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.6442668647903\n",
      "Episode: 31, Step: 33\n",
      "Next Action: [-0.816\n",
      "Step reward: -15.99469078064141, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.6389576454318\n",
      "Episode: 31, Step: 34\n",
      "Next Action: [-1.207\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.6389576454318\n",
      "Episode: 31, Step: 35\n",
      "Next Action: [-1.362\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.6389576454318\n",
      "Episode: 31, Step: 36\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.98594619847457, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.6249038439064\n",
      "Episode: 31, Step: 37\n",
      "Next Action: [-1.203\n",
      "Step reward: -15.973205377408611, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.598109221315\n",
      "Episode: 31, Step: 38\n",
      "Next Action: [-1.133\n",
      "Step reward: -15.96901048180959, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.5671197031246\n",
      "Episode: 31, Step: 39\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.968746930503015, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.5358666336276\n",
      "Episode: 31, Step: 40\n",
      "Next Action: [-1.294\n",
      "Step reward: -15.96386214420035, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.499728777828\n",
      "Episode: 31, Step: 41\n",
      "Next Action: [-1.587\n",
      "Step reward: -15.963298113056164, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.4630268908841\n",
      "Episode: 31, Step: 42\n",
      "Next Action: [-1.549\n",
      "Step reward: -15.991371077602308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.4543979684863\n",
      "Episode: 31, Step: 43\n",
      "Next Action: [-1.492\n",
      "Step reward: -15.985062536136825, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.4394605046232\n",
      "Episode: 31, Step: 44\n",
      "Next Action: [-1.769\n",
      "Step reward: -15.974068441465398, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.4135289460886\n",
      "Episode: 31, Step: 45\n",
      "Next Action: [-2.040\n",
      "Step reward: -15.975833777458066, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.3893627235467\n",
      "Episode: 31, Step: 46\n",
      "Next Action: [-1.925\n",
      "Step reward: -15.983317069354621, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.3726797929013\n",
      "Episode: 31, Step: 47\n",
      "Next Action: [-1.609\n",
      "Step reward: -15.999267094249038, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.3719468871503\n",
      "Episode: 31, Step: 48\n",
      "Next Action: [-1.249\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.3719468871503\n",
      "Episode: 31, Step: 49\n",
      "Next Action: [-1.097\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.3719468871503\n",
      "Episode: 31, Step: 50\n",
      "Next Action: [-1.012\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.3719468871503\n",
      "Episode: 31, Step: 51\n",
      "Next Action: [-0.918\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.3719468871503\n",
      "Episode: 31, Step: 52\n",
      "Next Action: [-1.047\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.3719468871503\n",
      "Episode: 31, Step: 53\n",
      "Next Action: [-1.078\n",
      "Step reward: -15.989259485109246, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.3612063722596\n",
      "Episode: 31, Step: 54\n",
      "Next Action: [-1.223\n",
      "Step reward: -15.986390462718303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.3475968349779\n",
      "Episode: 31, Step: 55\n",
      "Next Action: [-1.085\n",
      "Step reward: -15.977928963984542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.3255257989625\n",
      "Episode: 31, Step: 56\n",
      "Next Action: [-1.166\n",
      "Step reward: -15.983464341815987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.3089901407785\n",
      "Episode: 31, Step: 57\n",
      "Next Action: [-0.947\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.3089901407785\n",
      "Episode: 31, Step: 58\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.98936319623197, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.2983533370104\n",
      "Episode: 31, Step: 59\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.974221720264167, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.2725750572746\n",
      "Episode: 31, Step: 60\n",
      "Next Action: [-0.947\n",
      "Step reward: -15.968894830144832, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.2414698874194\n",
      "Episode: 31, Step: 61\n",
      "Next Action: [-1.217\n",
      "Step reward: -15.967227647480463, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.2086975348999\n",
      "Episode: 31, Step: 62\n",
      "Next Action: [-1.292\n",
      "Step reward: -15.97064345876809, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.179340993668\n",
      "Episode: 31, Step: 63\n",
      "Next Action: [-1.433\n",
      "Step reward: -15.974092690117336, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.1534336837854\n",
      "Episode: 31, Step: 64\n",
      "Next Action: [-1.495\n",
      "Step reward: -15.969986249640408, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.1234199334258\n",
      "Episode: 31, Step: 65\n",
      "Next Action: [-1.463\n",
      "Step reward: -15.975231179958318, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.0986511133842\n",
      "Episode: 31, Step: 66\n",
      "Next Action: [-1.149\n",
      "Step reward: -15.983163134775294, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.0818142481596\n",
      "Episode: 31, Step: 67\n",
      "Next Action: [-1.142\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.0818142481596\n",
      "Episode: 31, Step: 68\n",
      "Next Action: [-1.178\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.0818142481596\n",
      "Episode: 31, Step: 69\n",
      "Next Action: [-1.142\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.0818142481596\n",
      "Episode: 31, Step: 70\n",
      "Next Action: [-1.158\n",
      "Step reward: -15.9980617670285, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.0798760151881\n",
      "Episode: 31, Step: 71\n",
      "Next Action: [-1.204\n",
      "Step reward: -15.994319420524175, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.0741954357122\n",
      "Episode: 31, Step: 72\n",
      "Next Action: [-1.465\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.0741954357122\n",
      "Episode: 31, Step: 73\n",
      "Next Action: [-1.175\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.0741954357122\n",
      "Episode: 31, Step: 74\n",
      "Next Action: [-0.961\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.0741954357122\n",
      "Episode: 31, Step: 75\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.990558266353004, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.0647537020652\n",
      "Episode: 31, Step: 76\n",
      "Next Action: [-0.684\n",
      "Step reward: -15.984846085331542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.0495997873968\n",
      "Episode: 31, Step: 77\n",
      "Next Action: [-0.927\n",
      "Step reward: -15.967927240775756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.0175270281725\n",
      "Episode: 31, Step: 78\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.948784925680906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.9663119538534\n",
      "Episode: 31, Step: 79\n",
      "Next Action: [-1.285\n",
      "Step reward: -15.943799787098477, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.9101117409518\n",
      "Episode: 31, Step: 80\n",
      "Next Action: [-1.398\n",
      "Step reward: -15.940544322257354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.8506560632093\n",
      "Episode: 31, Step: 81\n",
      "Next Action: [-1.335\n",
      "Step reward: -15.957239462038984, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.8078955252483\n",
      "Episode: 31, Step: 82\n",
      "Next Action: [-1.344\n",
      "Step reward: -15.954834224156931, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.7627297494053\n",
      "Episode: 31, Step: 83\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.953482827270554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.7162125766758\n",
      "Episode: 31, Step: 84\n",
      "Next Action: [-1.491\n",
      "Step reward: -15.9325669752382, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.648779551914\n",
      "Episode: 31, Step: 85\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.958986065289707, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.6077656172038\n",
      "Episode: 31, Step: 86\n",
      "Next Action: [-1.487\n",
      "Step reward: -15.983269747379952, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.5910353645838\n",
      "Episode: 31, Step: 87\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.98267034153975, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1400.5737057061235\n",
      "Episode: 31, Step: 88\n",
      "Next Action: [-1.472\n",
      "Step reward: -15.9638199704622, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.5375256765858\n",
      "Episode: 31, Step: 89\n",
      "Next Action: [-1.811\n",
      "Step reward: -15.943513857257443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.4810395338432\n",
      "Episode: 31, Step: 90\n",
      "Next Action: [-1.474\n",
      "Step reward: -15.928612633631808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.4096521674749\n",
      "Episode: 31, Step: 91\n",
      "Next Action: [-1.852\n",
      "Step reward: -15.943735791464228, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.353387958939\n",
      "Episode: 31, Step: 92\n",
      "Next Action: [-1.831\n",
      "Step reward: -15.974954508551436, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.3283424674905\n",
      "Episode: 31, Step: 93\n",
      "Next Action: [-1.848\n",
      "Step reward: -15.972922994852425, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.301265462343\n",
      "Episode: 31, Step: 94\n",
      "Next Action: [-1.945\n",
      "Step reward: -15.965491963906409, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.2667574262496\n",
      "Episode: 31, Step: 95\n",
      "Next Action: [-1.854\n",
      "Step reward: -15.969095844655536, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.2358532709052\n",
      "Episode: 31, Step: 96\n",
      "Next Action: [-1.383\n",
      "Step reward: -15.980738161219032, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.2165914321242\n",
      "Episode: 31, Step: 97\n",
      "Next Action: [-1.331\n",
      "Step reward: -15.996908675067994, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.2135001071922\n",
      "Episode: 31, Step: 98\n",
      "Next Action: [-1.261\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.2135001071922\n",
      "Episode: 31, Step: 99\n",
      "Next Action: [-0.997\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.2135001071922\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 6.5598978996276855\n",
      "Actor loss: 28.829381942749023\n",
      "Critic loss: 2.819265842437744\n",
      "Actor loss: 48.373008728027344\n",
      "Critic loss: 11.106710433959961\n",
      "Actor loss: 43.333740234375\n",
      "Critic loss: 6.775047302246094\n",
      "Actor loss: 30.698467254638672\n",
      "Critic loss: 95.22488403320312\n",
      "Actor loss: 30.19179344177246\n",
      "Critic loss: 3.566190004348755\n",
      "Actor loss: 32.88833999633789\n",
      "Critic loss: 15.129531860351562\n",
      "Actor loss: 38.05693817138672\n",
      "Critic loss: 2.3419361114501953\n",
      "Actor loss: 54.41515350341797\n",
      "Critic loss: 1.9816404581069946\n",
      "Actor loss: 53.330772399902344\n",
      "Critic loss: 10.248682022094727\n",
      "Actor loss: 48.38121795654297\n",
      "Episode: 32\n",
      "Episode: 32, Step: 0\n",
      "Next Action: [-1.130\n",
      "Step reward: -12.395365215763201, Next State: [-1.\n",
      "Total episode reward: -12.395365215763201\n",
      "Episode: 32, Step: 1\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.217996843899847, Next State: [-1.\n",
      "Total episode reward: -27.613362059663046\n",
      "Episode: 32, Step: 2\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.768142606766425, Next State: [-1.\n",
      "Total episode reward: -43.38150466642947\n",
      "Episode: 32, Step: 3\n",
      "Next Action: [-0.625\n",
      "Step reward: -15.889011653968488, Next State: [-1.\n",
      "Total episode reward: -59.27051632039796\n",
      "Episode: 32, Step: 4\n",
      "Next Action: [-0.591\n",
      "Step reward: -15.95847020702188, Next State: [-1. \n",
      "Total episode reward: -75.22898652741983\n",
      "Episode: 32, Step: 5\n",
      "Next Action: [-0.485\n",
      "Step reward: -15.963538088765786, Next State: [-1.\n",
      "Total episode reward: -91.19252461618562\n",
      "Episode: 32, Step: 6\n",
      "Next Action: [-0.658\n",
      "Step reward: -15.928120094792616, Next State: [-1.\n",
      "Total episode reward: -107.12064471097824\n",
      "Episode: 32, Step: 7\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.940085415799832, Next State: [-1.\n",
      "Total episode reward: -123.06073012677807\n",
      "Episode: 32, Step: 8\n",
      "Next Action: [-0.883\n",
      "Step reward: -15.946050361208597, Next State: [-1.\n",
      "Total episode reward: -139.00678048798667\n",
      "Episode: 32, Step: 9\n",
      "Next Action: [-0.729\n",
      "Step reward: -15.959883796604013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.96666428459068\n",
      "Episode: 32, Step: 10\n",
      "Next Action: [-0.109\n",
      "Step reward: -15.952679963787421, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.9193442483781\n",
      "Episode: 32, Step: 11\n",
      "Next Action: [-0.446\n",
      "Step reward: -15.957759586944583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.87710383532269\n",
      "Episode: 32, Step: 12\n",
      "Next Action: [-0.302\n",
      "Step reward: -15.963268804783635, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.84037264010632\n",
      "Episode: 32, Step: 13\n",
      "Next Action: [-0.355\n",
      "Step reward: -15.952570089521492, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.7929427296278\n",
      "Episode: 32, Step: 14\n",
      "Next Action: [-0.200\n",
      "Step reward: -15.956182711984324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.74912544161214\n",
      "Episode: 32, Step: 15\n",
      "Next Action: [-0.004\n",
      "Step reward: -15.937345404323597, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.68647084593573\n",
      "Episode: 32, Step: 16\n",
      "Next Action: [-0.091\n",
      "Step reward: -15.936416643639609, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.62288748957536\n",
      "Episode: 32, Step: 17\n",
      "Next Action: [-0.215\n",
      "Step reward: -15.94664848719622, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.5695359767716\n",
      "Episode: 32, Step: 18\n",
      "Next Action: [-0.781\n",
      "Step reward: -15.962714704278927, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.5322506810505\n",
      "Episode: 32, Step: 19\n",
      "Next Action: [-0.969\n",
      "Step reward: -15.960587516226543, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.4928381972771\n",
      "Episode: 32, Step: 20\n",
      "Next Action: [-0.787\n",
      "Step reward: -15.9613224622365, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.4541606595136\n",
      "Episode: 32, Step: 21\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.973556579206589, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.4277172387202\n",
      "Episode: 32, Step: 22\n",
      "Next Action: [-0.328\n",
      "Step reward: -15.964332658528509, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.39204989724874\n",
      "Episode: 32, Step: 23\n",
      "Next Action: [-0.567\n",
      "Step reward: -15.957570924103848, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.3496208213526\n",
      "Episode: 32, Step: 24\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.971249061470722, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.3208698828233\n",
      "Episode: 32, Step: 25\n",
      "Next Action: [-1.207\n",
      "Step reward: -15.969642545091089, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.29051242791434\n",
      "Episode: 32, Step: 26\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.96253642209917, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.2530488500135\n",
      "Episode: 32, Step: 27\n",
      "Next Action: [-1.102\n",
      "Step reward: -15.964989128520985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.2180379785345\n",
      "Episode: 32, Step: 28\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.972921776738517, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.190959755273\n",
      "Episode: 32, Step: 29\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.96741377016701, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.15837352544\n",
      "Episode: 32, Step: 30\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.977040388715102, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.13541391415515\n",
      "Episode: 32, Step: 31\n",
      "Next Action: [-0.989\n",
      "Step reward: -15.96704838539363, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.1024622995488\n",
      "Episode: 32, Step: 32\n",
      "Next Action: [-1.255\n",
      "Step reward: -15.940693594036315, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.0431558935851\n",
      "Episode: 32, Step: 33\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.93914685664875, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.9823027502339\n",
      "Episode: 32, Step: 34\n",
      "Next Action: [-1.651\n",
      "Step reward: -15.954121241217818, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.9364239914516\n",
      "Episode: 32, Step: 35\n",
      "Next Action: [-1.548\n",
      "Step reward: -15.964337477104953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.9007614685567\n",
      "Episode: 32, Step: 36\n",
      "Next Action: [-1.604\n",
      "Step reward: -15.971679765051423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.8724412336081\n",
      "Episode: 32, Step: 37\n",
      "Next Action: [-1.578\n",
      "Step reward: -15.97285802165293, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.845299255261\n",
      "Episode: 32, Step: 38\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.984339934636377, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.8296391898974\n",
      "Episode: 32, Step: 39\n",
      "Next Action: [-1.403\n",
      "Step reward: -15.991983770801468, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.8216229606988\n",
      "Episode: 32, Step: 40\n",
      "Next Action: [-1.445\n",
      "Step reward: -15.978496893930913, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.8001198546298\n",
      "Episode: 32, Step: 41\n",
      "Next Action: [-1.630\n",
      "Step reward: -15.947318142117506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.7474379967473\n",
      "Episode: 32, Step: 42\n",
      "Next Action: [-1.759\n",
      "Step reward: -15.93240349035852, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.6798414871058\n",
      "Episode: 32, Step: 43\n",
      "Next Action: [-1.735\n",
      "Step reward: -15.937269302429952, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.6171107895358\n",
      "Episode: 32, Step: 44\n",
      "Next Action: [-1.458\n",
      "Step reward: -15.938580015777427, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.5556908053132\n",
      "Episode: 32, Step: 45\n",
      "Next Action: [-1.166\n",
      "Step reward: -15.936526394376248, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.4922171996895\n",
      "Episode: 32, Step: 46\n",
      "Next Action: [-1.031\n",
      "Step reward: -15.94682310211601, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.4390403018056\n",
      "Episode: 32, Step: 47\n",
      "Next Action: [-0.900\n",
      "Step reward: -15.94310634710041, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.382146648906\n",
      "Episode: 32, Step: 48\n",
      "Next Action: [-0.857\n",
      "Step reward: -15.936563690295783, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.3187103392017\n",
      "Episode: 32, Step: 49\n",
      "Next Action: [-0.583\n",
      "Step reward: -15.952021573236257, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.270731912438\n",
      "Episode: 32, Step: 50\n",
      "Next Action: [-0.584\n",
      "Step reward: -15.971440984710556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.2421728971485\n",
      "Episode: 32, Step: 51\n",
      "Next Action: [-0.927\n",
      "Step reward: -15.965204592465557, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.207377489614\n",
      "Episode: 32, Step: 52\n",
      "Next Action: [-0.786\n",
      "Step reward: -15.969763882116009, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.17714137173\n",
      "Episode: 32, Step: 53\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.959076321027057, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.1362176927571\n",
      "Episode: 32, Step: 54\n",
      "Next Action: [-1.102\n",
      "Step reward: -15.965449884879082, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.1016675776361\n",
      "Episode: 32, Step: 55\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.96563163171555, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.0672992093516\n",
      "Episode: 32, Step: 56\n",
      "Next Action: [-1.025\n",
      "Step reward: -15.983917095194949, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.0512163045465\n",
      "Episode: 32, Step: 57\n",
      "Next Action: [-0.725\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.0512163045465\n",
      "Episode: 32, Step: 58\n",
      "Next Action: [-0.447\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.0512163045465\n",
      "Episode: 32, Step: 59\n",
      "Next Action: [-0.568\n",
      "Step reward: -15.999713650579208, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.0509299551258\n",
      "Episode: 32, Step: 60\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.997759024479064, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.0486889796048\n",
      "Episode: 32, Step: 61\n",
      "Next Action: [-0.854\n",
      "Step reward: -15.970315053243581, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.0190040328483\n",
      "Episode: 32, Step: 62\n",
      "Next Action: [-0.887\n",
      "Step reward: -15.957002637107836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.9760066699562\n",
      "Episode: 32, Step: 63\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.948039834577155, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.9240465045333\n",
      "Episode: 32, Step: 64\n",
      "Next Action: [-0.819\n",
      "Step reward: -15.943402793166742, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.8674492977\n",
      "Episode: 32, Step: 65\n",
      "Next Action: [-1.125\n",
      "Step reward: -15.92045665275536, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.7879059504553\n",
      "Episode: 32, Step: 66\n",
      "Next Action: [-1.083\n",
      "Step reward: -15.913189763333794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.701095713789\n",
      "Episode: 32, Step: 67\n",
      "Next Action: [-0.954\n",
      "Step reward: -15.908604671605602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.6097003853947\n",
      "Episode: 32, Step: 68\n",
      "Next Action: [-1.065\n",
      "Step reward: -15.918198389766433, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.5278987751612\n",
      "Episode: 32, Step: 69\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.946712989340105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.4746117645013\n",
      "Episode: 32, Step: 70\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.955201032169402, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.4298127966708\n",
      "Episode: 32, Step: 71\n",
      "Next Action: [-0.874\n",
      "Step reward: -15.95750048505994, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.3873132817307\n",
      "Episode: 32, Step: 72\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.953227163266227, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.340540444997\n",
      "Episode: 32, Step: 73\n",
      "Next Action: [-1.334\n",
      "Step reward: -15.954101493596598, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.2946419385937\n",
      "Episode: 32, Step: 74\n",
      "Next Action: [-1.386\n",
      "Step reward: -15.94401219266872, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.2386541312624\n",
      "Episode: 32, Step: 75\n",
      "Next Action: [-1.478\n",
      "Step reward: -15.934875960800547, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.173530092063\n",
      "Episode: 32, Step: 76\n",
      "Next Action: [-1.480\n",
      "Step reward: -15.92419232996016, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.097722422023\n",
      "Episode: 32, Step: 77\n",
      "Next Action: [-1.440\n",
      "Step reward: -15.915327156070141, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.0130495780932\n",
      "Episode: 32, Step: 78\n",
      "Next Action: [-1.583\n",
      "Step reward: -15.935341441050525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.9483910191436\n",
      "Episode: 32, Step: 79\n",
      "Next Action: [-1.283\n",
      "Step reward: -15.934571832349967, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.8829628514936\n",
      "Episode: 32, Step: 80\n",
      "Next Action: [-1.542\n",
      "Step reward: -15.945919029386694, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.8288818808803\n",
      "Episode: 32, Step: 81\n",
      "Next Action: [-1.432\n",
      "Step reward: -15.959060467780805, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.787942348661\n",
      "Episode: 32, Step: 82\n",
      "Next Action: [-1.512\n",
      "Step reward: -15.966932080693411, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.7548744293545\n",
      "Episode: 32, Step: 83\n",
      "Next Action: [-1.295\n",
      "Step reward: -15.968366090276366, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.7232405196307\n",
      "Episode: 32, Step: 84\n",
      "Next Action: [-1.458\n",
      "Step reward: -15.940215866315231, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.663456385946\n",
      "Episode: 32, Step: 85\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.917498875087912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.5809552610338\n",
      "Episode: 32, Step: 86\n",
      "Next Action: [-1.121\n",
      "Step reward: -15.914034975378037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.4949902364117\n",
      "Episode: 32, Step: 87\n",
      "Next Action: [-1.147\n",
      "Step reward: -15.942317325644728, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.4373075620565\n",
      "Episode: 32, Step: 88\n",
      "Next Action: [-1.184\n",
      "Step reward: -15.962330398895103, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.3996379609516\n",
      "Episode: 32, Step: 89\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.958772726012654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.3584106869644\n",
      "Episode: 32, Step: 90\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.947573709145606, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.30598439611\n",
      "Episode: 32, Step: 91\n",
      "Next Action: [-0.799\n",
      "Step reward: -15.964832544517051, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.2708169406271\n",
      "Episode: 32, Step: 92\n",
      "Next Action: [-0.887\n",
      "Step reward: -15.972500609078615, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.2433175497058\n",
      "Episode: 32, Step: 93\n",
      "Next Action: [-0.587\n",
      "Step reward: -15.973240403875103, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.2165579535808\n",
      "Episode: 32, Step: 94\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.944775492183059, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.1613334457638\n",
      "Episode: 32, Step: 95\n",
      "Next Action: [-0.828\n",
      "Step reward: -15.945314881349395, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.1066483271131\n",
      "Episode: 32, Step: 96\n",
      "Next Action: [-0.935\n",
      "Step reward: -15.947824109337985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.054472436451\n",
      "Episode: 32, Step: 97\n",
      "Next Action: [-0.566\n",
      "Step reward: -15.941328390486426, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.9958008269375\n",
      "Episode: 32, Step: 98\n",
      "Next Action: [-0.633\n",
      "Step reward: -15.941384615684393, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1574.937185442622\n",
      "Episode: 32, Step: 99\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.942507171185904, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1590.879692613808\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.0805575847625732\n",
      "Actor loss: 52.0376091003418\n",
      "Critic loss: 1.6238207817077637\n",
      "Actor loss: 56.06463623046875\n",
      "Critic loss: 1.9247907400131226\n",
      "Actor loss: 58.92350387573242\n",
      "Critic loss: 21.178783416748047\n",
      "Actor loss: 41.6544189453125\n",
      "Critic loss: 11.865869522094727\n",
      "Actor loss: 35.36445999145508\n",
      "Critic loss: 3.303081512451172\n",
      "Actor loss: 60.309715270996094\n",
      "Critic loss: 4.793944835662842\n",
      "Actor loss: 55.4216423034668\n",
      "Critic loss: 573.340576171875\n",
      "Actor loss: 53.993408203125\n",
      "Critic loss: 2.7349777221679688\n",
      "Actor loss: 42.32488250732422\n",
      "Critic loss: 2.2241766452789307\n",
      "Actor loss: 46.05181884765625\n",
      "Episode: 33\n",
      "Episode: 33, Step: 0\n",
      "Next Action: [-0.807\n",
      "Step reward: -12.05564511531404, Next State: [-1.0\n",
      "Total episode reward: -12.05564511531404\n",
      "Episode: 33, Step: 1\n",
      "Next Action: [-6.217\n",
      "Step reward: -14.794239549221519, Next State: [-1.\n",
      "Total episode reward: -26.84988466453556\n",
      "Episode: 33, Step: 2\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.54743325139547, Next State: [-1. \n",
      "Total episode reward: -42.397317915931026\n",
      "Episode: 33, Step: 3\n",
      "Next Action: [-0.785\n",
      "Step reward: -15.7880039955958, Next State: [-1.  \n",
      "Total episode reward: -58.18532191152683\n",
      "Episode: 33, Step: 4\n",
      "Next Action: [-0.860\n",
      "Step reward: -15.872208152698423, Next State: [-1.\n",
      "Total episode reward: -74.05753006422525\n",
      "Episode: 33, Step: 5\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.916058376663093, Next State: [-1.\n",
      "Total episode reward: -89.97358844088834\n",
      "Episode: 33, Step: 6\n",
      "Next Action: [-1.149\n",
      "Step reward: -15.934817455786002, Next State: [-1.\n",
      "Total episode reward: -105.90840589667434\n",
      "Episode: 33, Step: 7\n",
      "Next Action: [-1.107\n",
      "Step reward: -15.946210754404339, Next State: [-1.\n",
      "Total episode reward: -121.85461665107867\n",
      "Episode: 33, Step: 8\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.963510991350727, Next State: [-1.\n",
      "Total episode reward: -137.8181276424294\n",
      "Episode: 33, Step: 9\n",
      "Next Action: [-1.245\n",
      "Step reward: -15.933554624647478, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.7516822670769\n",
      "Episode: 33, Step: 10\n",
      "Next Action: [-1.330\n",
      "Step reward: -15.927495752352307, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.6791780194292\n",
      "Episode: 33, Step: 11\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.937847682472285, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.6170257019015\n",
      "Episode: 33, Step: 12\n",
      "Next Action: [-1.239\n",
      "Step reward: -15.949314743216153, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.56634044511765\n",
      "Episode: 33, Step: 13\n",
      "Next Action: [-1.310\n",
      "Step reward: -15.962462921897158, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.5288033670148\n",
      "Episode: 33, Step: 14\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.95642258419344, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.48522595120824\n",
      "Episode: 33, Step: 15\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.948165071258165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.4333910224664\n",
      "Episode: 33, Step: 16\n",
      "Next Action: [-1.346\n",
      "Step reward: -15.938394380209202, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.3717854026756\n",
      "Episode: 33, Step: 17\n",
      "Next Action: [-1.256\n",
      "Step reward: -15.940386325796984, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.31217172847255\n",
      "Episode: 33, Step: 18\n",
      "Next Action: [-1.246\n",
      "Step reward: -15.930145175056985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.2423169035295\n",
      "Episode: 33, Step: 19\n",
      "Next Action: [-1.015\n",
      "Step reward: -15.917160696695639, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.15947760022516\n",
      "Episode: 33, Step: 20\n",
      "Next Action: [-1.110\n",
      "Step reward: -15.91025238115799, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.06972998138315\n",
      "Episode: 33, Step: 21\n",
      "Next Action: [-1.264\n",
      "Step reward: -15.912923988537178, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.9826539699203\n",
      "Episode: 33, Step: 22\n",
      "Next Action: [-1.246\n",
      "Step reward: -15.947110829586435, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.92976479950676\n",
      "Episode: 33, Step: 23\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.961655131962642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.8914199314694\n",
      "Episode: 33, Step: 24\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.966539350035006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.8579592815044\n",
      "Episode: 33, Step: 25\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.970993283825118, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.82895256532953\n",
      "Episode: 33, Step: 26\n",
      "Next Action: [-1.380\n",
      "Step reward: -15.966489978829497, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.795442544159\n",
      "Episode: 33, Step: 27\n",
      "Next Action: [-1.283\n",
      "Step reward: -15.961516838570269, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.7569593827293\n",
      "Episode: 33, Step: 28\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.962171924101462, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.7191313068308\n",
      "Episode: 33, Step: 29\n",
      "Next Action: [-1.432\n",
      "Step reward: -15.973279499280537, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.6924108061113\n",
      "Episode: 33, Step: 30\n",
      "Next Action: [-1.461\n",
      "Step reward: -15.97217969618339, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.6645905022947\n",
      "Episode: 33, Step: 31\n",
      "Next Action: [-1.409\n",
      "Step reward: -15.978567286937649, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.64315778923236\n",
      "Episode: 33, Step: 32\n",
      "Next Action: [-1.582\n",
      "Step reward: -15.980763190303598, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.6239209795359\n",
      "Episode: 33, Step: 33\n",
      "Next Action: [-1.332\n",
      "Step reward: -15.994853827925182, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.6187748074611\n",
      "Episode: 33, Step: 34\n",
      "Next Action: [-1.086\n",
      "Step reward: -15.986922269374533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.6056970768357\n",
      "Episode: 33, Step: 35\n",
      "Next Action: [-0.791\n",
      "Step reward: -15.97620479308455, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.5819018699202\n",
      "Episode: 33, Step: 36\n",
      "Next Action: [-0.619\n",
      "Step reward: -15.960251845988147, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.5421537159084\n",
      "Episode: 33, Step: 37\n",
      "Next Action: [-8.393\n",
      "Step reward: -15.953279637674965, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.4954333535834\n",
      "Episode: 33, Step: 38\n",
      "Next Action: [-1.229\n",
      "Step reward: -15.930701703304237, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.4261350568877\n",
      "Episode: 33, Step: 39\n",
      "Next Action: [-0.946\n",
      "Step reward: -15.927703740010093, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.3538387968978\n",
      "Episode: 33, Step: 40\n",
      "Next Action: [-1.065\n",
      "Step reward: -15.908458323948006, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.2622971208458\n",
      "Episode: 33, Step: 41\n",
      "Next Action: [-1.024\n",
      "Step reward: -15.902939073989437, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.1652361948352\n",
      "Episode: 33, Step: 42\n",
      "Next Action: [-1.193\n",
      "Step reward: -15.9107057646106, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.0759419594458\n",
      "Episode: 33, Step: 43\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.919866096832767, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -695.9958080562785\n",
      "Episode: 33, Step: 44\n",
      "Next Action: [-0.787\n",
      "Step reward: -15.943081907944599, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -711.9388899642231\n",
      "Episode: 33, Step: 45\n",
      "Next Action: [-0.759\n",
      "Step reward: -15.976443836469977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -727.9153338006931\n",
      "Episode: 33, Step: 46\n",
      "Next Action: [-0.470\n",
      "Step reward: -15.963359933385455, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -743.8786937340786\n",
      "Episode: 33, Step: 47\n",
      "Next Action: [-0.446\n",
      "Step reward: -15.963704867478624, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -759.8423986015573\n",
      "Episode: 33, Step: 48\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.978854467742222, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -775.8212530692995\n",
      "Episode: 33, Step: 49\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.993389663802297, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -791.8146427331018\n",
      "Episode: 33, Step: 50\n",
      "Next Action: [-1.196\n",
      "Step reward: -15.985405535495735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -807.8000482685975\n",
      "Episode: 33, Step: 51\n",
      "Next Action: [-1.535\n",
      "Step reward: -15.975917094134386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -823.7759653627319\n",
      "Episode: 33, Step: 52\n",
      "Next Action: [-1.684\n",
      "Step reward: -15.969804185605858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -839.7457695483378\n",
      "Episode: 33, Step: 53\n",
      "Next Action: [-1.592\n",
      "Step reward: -15.959057424530076, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -855.7048269728679\n",
      "Episode: 33, Step: 54\n",
      "Next Action: [-1.311\n",
      "Step reward: -15.969226833558023, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -871.6740538064258\n",
      "Episode: 33, Step: 55\n",
      "Next Action: [-1.309\n",
      "Step reward: -15.980786637259389, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.6548404436852\n",
      "Episode: 33, Step: 56\n",
      "Next Action: [-1.215\n",
      "Step reward: -15.988589520313585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.6434299639988\n",
      "Episode: 33, Step: 57\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.994414262742806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.6378442267415\n",
      "Episode: 33, Step: 58\n",
      "Next Action: [-1.296\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.6378442267415\n",
      "Episode: 33, Step: 59\n",
      "Next Action: [-1.432\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.6378442267415\n",
      "Episode: 33, Step: 60\n",
      "Next Action: [-1.397\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.6378442267415\n",
      "Episode: 33, Step: 61\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.993983089792543, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.6318273165341\n",
      "Episode: 33, Step: 62\n",
      "Next Action: [-1.016\n",
      "Step reward: -15.969112060522479, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.6009393770565\n",
      "Episode: 33, Step: 63\n",
      "Next Action: [-1.008\n",
      "Step reward: -15.967732740551728, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.5686721176082\n",
      "Episode: 33, Step: 64\n",
      "Next Action: [-0.852\n",
      "Step reward: -15.97220517369372, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.540877291302\n",
      "Episode: 33, Step: 65\n",
      "Next Action: [-0.789\n",
      "Step reward: -15.98265552786156, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.5235328191636\n",
      "Episode: 33, Step: 66\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.983530096222617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.5070629153863\n",
      "Episode: 33, Step: 67\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.98447534968067, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.491538265067\n",
      "Episode: 33, Step: 68\n",
      "Next Action: [-1.173\n",
      "Step reward: -15.964031231727622, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.4555694967946\n",
      "Episode: 33, Step: 69\n",
      "Next Action: [-1.023\n",
      "Step reward: -15.956979883015375, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.41254937981\n",
      "Episode: 33, Step: 70\n",
      "Next Action: [-1.078\n",
      "Step reward: -15.972485693656411, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.3850350734663\n",
      "Episode: 33, Step: 71\n",
      "Next Action: [-1.435\n",
      "Step reward: -15.988625091905265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1143.3736601653716\n",
      "Episode: 33, Step: 72\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.990272841005268, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.3639330063768\n",
      "Episode: 33, Step: 73\n",
      "Next Action: [-8.795\n",
      "Step reward: -15.98412265120451, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.3480556575812\n",
      "Episode: 33, Step: 74\n",
      "Next Action: [-0.925\n",
      "Step reward: -15.981124675969568, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.3291803335508\n",
      "Episode: 33, Step: 75\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.961625050293762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.2908053838446\n",
      "Episode: 33, Step: 76\n",
      "Next Action: [-0.937\n",
      "Step reward: -15.953844975473942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.2446503593185\n",
      "Episode: 33, Step: 77\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.959757164923579, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.204407524242\n",
      "Episode: 33, Step: 78\n",
      "Next Action: [-0.702\n",
      "Step reward: -15.946249108806741, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.1506566330488\n",
      "Episode: 33, Step: 79\n",
      "Next Action: [-0.595\n",
      "Step reward: -15.944782220573291, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.095438853622\n",
      "Episode: 33, Step: 80\n",
      "Next Action: [-0.872\n",
      "Step reward: -15.95073611688404, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.046174970506\n",
      "Episode: 33, Step: 81\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.96951228338414, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.01568725389\n",
      "Episode: 33, Step: 82\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.970077026669532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.9857642805596\n",
      "Episode: 33, Step: 83\n",
      "Next Action: [-0.889\n",
      "Step reward: -15.989461455184427, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.975225735744\n",
      "Episode: 33, Step: 84\n",
      "Next Action: [-0.690\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.975225735744\n",
      "Episode: 33, Step: 85\n",
      "Next Action: [-0.520\n",
      "Step reward: -15.987161035675834, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.96238677142\n",
      "Episode: 33, Step: 86\n",
      "Next Action: [-0.608\n",
      "Step reward: -15.986909258270787, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.9492960296907\n",
      "Episode: 33, Step: 87\n",
      "Next Action: [-0.655\n",
      "Step reward: -15.985678331655565, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.9349743613463\n",
      "Episode: 33, Step: 88\n",
      "Next Action: [-0.751\n",
      "Step reward: -15.970894508537018, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.9058688698833\n",
      "Episode: 33, Step: 89\n",
      "Next Action: [-0.646\n",
      "Step reward: -15.969326791978506, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.8751956618619\n",
      "Episode: 33, Step: 90\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.981858744960308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.8570544068223\n",
      "Episode: 33, Step: 91\n",
      "Next Action: [-0.417\n",
      "Step reward: -15.980773152287858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.83782755911\n",
      "Episode: 33, Step: 92\n",
      "Next Action: [-0.312\n",
      "Step reward: -15.969857996233635, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.8076855553438\n",
      "Episode: 33, Step: 93\n",
      "Next Action: [-0.219\n",
      "Step reward: -15.970238730064505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1494.7779242854083\n",
      "Episode: 33, Step: 94\n",
      "Next Action: [-0.490\n",
      "Step reward: -15.991858216921433, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1510.7697825023297\n",
      "Episode: 33, Step: 95\n",
      "Next Action: [-0.668\n",
      "Step reward: -15.996009242814116, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1526.7657917451438\n",
      "Episode: 33, Step: 96\n",
      "Next Action: [-1.079\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1542.7657917451438\n",
      "Episode: 33, Step: 97\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.998987572858463, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.7647793180022\n",
      "Episode: 33, Step: 98\n",
      "Next Action: [-0.688\n",
      "Step reward: -15.97446813407878, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1574.739247452081\n",
      "Episode: 33, Step: 99\n",
      "Next Action: [-0.521\n",
      "Step reward: -15.97334014874727, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1590.712587600828\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.1010165214538574\n",
      "Actor loss: 29.428909301757812\n",
      "Critic loss: 8.587254524230957\n",
      "Actor loss: 35.89318084716797\n",
      "Critic loss: 23.49263572692871\n",
      "Actor loss: 34.417327880859375\n",
      "Critic loss: 12.08498764038086\n",
      "Actor loss: 22.814767837524414\n",
      "Critic loss: 4.200475215911865\n",
      "Actor loss: 26.167037963867188\n",
      "Critic loss: 17.411252975463867\n",
      "Actor loss: 34.709678649902344\n",
      "Critic loss: 40.75544738769531\n",
      "Actor loss: 28.54989242553711\n",
      "Critic loss: 24.65320587158203\n",
      "Actor loss: 38.8774299621582\n",
      "Critic loss: 54.372535705566406\n",
      "Actor loss: 32.71520233154297\n",
      "Critic loss: 10.583428382873535\n",
      "Actor loss: 29.372806549072266\n",
      "Episode: 34\n",
      "Episode: 34, Step: 0\n",
      "Next Action: [-0.553\n",
      "Step reward: -11.767542345960534, Next State: [-0.\n",
      "Total episode reward: -11.767542345960534\n",
      "Episode: 34, Step: 1\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.074582762095575, Next State: [-0.\n",
      "Total episode reward: -26.84212510805611\n",
      "Episode: 34, Step: 2\n",
      "Next Action: [-0.419\n",
      "Step reward: -15.651930537035776, Next State: [-1.\n",
      "Total episode reward: -42.49405564509188\n",
      "Episode: 34, Step: 3\n",
      "Next Action: [-0.453\n",
      "Step reward: -15.820123661645738, Next State: [-1.\n",
      "Total episode reward: -58.31417930673762\n",
      "Episode: 34, Step: 4\n",
      "Next Action: [-0.584\n",
      "Step reward: -15.9124761056936, Next State: [-1.  \n",
      "Total episode reward: -74.22665541243123\n",
      "Episode: 34, Step: 5\n",
      "Next Action: [-0.371\n",
      "Step reward: -15.942665131522675, Next State: [-1.\n",
      "Total episode reward: -90.1693205439539\n",
      "Episode: 34, Step: 6\n",
      "Next Action: [-0.646\n",
      "Step reward: -15.938735426566296, Next State: [-1.\n",
      "Total episode reward: -106.1080559705202\n",
      "Episode: 34, Step: 7\n",
      "Next Action: [-0.822\n",
      "Step reward: -15.943827945415483, Next State: [-1.\n",
      "Total episode reward: -122.05188391593568\n",
      "Episode: 34, Step: 8\n",
      "Next Action: [-0.634\n",
      "Step reward: -15.919313099505557, Next State: [-1.\n",
      "Total episode reward: -137.97119701544122\n",
      "Episode: 34, Step: 9\n",
      "Next Action: [-0.392\n",
      "Step reward: -15.915623488683133, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.88682050412436\n",
      "Episode: 34, Step: 10\n",
      "Next Action: [-0.578\n",
      "Step reward: -15.914470954290168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.80129145841454\n",
      "Episode: 34, Step: 11\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.938161369054159, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.7394528274687\n",
      "Episode: 34, Step: 12\n",
      "Next Action: [-7.009\n",
      "Step reward: -15.973216493646255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.71266932111496\n",
      "Episode: 34, Step: 13\n",
      "Next Action: [-0.788\n",
      "Step reward: -15.992979811613756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.70564913272872\n",
      "Episode: 34, Step: 14\n",
      "Next Action: [-0.804\n",
      "Step reward: -15.991328013034048, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.69697714576276\n",
      "Episode: 34, Step: 15\n",
      "Next Action: [-0.809\n",
      "Step reward: -15.98951673315901, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.68649387892177\n",
      "Episode: 34, Step: 16\n",
      "Next Action: [-0.853\n",
      "Step reward: -15.966649758306872, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.65314363722865\n",
      "Episode: 34, Step: 17\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.962157203535021, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.61530084076367\n",
      "Episode: 34, Step: 18\n",
      "Next Action: [-0.921\n",
      "Step reward: -15.960155289781188, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.57545613054486\n",
      "Episode: 34, Step: 19\n",
      "Next Action: [-1.297\n",
      "Step reward: -15.949300290716595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.5247564212615\n",
      "Episode: 34, Step: 20\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.970191555825016, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.4949479770865\n",
      "Episode: 34, Step: 21\n",
      "Next Action: [-1.209\n",
      "Step reward: -15.960975584678474, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.45592356176496\n",
      "Episode: 34, Step: 22\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.948451151296661, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.4043747130616\n",
      "Episode: 34, Step: 23\n",
      "Next Action: [-0.891\n",
      "Step reward: -15.946870461311972, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.35124517437356\n",
      "Episode: 34, Step: 24\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.952169432780781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.30341460715437\n",
      "Episode: 34, Step: 25\n",
      "Next Action: [-0.801\n",
      "Step reward: -15.951413486023759, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.2548280931781\n",
      "Episode: 34, Step: 26\n",
      "Next Action: [-0.434\n",
      "Step reward: -15.955202118871807, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.2100302120499\n",
      "Episode: 34, Step: 27\n",
      "Next Action: [-4.879\n",
      "Step reward: -15.968932908908648, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.17896312095854\n",
      "Episode: 34, Step: 28\n",
      "Next Action: [-0.379\n",
      "Step reward: -15.967903674287703, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.14686679524624\n",
      "Episode: 34, Step: 29\n",
      "Next Action: [-0.327\n",
      "Step reward: -15.95507675460461, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.10194354985083\n",
      "Episode: 34, Step: 30\n",
      "Next Action: [-0.391\n",
      "Step reward: -15.9573562110647, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.0592997609155\n",
      "Episode: 34, Step: 31\n",
      "Next Action: [-0.520\n",
      "Step reward: -15.957652371557531, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.016952132473\n",
      "Episode: 34, Step: 32\n",
      "Next Action: [-0.916\n",
      "Step reward: -15.956161538195094, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.9731136706681\n",
      "Episode: 34, Step: 33\n",
      "Next Action: [-8.163\n",
      "Step reward: -15.960020606996451, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.9331342776645\n",
      "Episode: 34, Step: 34\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.994108883412519, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.9272431610771\n",
      "Episode: 34, Step: 35\n",
      "Next Action: [-0.836\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.9272431610771\n",
      "Episode: 34, Step: 36\n",
      "Next Action: [-1.249\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.9272431610771\n",
      "Episode: 34, Step: 37\n",
      "Next Action: [-1.248\n",
      "Step reward: -15.99757756186923, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.9248207229463\n",
      "Episode: 34, Step: 38\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.979072074183076, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.9038927971293\n",
      "Episode: 34, Step: 39\n",
      "Next Action: [-0.964\n",
      "Step reward: -15.96917343797319, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.8730662351024\n",
      "Episode: 34, Step: 40\n",
      "Next Action: [-0.888\n",
      "Step reward: -15.968719767938854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.8417860030413\n",
      "Episode: 34, Step: 41\n",
      "Next Action: [-1.162\n",
      "Step reward: -15.969935902021914, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.8117219050632\n",
      "Episode: 34, Step: 42\n",
      "Next Action: [-1.081\n",
      "Step reward: -15.988972107558286, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.8006940126215\n",
      "Episode: 34, Step: 43\n",
      "Next Action: [-1.197\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.8006940126215\n",
      "Episode: 34, Step: 44\n",
      "Next Action: [-1.426\n",
      "Step reward: -15.983909974137385, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.784603986759\n",
      "Episode: 34, Step: 45\n",
      "Next Action: [-1.554\n",
      "Step reward: -15.98671007900003, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.7713140657589\n",
      "Episode: 34, Step: 46\n",
      "Next Action: [-1.461\n",
      "Step reward: -15.97243564153917, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.7437497072981\n",
      "Episode: 34, Step: 47\n",
      "Next Action: [-1.586\n",
      "Step reward: -15.954821237595798, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.6985709448938\n",
      "Episode: 34, Step: 48\n",
      "Next Action: [-1.411\n",
      "Step reward: -15.938924385315424, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.6374953302093\n",
      "Episode: 34, Step: 49\n",
      "Next Action: [-1.405\n",
      "Step reward: -15.941500634888188, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.5789959650974\n",
      "Episode: 34, Step: 50\n",
      "Next Action: [-1.629\n",
      "Step reward: -15.954375440271136, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.5333714053686\n",
      "Episode: 34, Step: 51\n",
      "Next Action: [-1.499\n",
      "Step reward: -15.940834222706968, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.4742056280755\n",
      "Episode: 34, Step: 52\n",
      "Next Action: [-1.405\n",
      "Step reward: -15.966359332196864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.4405649602724\n",
      "Episode: 34, Step: 53\n",
      "Next Action: [-1.513\n",
      "Step reward: -15.985846397039758, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.4264113573121\n",
      "Episode: 34, Step: 54\n",
      "Next Action: [-1.500\n",
      "Step reward: -15.994693273869274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.4211046311814\n",
      "Episode: 34, Step: 55\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.983785083820312, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.4048897150017\n",
      "Episode: 34, Step: 56\n",
      "Next Action: [-1.527\n",
      "Step reward: -15.969820837346115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.3747105523478\n",
      "Episode: 34, Step: 57\n",
      "Next Action: [-0.995\n",
      "Step reward: -15.959874204105889, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.3345847564536\n",
      "Episode: 34, Step: 58\n",
      "Next Action: [-0.985\n",
      "Step reward: -15.982805171949352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.317389928403\n",
      "Episode: 34, Step: 59\n",
      "Next Action: [-0.981\n",
      "Step reward: -15.993996844264343, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.3113867726673\n",
      "Episode: 34, Step: 60\n",
      "Next Action: [-0.896\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.3113867726673\n",
      "Episode: 34, Step: 61\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.987473736661697, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.2988605093291\n",
      "Episode: 34, Step: 62\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.994431705836961, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.293292215166\n",
      "Episode: 34, Step: 63\n",
      "Next Action: [-0.544\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.293292215166\n",
      "Episode: 34, Step: 64\n",
      "Next Action: [-0.522\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.293292215166\n",
      "Episode: 34, Step: 65\n",
      "Next Action: [-0.609\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.293292215166\n",
      "Episode: 34, Step: 66\n",
      "Next Action: [-0.656\n",
      "Step reward: -15.963303319048618, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.2565955342147\n",
      "Episode: 34, Step: 67\n",
      "Next Action: [-0.678\n",
      "Step reward: -15.959988580356994, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.2165841145718\n",
      "Episode: 34, Step: 68\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.958693133615963, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.1752772481877\n",
      "Episode: 34, Step: 69\n",
      "Next Action: [-1.145\n",
      "Step reward: -15.967141039143979, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.1424182873318\n",
      "Episode: 34, Step: 70\n",
      "Next Action: [-1.266\n",
      "Step reward: -15.951550941253906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.0939692285856\n",
      "Episode: 34, Step: 71\n",
      "Next Action: [-0.802\n",
      "Step reward: -15.94718358474707, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.0411528133327\n",
      "Episode: 34, Step: 72\n",
      "Next Action: [-0.705\n",
      "Step reward: -15.940775340100556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.9819281534333\n",
      "Episode: 34, Step: 73\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.925266309036342, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.9071944624695\n",
      "Episode: 34, Step: 74\n",
      "Next Action: [-0.847\n",
      "Step reward: -15.914134764697256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.8213292271669\n",
      "Episode: 34, Step: 75\n",
      "Next Action: [-0.818\n",
      "Step reward: -15.908027024255968, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.7293562514228\n",
      "Episode: 34, Step: 76\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.929898488712505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.6592547401353\n",
      "Episode: 34, Step: 77\n",
      "Next Action: [-0.838\n",
      "Step reward: -15.941058530955884, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.6003132710912\n",
      "Episode: 34, Step: 78\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.955488449164887, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.555801720256\n",
      "Episode: 34, Step: 79\n",
      "Next Action: [-0.664\n",
      "Step reward: -15.968909194734605, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.5247109149907\n",
      "Episode: 34, Step: 80\n",
      "Next Action: [-0.428\n",
      "Step reward: -15.974962460068475, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.4996733750593\n",
      "Episode: 34, Step: 81\n",
      "Next Action: [-0.371\n",
      "Step reward: -15.970344897876469, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.4700182729357\n",
      "Episode: 34, Step: 82\n",
      "Next Action: [-0.238\n",
      "Step reward: -15.949219240103224, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.419237513039\n",
      "Episode: 34, Step: 83\n",
      "Next Action: [-0.121\n",
      "Step reward: -15.923321085495916, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.3425585985349\n",
      "Episode: 34, Step: 84\n",
      "Next Action: [-0.551\n",
      "Step reward: -15.916071256933316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.2586298554681\n",
      "Episode: 34, Step: 85\n",
      "Next Action: [-0.638\n",
      "Step reward: -15.925183095206018, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.183812950674\n",
      "Episode: 34, Step: 86\n",
      "Next Action: [-0.776\n",
      "Step reward: -15.956412105037558, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.1402250557117\n",
      "Episode: 34, Step: 87\n",
      "Next Action: [-0.945\n",
      "Step reward: -15.95506476317566, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.0952898188873\n",
      "Episode: 34, Step: 88\n",
      "Next Action: [-1.147\n",
      "Step reward: -15.944900091334713, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.040189910222\n",
      "Episode: 34, Step: 89\n",
      "Next Action: [-1.020\n",
      "Step reward: -15.972547433291618, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.0127373435134\n",
      "Episode: 34, Step: 90\n",
      "Next Action: [-1.047\n",
      "Step reward: -15.985198475212297, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.9979358187256\n",
      "Episode: 34, Step: 91\n",
      "Next Action: [-0.954\n",
      "Step reward: -15.983413077226897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.9813488959526\n",
      "Episode: 34, Step: 92\n",
      "Next Action: [-1.189\n",
      "Step reward: -15.971626649896784, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.9529755458493\n",
      "Episode: 34, Step: 93\n",
      "Next Action: [-1.244\n",
      "Step reward: -15.983736325111888, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1494.9367118709613\n",
      "Episode: 34, Step: 94\n",
      "Next Action: [-1.179\n",
      "Step reward: -15.987264196425949, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1510.9239760673872\n",
      "Episode: 34, Step: 95\n",
      "Next Action: [-1.416\n",
      "Step reward: -15.984924696940404, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1526.9089007643277\n",
      "Episode: 34, Step: 96\n",
      "Next Action: [-1.218\n",
      "Step reward: -15.987363989611936, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1542.8962647539397\n",
      "Episode: 34, Step: 97\n",
      "Next Action: [-1.265\n",
      "Step reward: -15.982418755056377, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.8786835089961\n",
      "Episode: 34, Step: 98\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.97971392860636, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1574.8583974376024\n",
      "Episode: 34, Step: 99\n",
      "Next Action: [-1.318\n",
      "Step reward: -15.969900344087524, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1590.82829778169\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 28.799821853637695\n",
      "Actor loss: 32.4593391418457\n",
      "Critic loss: 28.519392013549805\n",
      "Actor loss: 35.132423400878906\n",
      "Critic loss: 1.4802498817443848\n",
      "Actor loss: 57.542198181152344\n",
      "Critic loss: 2.9826321601867676\n",
      "Actor loss: 59.43000030517578\n",
      "Critic loss: 23.461122512817383\n",
      "Actor loss: 66.07933044433594\n",
      "Critic loss: 7.824787139892578\n",
      "Actor loss: 59.236244201660156\n",
      "Critic loss: 44.518577575683594\n",
      "Actor loss: 37.57095718383789\n",
      "Critic loss: 16.95932388305664\n",
      "Actor loss: 61.74163818359375\n",
      "Critic loss: 52.15214157104492\n",
      "Actor loss: 33.27717971801758\n",
      "Critic loss: 10.10892105102539\n",
      "Actor loss: 38.74754333496094\n",
      "Episode: 35\n",
      "Episode: 35, Step: 0\n",
      "Next Action: [-1.171\n",
      "Step reward: -11.912062787961485, Next State: [-1.\n",
      "Total episode reward: -11.912062787961485\n",
      "Episode: 35, Step: 1\n",
      "Next Action: [-1.314\n",
      "Step reward: -14.87857824887008, Next State: [-1. \n",
      "Total episode reward: -26.790641036831566\n",
      "Episode: 35, Step: 2\n",
      "Next Action: [-1.518\n",
      "Step reward: -15.592399754362061, Next State: [-1.\n",
      "Total episode reward: -42.38304079119363\n",
      "Episode: 35, Step: 3\n",
      "Next Action: [-1.493\n",
      "Step reward: -15.793988829263641, Next State: [-1.\n",
      "Total episode reward: -58.17702962045727\n",
      "Episode: 35, Step: 4\n",
      "Next Action: [-1.015\n",
      "Step reward: -15.859158758823241, Next State: [-1.\n",
      "Total episode reward: -74.0361883792805\n",
      "Episode: 35, Step: 5\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.928259816009469, Next State: [-1.\n",
      "Total episode reward: -89.96444819528998\n",
      "Episode: 35, Step: 6\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.957721738537833, Next State: [-1.\n",
      "Total episode reward: -105.9221699338278\n",
      "Episode: 35, Step: 7\n",
      "Next Action: [-0.777\n",
      "Step reward: -15.956773515124986, Next State: [-1.\n",
      "Total episode reward: -121.87894344895278\n",
      "Episode: 35, Step: 8\n",
      "Next Action: [-6.003\n",
      "Step reward: -15.949112041544582, Next State: [-1.\n",
      "Total episode reward: -137.82805549049738\n",
      "Episode: 35, Step: 9\n",
      "Next Action: [-1.025\n",
      "Step reward: -15.964598023611126, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.7926535141085\n",
      "Episode: 35, Step: 10\n",
      "Next Action: [-0.950\n",
      "Step reward: -15.97828955221136, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.77094306631986\n",
      "Episode: 35, Step: 11\n",
      "Next Action: [-0.981\n",
      "Step reward: -15.976806334866742, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.7477494011866\n",
      "Episode: 35, Step: 12\n",
      "Next Action: [-0.473\n",
      "Step reward: -15.9779368222463, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.72568622343292\n",
      "Episode: 35, Step: 13\n",
      "Next Action: [-0.660\n",
      "Step reward: -15.994077213620134, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.71976343705305\n",
      "Episode: 35, Step: 14\n",
      "Next Action: [-0.889\n",
      "Step reward: -15.998538694332574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.71830213138563\n",
      "Episode: 35, Step: 15\n",
      "Next Action: [-0.971\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.71830213138563\n",
      "Episode: 35, Step: 16\n",
      "Next Action: [-0.943\n",
      "Step reward: -15.996730822736208, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.71503295412185\n",
      "Episode: 35, Step: 17\n",
      "Next Action: [-1.036\n",
      "Step reward: -15.9872815598966, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.70231451401844\n",
      "Episode: 35, Step: 18\n",
      "Next Action: [-9.974\n",
      "Step reward: -15.973371096871787, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.67568561089024\n",
      "Episode: 35, Step: 19\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.987516039075592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.66320164996586\n",
      "Episode: 35, Step: 20\n",
      "Next Action: [-1.328\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.66320164996586\n",
      "Episode: 35, Step: 21\n",
      "Next Action: [-1.498\n",
      "Step reward: -15.998667692001673, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.66186934196753\n",
      "Episode: 35, Step: 22\n",
      "Next Action: [-1.776\n",
      "Step reward: -15.9861767601429, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.6480461021104\n",
      "Episode: 35, Step: 23\n",
      "Next Action: [-1.850\n",
      "Step reward: -15.987888081931995, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.6359341840424\n",
      "Episode: 35, Step: 24\n",
      "Next Action: [-1.872\n",
      "Step reward: -15.996516510237525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.63245069427995\n",
      "Episode: 35, Step: 25\n",
      "Next Action: [-1.645\n",
      "Step reward: -15.996422826226183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.62887352050615\n",
      "Episode: 35, Step: 26\n",
      "Next Action: [-1.732\n",
      "Step reward: -15.992416783573645, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.6212903040798\n",
      "Episode: 35, Step: 27\n",
      "Next Action: [-1.650\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.6212903040798\n",
      "Episode: 35, Step: 28\n",
      "Next Action: [-1.466\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.6212903040798\n",
      "Episode: 35, Step: 29\n",
      "Next Action: [-1.539\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.6212903040798\n",
      "Episode: 35, Step: 30\n",
      "Next Action: [-0.840\n",
      "Step reward: -15.978116233097905, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.59940653717774\n",
      "Episode: 35, Step: 31\n",
      "Next Action: [-0.868\n",
      "Step reward: -15.981160692317397, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.58056722949516\n",
      "Episode: 35, Step: 32\n",
      "Next Action: [-0.926\n",
      "Step reward: -15.973735652392293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.5543028818875\n",
      "Episode: 35, Step: 33\n",
      "Next Action: [-0.759\n",
      "Step reward: -15.982104925030168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.5364078069176\n",
      "Episode: 35, Step: 34\n",
      "Next Action: [-0.679\n",
      "Step reward: -15.9831841192385, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.5195919261561\n",
      "Episode: 35, Step: 35\n",
      "Next Action: [-0.590\n",
      "Step reward: -15.977489774426655, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.4970817005827\n",
      "Episode: 35, Step: 36\n",
      "Next Action: [-0.499\n",
      "Step reward: -15.96711433610046, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.4641960366832\n",
      "Episode: 35, Step: 37\n",
      "Next Action: [-0.898\n",
      "Step reward: -15.961402879590842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.4255989162741\n",
      "Episode: 35, Step: 38\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.975216147660621, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.4008150639347\n",
      "Episode: 35, Step: 39\n",
      "Next Action: [-0.650\n",
      "Step reward: -15.98493651040213, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.3857515743368\n",
      "Episode: 35, Step: 40\n",
      "Next Action: [-0.497\n",
      "Step reward: -15.972372255413052, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.3581238297498\n",
      "Episode: 35, Step: 41\n",
      "Next Action: [-0.618\n",
      "Step reward: -15.970691474098585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.3288153038484\n",
      "Episode: 35, Step: 42\n",
      "Next Action: [-1.016\n",
      "Step reward: -15.964734498265083, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.2935498021135\n",
      "Episode: 35, Step: 43\n",
      "Next Action: [-1.110\n",
      "Step reward: -15.970590321582376, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.2641401236958\n",
      "Episode: 35, Step: 44\n",
      "Next Action: [-1.010\n",
      "Step reward: -15.979032504670494, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.2431726283663\n",
      "Episode: 35, Step: 45\n",
      "Next Action: [-0.871\n",
      "Step reward: -15.993058659626227, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.2362312879925\n",
      "Episode: 35, Step: 46\n",
      "Next Action: [-0.784\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.2362312879925\n",
      "Episode: 35, Step: 47\n",
      "Next Action: [-0.724\n",
      "Step reward: -15.999778392834912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.2360096808275\n",
      "Episode: 35, Step: 48\n",
      "Next Action: [-0.479\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.2360096808275\n",
      "Episode: 35, Step: 49\n",
      "Next Action: [-0.771\n",
      "Step reward: -15.998422354397244, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.2344320352247\n",
      "Episode: 35, Step: 50\n",
      "Next Action: [-0.540\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.2344320352247\n",
      "Episode: 35, Step: 51\n",
      "Next Action: [-0.740\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.2344320352247\n",
      "Episode: 35, Step: 52\n",
      "Next Action: [-0.621\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.2344320352247\n",
      "Episode: 35, Step: 53\n",
      "Next Action: [-0.687\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.2344320352247\n",
      "Episode: 35, Step: 54\n",
      "Next Action: [-1.165\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.2344320352247\n",
      "Episode: 35, Step: 55\n",
      "Next Action: [-1.185\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.2344320352247\n",
      "Episode: 35, Step: 56\n",
      "Next Action: [-1.138\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.2344320352247\n",
      "Episode: 35, Step: 57\n",
      "Next Action: [-1.166\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.2344320352247\n",
      "Episode: 35, Step: 58\n",
      "Next Action: [-1.167\n",
      "Step reward: -15.998710912184551, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.2331429474093\n",
      "Episode: 35, Step: 59\n",
      "Next Action: [-1.445\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.2331429474093\n",
      "Episode: 35, Step: 60\n",
      "Next Action: [-1.611\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.2331429474093\n",
      "Episode: 35, Step: 61\n",
      "Next Action: [-1.636\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.2331429474093\n",
      "Episode: 35, Step: 62\n",
      "Next Action: [-1.597\n",
      "Step reward: -15.997135364397739, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.2302783118071\n",
      "Episode: 35, Step: 63\n",
      "Next Action: [-1.653\n",
      "Step reward: -15.973138039302386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.2034163511095\n",
      "Episode: 35, Step: 64\n",
      "Next Action: [-1.600\n",
      "Step reward: -15.96487052570512, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.1682868768146\n",
      "Episode: 35, Step: 65\n",
      "Next Action: [-1.577\n",
      "Step reward: -15.981389774605466, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.1496766514201\n",
      "Episode: 35, Step: 66\n",
      "Next Action: [-1.458\n",
      "Step reward: -15.959610166101651, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.1092868175217\n",
      "Episode: 35, Step: 67\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.95517986822492, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.0644666857465\n",
      "Episode: 35, Step: 68\n",
      "Next Action: [-0.917\n",
      "Step reward: -15.93954308342823, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.0040097691747\n",
      "Episode: 35, Step: 69\n",
      "Next Action: [-1.407\n",
      "Step reward: -15.937053104835465, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.9410628740102\n",
      "Episode: 35, Step: 70\n",
      "Next Action: [-1.371\n",
      "Step reward: -15.940470352474113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.8815332264844\n",
      "Episode: 35, Step: 71\n",
      "Next Action: [-1.231\n",
      "Step reward: -15.92503798696297, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.8065712134473\n",
      "Episode: 35, Step: 72\n",
      "Next Action: [-0.858\n",
      "Step reward: -15.949168268598624, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.755739482046\n",
      "Episode: 35, Step: 73\n",
      "Next Action: [-1.389\n",
      "Step reward: -15.994474471361759, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.7502139534079\n",
      "Episode: 35, Step: 74\n",
      "Next Action: [-1.486\n",
      "Step reward: -15.9686147683934, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.7188287218012\n",
      "Episode: 35, Step: 75\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.963348304607823, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.682177026409\n",
      "Episode: 35, Step: 76\n",
      "Next Action: [-1.537\n",
      "Step reward: -15.959173422817402, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.6413504492264\n",
      "Episode: 35, Step: 77\n",
      "Next Action: [-1.360\n",
      "Step reward: -15.950107349525549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.591457798752\n",
      "Episode: 35, Step: 78\n",
      "Next Action: [-1.557\n",
      "Step reward: -15.94942759676982, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.5408853955216\n",
      "Episode: 35, Step: 79\n",
      "Next Action: [-1.589\n",
      "Step reward: -15.964485273021836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.5053706685435\n",
      "Episode: 35, Step: 80\n",
      "Next Action: [-1.678\n",
      "Step reward: -15.96777889747085, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.4731495660144\n",
      "Episode: 35, Step: 81\n",
      "Next Action: [-1.948\n",
      "Step reward: -15.995234243980889, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.4683838099952\n",
      "Episode: 35, Step: 82\n",
      "Next Action: [-1.712\n",
      "Step reward: -15.989290203858776, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.4576740138539\n",
      "Episode: 35, Step: 83\n",
      "Next Action: [-1.581\n",
      "Step reward: -15.998745095782175, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.456419109636\n",
      "Episode: 35, Step: 84\n",
      "Next Action: [-1.761\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.456419109636\n",
      "Episode: 35, Step: 85\n",
      "Next Action: [-1.557\n",
      "Step reward: -15.988368172385927, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.444787282022\n",
      "Episode: 35, Step: 86\n",
      "Next Action: [-1.498\n",
      "Step reward: -15.993371968092546, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.4381592501145\n",
      "Episode: 35, Step: 87\n",
      "Next Action: [-1.337\n",
      "Step reward: -15.9537445535874, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1400.391903803702\n",
      "Episode: 35, Step: 88\n",
      "Next Action: [-1.344\n",
      "Step reward: -15.93722380030395, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.3291276040059\n",
      "Episode: 35, Step: 89\n",
      "Next Action: [-1.304\n",
      "Step reward: -15.933781421505953, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.2629090255118\n",
      "Episode: 35, Step: 90\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.927911110645486, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.1908201361573\n",
      "Episode: 35, Step: 91\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.93393596239261, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.1247560985498\n",
      "Episode: 35, Step: 92\n",
      "Next Action: [-1.057\n",
      "Step reward: -15.96090973952956, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.0856658380794\n",
      "Episode: 35, Step: 93\n",
      "Next Action: [-1.412\n",
      "Step reward: -15.982054167079044, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.0677200051584\n",
      "Episode: 35, Step: 94\n",
      "Next Action: [-1.251\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.0677200051584\n",
      "Episode: 35, Step: 95\n",
      "Next Action: [-1.435\n",
      "Step reward: -15.998573733085243, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.0662937382438\n",
      "Episode: 35, Step: 96\n",
      "Next Action: [-1.340\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.0662937382438\n",
      "Episode: 35, Step: 97\n",
      "Next Action: [-1.614\n",
      "Step reward: -15.99518305063594, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.0614767888796\n",
      "Episode: 35, Step: 98\n",
      "Next Action: [-1.589\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.0614767888796\n",
      "Episode: 35, Step: 99\n",
      "Next Action: [-1.736\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.0614767888796\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.8124985694885254\n",
      "Actor loss: 52.16337966918945\n",
      "Critic loss: 6.728196144104004\n",
      "Actor loss: 59.82532501220703\n",
      "Critic loss: 8.712626457214355\n",
      "Actor loss: 40.81816482543945\n",
      "Critic loss: 1.4719831943511963\n",
      "Actor loss: 52.2250862121582\n",
      "Critic loss: 2.056875467300415\n",
      "Actor loss: 46.519309997558594\n",
      "Critic loss: 3.69280743598938\n",
      "Actor loss: 43.32581329345703\n",
      "Critic loss: 4.361471176147461\n",
      "Actor loss: 36.936309814453125\n",
      "Critic loss: 7.72884464263916\n",
      "Actor loss: 40.2911376953125\n",
      "Critic loss: 5.878519535064697\n",
      "Actor loss: 32.92680358886719\n",
      "Critic loss: 3.321216106414795\n",
      "Actor loss: 56.7738151550293\n",
      "Episode: 36\n",
      "Episode: 36, Step: 0\n",
      "Next Action: [-1.495\n",
      "Step reward: -12.113351213767737, Next State: [-1.\n",
      "Total episode reward: -12.113351213767737\n",
      "Episode: 36, Step: 1\n",
      "Next Action: [-1.810\n",
      "Step reward: -15.146844250443847, Next State: [-1.\n",
      "Total episode reward: -27.260195464211584\n",
      "Episode: 36, Step: 2\n",
      "Next Action: [-1.695\n",
      "Step reward: -15.753144561980264, Next State: [-1.\n",
      "Total episode reward: -43.01334002619185\n",
      "Episode: 36, Step: 3\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.899339733097783, Next State: [-1.\n",
      "Total episode reward: -58.91267975928963\n",
      "Episode: 36, Step: 4\n",
      "Next Action: [-1.433\n",
      "Step reward: -15.930693365296403, Next State: [-1.\n",
      "Total episode reward: -74.84337312458604\n",
      "Episode: 36, Step: 5\n",
      "Next Action: [-1.474\n",
      "Step reward: -15.958198483549676, Next State: [-1.\n",
      "Total episode reward: -90.80157160813572\n",
      "Episode: 36, Step: 6\n",
      "Next Action: [-1.394\n",
      "Step reward: -15.976989481883434, Next State: [-1.\n",
      "Total episode reward: -106.77856109001915\n",
      "Episode: 36, Step: 7\n",
      "Next Action: [-1.151\n",
      "Step reward: -15.976409631011938, Next State: [-1.\n",
      "Total episode reward: -122.75497072103109\n",
      "Episode: 36, Step: 8\n",
      "Next Action: [-1.501\n",
      "Step reward: -15.988011791124247, Next State: [-1.\n",
      "Total episode reward: -138.74298251215532\n",
      "Episode: 36, Step: 9\n",
      "Next Action: [-1.790\n",
      "Step reward: -15.99282674102696, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.73580925318228\n",
      "Episode: 36, Step: 10\n",
      "Next Action: [-1.517\n",
      "Step reward: -15.984230500839347, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.72003975402163\n",
      "Episode: 36, Step: 11\n",
      "Next Action: [-1.436\n",
      "Step reward: -15.97350023538083, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.69353998940247\n",
      "Episode: 36, Step: 12\n",
      "Next Action: [-1.037\n",
      "Step reward: -15.974778385296595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.66831837469906\n",
      "Episode: 36, Step: 13\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.946202149207739, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.6145205239068\n",
      "Episode: 36, Step: 14\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.917816486155376, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.53233701006218\n",
      "Episode: 36, Step: 15\n",
      "Next Action: [-0.972\n",
      "Step reward: -15.921640468176573, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.45397747823876\n",
      "Episode: 36, Step: 16\n",
      "Next Action: [-0.877\n",
      "Step reward: -15.925035680523843, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.3790131587626\n",
      "Episode: 36, Step: 17\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.941547621632187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.32056078039477\n",
      "Episode: 36, Step: 18\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.94594529660405, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.26650607699884\n",
      "Episode: 36, Step: 19\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.942120890915763, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.2086269679146\n",
      "Episode: 36, Step: 20\n",
      "Next Action: [-1.252\n",
      "Step reward: -15.948765146027428, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.157392113942\n",
      "Episode: 36, Step: 21\n",
      "Next Action: [-1.128\n",
      "Step reward: -15.975565902936834, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.13295801687883\n",
      "Episode: 36, Step: 22\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.979007191248945, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.1119652081278\n",
      "Episode: 36, Step: 23\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.98934459601494, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.1013098041427\n",
      "Episode: 36, Step: 24\n",
      "Next Action: [-1.256\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.1013098041427\n",
      "Episode: 36, Step: 25\n",
      "Next Action: [-1.159\n",
      "Step reward: -15.993678882975486, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.0949886871182\n",
      "Episode: 36, Step: 26\n",
      "Next Action: [-1.062\n",
      "Step reward: -15.992452975338853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.087441662457\n",
      "Episode: 36, Step: 27\n",
      "Next Action: [-1.061\n",
      "Step reward: -15.993917872758352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.08135953521537\n",
      "Episode: 36, Step: 28\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.995091929830876, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.07645146504626\n",
      "Episode: 36, Step: 29\n",
      "Next Action: [-0.670\n",
      "Step reward: -15.984243806328658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.0606952713749\n",
      "Episode: 36, Step: 30\n",
      "Next Action: [-0.731\n",
      "Step reward: -15.986766085871862, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.0474613572468\n",
      "Episode: 36, Step: 31\n",
      "Next Action: [-0.755\n",
      "Step reward: -15.99236485970898, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.03982621695576\n",
      "Episode: 36, Step: 32\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.994391864901822, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.0342180818576\n",
      "Episode: 36, Step: 33\n",
      "Next Action: [-0.781\n",
      "Step reward: -15.985737810172642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.0199558920302\n",
      "Episode: 36, Step: 34\n",
      "Next Action: [-1.146\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.0199558920302\n",
      "Episode: 36, Step: 35\n",
      "Next Action: [-1.556\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.0199558920302\n",
      "Episode: 36, Step: 36\n",
      "Next Action: [-1.695\n",
      "Step reward: -15.99770120512887, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.0176570971591\n",
      "Episode: 36, Step: 37\n",
      "Next Action: [-1.841\n",
      "Step reward: -15.981788994409305, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.9994460915684\n",
      "Episode: 36, Step: 38\n",
      "Next Action: [-1.820\n",
      "Step reward: -15.98681842323373, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.9862645148022\n",
      "Episode: 36, Step: 39\n",
      "Next Action: [-1.380\n",
      "Step reward: -15.991067198339449, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.9773317131417\n",
      "Episode: 36, Step: 40\n",
      "Next Action: [-1.514\n",
      "Step reward: -15.955977230240196, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.9333089433819\n",
      "Episode: 36, Step: 41\n",
      "Next Action: [-1.660\n",
      "Step reward: -15.948307358521948, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.8816163019038\n",
      "Episode: 36, Step: 42\n",
      "Next Action: [-1.595\n",
      "Step reward: -15.93847396127431, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.8200902631781\n",
      "Episode: 36, Step: 43\n",
      "Next Action: [-1.615\n",
      "Step reward: -15.966551940586816, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.7866422037649\n",
      "Episode: 36, Step: 44\n",
      "Next Action: [-1.569\n",
      "Step reward: -15.995259120611575, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.7819013243765\n",
      "Episode: 36, Step: 45\n",
      "Next Action: [-1.792\n",
      "Step reward: -15.993311688200997, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.7752130125774\n",
      "Episode: 36, Step: 46\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.98944803051806, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.7646610430954\n",
      "Episode: 36, Step: 47\n",
      "Next Action: [-1.185\n",
      "Step reward: -15.98954103546249, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.7542020785579\n",
      "Episode: 36, Step: 48\n",
      "Next Action: [-1.273\n",
      "Step reward: -15.99995390614297, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.7541559847009\n",
      "Episode: 36, Step: 49\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.998745106671583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.7529010913725\n",
      "Episode: 36, Step: 50\n",
      "Next Action: [-1.400\n",
      "Step reward: -15.992149413286207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.7450505046587\n",
      "Episode: 36, Step: 51\n",
      "Next Action: [-1.204\n",
      "Step reward: -15.979996586774806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.7250470914336\n",
      "Episode: 36, Step: 52\n",
      "Next Action: [-1.165\n",
      "Step reward: -15.962690377729846, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.6877374691634\n",
      "Episode: 36, Step: 53\n",
      "Next Action: [-1.014\n",
      "Step reward: -15.95008657222602, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.6378240413894\n",
      "Episode: 36, Step: 54\n",
      "Next Action: [-1.443\n",
      "Step reward: -15.93055384074581, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.5683778821352\n",
      "Episode: 36, Step: 55\n",
      "Next Action: [-1.320\n",
      "Step reward: -15.950474258009839, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.5188521401451\n",
      "Episode: 36, Step: 56\n",
      "Next Action: [-1.267\n",
      "Step reward: -15.96781615691355, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.4866682970586\n",
      "Episode: 36, Step: 57\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.977152075858095, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.4638203729166\n",
      "Episode: 36, Step: 58\n",
      "Next Action: [-1.225\n",
      "Step reward: -15.978400899383987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.4422212723007\n",
      "Episode: 36, Step: 59\n",
      "Next Action: [-9.893\n",
      "Step reward: -15.97845977498478, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.4206810472855\n",
      "Episode: 36, Step: 60\n",
      "Next Action: [-1.142\n",
      "Step reward: -15.977798836356692, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.3984798836422\n",
      "Episode: 36, Step: 61\n",
      "Next Action: [-1.505\n",
      "Step reward: -15.9727138511583, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.3711937348005\n",
      "Episode: 36, Step: 62\n",
      "Next Action: [-0.964\n",
      "Step reward: -15.969286792402066, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.3404805272025\n",
      "Episode: 36, Step: 63\n",
      "Next Action: [-0.840\n",
      "Step reward: -15.972931228294252, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.3134117554968\n",
      "Episode: 36, Step: 64\n",
      "Next Action: [-0.825\n",
      "Step reward: -15.987942282474906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.3013540379716\n",
      "Episode: 36, Step: 65\n",
      "Next Action: [-0.497\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.3013540379716\n",
      "Episode: 36, Step: 66\n",
      "Next Action: [-0.429\n",
      "Step reward: -15.99708885051391, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.2984428884854\n",
      "Episode: 36, Step: 67\n",
      "Next Action: [-0.595\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.2984428884854\n",
      "Episode: 36, Step: 68\n",
      "Next Action: [-0.529\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.2984428884854\n",
      "Episode: 36, Step: 69\n",
      "Next Action: [-0.534\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.2984428884854\n",
      "Episode: 36, Step: 70\n",
      "Next Action: [-0.954\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.2984428884854\n",
      "Episode: 36, Step: 71\n",
      "Next Action: [-0.923\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.2984428884854\n",
      "Episode: 36, Step: 72\n",
      "Next Action: [-1.144\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.2984428884854\n",
      "Episode: 36, Step: 73\n",
      "Next Action: [-1.258\n",
      "Step reward: -15.99020243620722, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.2886453246927\n",
      "Episode: 36, Step: 74\n",
      "Next Action: [-1.355\n",
      "Step reward: -15.992945604813487, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.2815909295061\n",
      "Episode: 36, Step: 75\n",
      "Next Action: [-1.378\n",
      "Step reward: -15.983551696776338, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.2651426262826\n",
      "Episode: 36, Step: 76\n",
      "Next Action: [-1.525\n",
      "Step reward: -15.975631412072628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.240774038355\n",
      "Episode: 36, Step: 77\n",
      "Next Action: [-1.605\n",
      "Step reward: -15.993448297610131, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.2342223359651\n",
      "Episode: 36, Step: 78\n",
      "Next Action: [-1.214\n",
      "Step reward: -15.988960675563176, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.2231830115284\n",
      "Episode: 36, Step: 79\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.992247818632926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.2154308301613\n",
      "Episode: 36, Step: 80\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.98233955225335, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.1977703824145\n",
      "Episode: 36, Step: 81\n",
      "Next Action: [-0.777\n",
      "Step reward: -15.999873250729019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.1976436331436\n",
      "Episode: 36, Step: 82\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.997334473602654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.1949781067462\n",
      "Episode: 36, Step: 83\n",
      "Next Action: [-0.703\n",
      "Step reward: -15.994340876784259, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.1893189835305\n",
      "Episode: 36, Step: 84\n",
      "Next Action: [-0.418\n",
      "Step reward: -15.994069577105217, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.1833885606356\n",
      "Episode: 36, Step: 85\n",
      "Next Action: [-0.533\n",
      "Step reward: -15.983597535196603, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.1669860958323\n",
      "Episode: 36, Step: 86\n",
      "Next Action: [-0.580\n",
      "Step reward: -15.963150376533406, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.1301364723656\n",
      "Episode: 36, Step: 87\n",
      "Next Action: [-0.333\n",
      "Step reward: -15.939517778893615, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.0696542512592\n",
      "Episode: 36, Step: 88\n",
      "Next Action: [-0.419\n",
      "Step reward: -15.924468897067191, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.9941231483265\n",
      "Episode: 36, Step: 89\n",
      "Next Action: [-0.400\n",
      "Step reward: -15.924345441215559, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.918468589542\n",
      "Episode: 36, Step: 90\n",
      "Next Action: [-0.800\n",
      "Step reward: -15.936808754538871, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.8552773440808\n",
      "Episode: 36, Step: 91\n",
      "Next Action: [-0.562\n",
      "Step reward: -15.94755373730068, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.8028310813816\n",
      "Episode: 36, Step: 92\n",
      "Next Action: [-0.276\n",
      "Step reward: -15.949552857032126, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.7523839384137\n",
      "Episode: 36, Step: 93\n",
      "Next Action: [-0.623\n",
      "Step reward: -15.94891365502628, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.70129759344\n",
      "Episode: 36, Step: 94\n",
      "Next Action: [-0.682\n",
      "Step reward: -15.960855171222411, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.6621527646623\n",
      "Episode: 36, Step: 95\n",
      "Next Action: [-0.730\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.6621527646623\n",
      "Episode: 36, Step: 96\n",
      "Next Action: [-0.295\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.6621527646623\n",
      "Episode: 36, Step: 97\n",
      "Next Action: [-0.511\n",
      "Step reward: -15.987516162213959, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.6496689268763\n",
      "Episode: 36, Step: 98\n",
      "Next Action: [-0.645\n",
      "Step reward: -15.970960453521231, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.6206293803975\n",
      "Episode: 36, Step: 99\n",
      "Next Action: [-7.197\n",
      "Step reward: -15.9395805752403, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.560209955638\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 1.6436020135879517\n",
      "Actor loss: 51.831275939941406\n",
      "Critic loss: 4.227195739746094\n",
      "Actor loss: 46.863197326660156\n",
      "Critic loss: 2.5853476524353027\n",
      "Actor loss: 48.25708770751953\n",
      "Critic loss: 9.779253959655762\n",
      "Actor loss: 60.697540283203125\n",
      "Critic loss: 8.853116989135742\n",
      "Actor loss: 50.07060623168945\n",
      "Critic loss: 6.570071220397949\n",
      "Actor loss: 43.020652770996094\n",
      "Critic loss: 11.393013954162598\n",
      "Actor loss: 44.8337287902832\n",
      "Critic loss: 2.179095506668091\n",
      "Actor loss: 52.68220138549805\n",
      "Critic loss: 5.630744457244873\n",
      "Actor loss: 59.002113342285156\n",
      "Critic loss: 8.674759864807129\n",
      "Actor loss: 60.39727020263672\n",
      "Episode: 37\n",
      "Episode: 37, Step: 0\n",
      "Next Action: [-0.589\n",
      "Step reward: -11.773738133890543, Next State: [-0.\n",
      "Total episode reward: -11.773738133890543\n",
      "Episode: 37, Step: 1\n",
      "Next Action: [-0.784\n",
      "Step reward: -14.501217096205883, Next State: [-1.\n",
      "Total episode reward: -26.274955230096424\n",
      "Episode: 37, Step: 2\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.413800550346023, Next State: [-1.\n",
      "Total episode reward: -41.68875578044245\n",
      "Episode: 37, Step: 3\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.729602505544344, Next State: [-1.\n",
      "Total episode reward: -57.41835828598679\n",
      "Episode: 37, Step: 4\n",
      "Next Action: [-1.349\n",
      "Step reward: -15.831984906854506, Next State: [-1.\n",
      "Total episode reward: -73.2503431928413\n",
      "Episode: 37, Step: 5\n",
      "Next Action: [-1.474\n",
      "Step reward: -15.881671073601504, Next State: [-1.\n",
      "Total episode reward: -89.13201426644281\n",
      "Episode: 37, Step: 6\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.950381885663221, Next State: [-1.\n",
      "Total episode reward: -105.08239615210603\n",
      "Episode: 37, Step: 7\n",
      "Next Action: [-1.526\n",
      "Step reward: -15.980843771675618, Next State: [-1.\n",
      "Total episode reward: -121.06323992378165\n",
      "Episode: 37, Step: 8\n",
      "Next Action: [-1.482\n",
      "Step reward: -15.982504390171362, Next State: [-1.\n",
      "Total episode reward: -137.045744313953\n",
      "Episode: 37, Step: 9\n",
      "Next Action: [-1.636\n",
      "Step reward: -15.984288505249753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.03003281920277\n",
      "Episode: 37, Step: 10\n",
      "Next Action: [-1.641\n",
      "Step reward: -15.970666302718469, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.00069912192123\n",
      "Episode: 37, Step: 11\n",
      "Next Action: [-1.550\n",
      "Step reward: -15.970550028296211, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.97124915021743\n",
      "Episode: 37, Step: 12\n",
      "Next Action: [-1.478\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.97124915021743\n",
      "Episode: 37, Step: 13\n",
      "Next Action: [-1.314\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.97124915021743\n",
      "Episode: 37, Step: 14\n",
      "Next Action: [-1.290\n",
      "Step reward: -15.99124089654313, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.96249004676056\n",
      "Episode: 37, Step: 15\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.984873871136854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.9473639178974\n",
      "Episode: 37, Step: 16\n",
      "Next Action: [-1.099\n",
      "Step reward: -15.9832311182871, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.9305950361845\n",
      "Episode: 37, Step: 17\n",
      "Next Action: [-0.847\n",
      "Step reward: -15.975392107013896, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.9059871431984\n",
      "Episode: 37, Step: 18\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.973108126704071, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.87909526990245\n",
      "Episode: 37, Step: 19\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.983491600399532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.862586870302\n",
      "Episode: 37, Step: 20\n",
      "Next Action: [-1.250\n",
      "Step reward: -15.991074724421741, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.85366159472375\n",
      "Episode: 37, Step: 21\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.976777612594729, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.8304392073185\n",
      "Episode: 37, Step: 22\n",
      "Next Action: [-1.122\n",
      "Step reward: -15.976132116751254, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.80657132406975\n",
      "Episode: 37, Step: 23\n",
      "Next Action: [-0.855\n",
      "Step reward: -15.989343561710722, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.79591488578046\n",
      "Episode: 37, Step: 24\n",
      "Next Action: [-0.816\n",
      "Step reward: -15.99157656723283, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.7874914530133\n",
      "Episode: 37, Step: 25\n",
      "Next Action: [-8.111\n",
      "Step reward: -15.99153291759047, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.77902437060374\n",
      "Episode: 37, Step: 26\n",
      "Next Action: [-0.574\n",
      "Step reward: -15.975603688476168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.7546280590799\n",
      "Episode: 37, Step: 27\n",
      "Next Action: [-0.967\n",
      "Step reward: -15.97474677175908, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.729374830839\n",
      "Episode: 37, Step: 28\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.987081480298864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.71645631113785\n",
      "Episode: 37, Step: 29\n",
      "Next Action: [-0.434\n",
      "Step reward: -15.984295178705663, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.7007514898435\n",
      "Episode: 37, Step: 30\n",
      "Next Action: [-0.181\n",
      "Step reward: -15.979796678914859, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.6805481687584\n",
      "Episode: 37, Step: 31\n",
      "Next Action: [-0.292\n",
      "Step reward: -15.991979238474226, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.6725274072326\n",
      "Episode: 37, Step: 32\n",
      "Next Action: [-0.294\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.6725274072326\n",
      "Episode: 37, Step: 33\n",
      "Next Action: [-0.245\n",
      "Step reward: -15.997571034845429, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.670098442078\n",
      "Episode: 37, Step: 34\n",
      "Next Action: [-0.492\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.670098442078\n",
      "Episode: 37, Step: 35\n",
      "Next Action: [-0.642\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.670098442078\n",
      "Episode: 37, Step: 36\n",
      "Next Action: [-0.668\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.670098442078\n",
      "Episode: 37, Step: 37\n",
      "Next Action: [-0.390\n",
      "Step reward: -15.993319482245766, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.6634179243238\n",
      "Episode: 37, Step: 38\n",
      "Next Action: [-0.718\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.6634179243238\n",
      "Episode: 37, Step: 39\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.995599106241457, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.6590170305652\n",
      "Episode: 37, Step: 40\n",
      "Next Action: [-0.499\n",
      "Step reward: -15.98863060203398, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.6476476325992\n",
      "Episode: 37, Step: 41\n",
      "Next Action: [-0.771\n",
      "Step reward: -15.977573038916212, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.6252206715154\n",
      "Episode: 37, Step: 42\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.971118823832267, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.5963394953477\n",
      "Episode: 37, Step: 43\n",
      "Next Action: [-1.317\n",
      "Step reward: -15.968739571843173, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.5650790671909\n",
      "Episode: 37, Step: 44\n",
      "Next Action: [-0.813\n",
      "Step reward: -15.9690983225011, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.534177389692\n",
      "Episode: 37, Step: 45\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.969007579587416, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.5031849692795\n",
      "Episode: 37, Step: 46\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.961455818142797, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.4646407874222\n",
      "Episode: 37, Step: 47\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.996131554942297, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.4607723423645\n",
      "Episode: 37, Step: 48\n",
      "Next Action: [-0.673\n",
      "Step reward: -15.997901133255663, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.4586734756202\n",
      "Episode: 37, Step: 49\n",
      "Next Action: [-0.576\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.4586734756202\n",
      "Episode: 37, Step: 50\n",
      "Next Action: [-0.522\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.4586734756202\n",
      "Episode: 37, Step: 51\n",
      "Next Action: [-0.404\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.4586734756202\n",
      "Episode: 37, Step: 52\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.998943661982064, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.4576171376023\n",
      "Episode: 37, Step: 53\n",
      "Next Action: [-1.258\n",
      "Step reward: -15.998227056727757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.45584419433\n",
      "Episode: 37, Step: 54\n",
      "Next Action: [-0.917\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.45584419433\n",
      "Episode: 37, Step: 55\n",
      "Next Action: [-0.839\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.45584419433\n",
      "Episode: 37, Step: 56\n",
      "Next Action: [-0.822\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.45584419433\n",
      "Episode: 37, Step: 57\n",
      "Next Action: [-0.978\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.45584419433\n",
      "Episode: 37, Step: 58\n",
      "Next Action: [-0.889\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.45584419433\n",
      "Episode: 37, Step: 59\n",
      "Next Action: [-0.913\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.45584419433\n",
      "Episode: 37, Step: 60\n",
      "Next Action: [-0.890\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.45584419433\n",
      "Episode: 37, Step: 61\n",
      "Next Action: [-0.847\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.45584419433\n",
      "Episode: 37, Step: 62\n",
      "Next Action: [-0.928\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.45584419433\n",
      "Episode: 37, Step: 63\n",
      "Next Action: [-0.591\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.45584419433\n",
      "Episode: 37, Step: 64\n",
      "Next Action: [-0.797\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.45584419433\n",
      "Episode: 37, Step: 65\n",
      "Next Action: [-0.504\n",
      "Step reward: -15.998049250047737, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.4538934443776\n",
      "Episode: 37, Step: 66\n",
      "Next Action: [-0.350\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.4538934443776\n",
      "Episode: 37, Step: 67\n",
      "Next Action: [-6.131\n",
      "Step reward: -15.992532365249621, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.4464258096273\n",
      "Episode: 37, Step: 68\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.998363863345634, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.444789672973\n",
      "Episode: 37, Step: 69\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.994606243685265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.4393959166582\n",
      "Episode: 37, Step: 70\n",
      "Next Action: [-1.376\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.4393959166582\n",
      "Episode: 37, Step: 71\n",
      "Next Action: [-1.670\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.4393959166582\n",
      "Episode: 37, Step: 72\n",
      "Next Action: [-1.663\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.4393959166582\n",
      "Episode: 37, Step: 73\n",
      "Next Action: [-1.447\n",
      "Step reward: -15.991375374942205, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.4307712916004\n",
      "Episode: 37, Step: 74\n",
      "Next Action: [-1.722\n",
      "Step reward: -15.987411258368361, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.4181825499688\n",
      "Episode: 37, Step: 75\n",
      "Next Action: [-1.319\n",
      "Step reward: -15.971633936261028, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.3898164862298\n",
      "Episode: 37, Step: 76\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.967528584081743, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.3573450703116\n",
      "Episode: 37, Step: 77\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.983032923509187, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.3403779938208\n",
      "Episode: 37, Step: 78\n",
      "Next Action: [-1.025\n",
      "Step reward: -15.987372802977628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.3277507967985\n",
      "Episode: 37, Step: 79\n",
      "Next Action: [-1.146\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.3277507967985\n",
      "Episode: 37, Step: 80\n",
      "Next Action: [-1.091\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.3277507967985\n",
      "Episode: 37, Step: 81\n",
      "Next Action: [-0.902\n",
      "Step reward: -15.992888593266386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.3206393900648\n",
      "Episode: 37, Step: 82\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.996819121480438, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.3174585115453\n",
      "Episode: 37, Step: 83\n",
      "Next Action: [-1.344\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.3174585115453\n",
      "Episode: 37, Step: 84\n",
      "Next Action: [-0.983\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.3174585115453\n",
      "Episode: 37, Step: 85\n",
      "Next Action: [-1.072\n",
      "Step reward: -15.994589901492395, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.3120484130377\n",
      "Episode: 37, Step: 86\n",
      "Next Action: [-1.221\n",
      "Step reward: -15.976781018046154, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.2888294310837\n",
      "Episode: 37, Step: 87\n",
      "Next Action: [-1.100\n",
      "Step reward: -15.964509776347256, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1400.253339207431\n",
      "Episode: 37, Step: 88\n",
      "Next Action: [-1.126\n",
      "Step reward: -15.964784067134852, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.2181232745659\n",
      "Episode: 37, Step: 89\n",
      "Next Action: [-0.859\n",
      "Step reward: -15.95659574060911, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.174719015175\n",
      "Episode: 37, Step: 90\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.95371921540946, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.1284382305844\n",
      "Episode: 37, Step: 91\n",
      "Next Action: [-1.149\n",
      "Step reward: -15.9655703205056, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.09400855109\n",
      "Episode: 37, Step: 92\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.970021506856344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.0640300579462\n",
      "Episode: 37, Step: 93\n",
      "Next Action: [-1.541\n",
      "Step reward: -15.981622838980131, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.0456528969264\n",
      "Episode: 37, Step: 94\n",
      "Next Action: [-1.454\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.0456528969264\n",
      "Episode: 37, Step: 95\n",
      "Next Action: [-1.299\n",
      "Step reward: -15.985729146391199, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.0313820433175\n",
      "Episode: 37, Step: 96\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.977072735217371, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.008454778535\n",
      "Episode: 37, Step: 97\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.976642626444876, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.9850974049798\n",
      "Episode: 37, Step: 98\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.984098929935874, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.9691963349158\n",
      "Episode: 37, Step: 99\n",
      "Next Action: [-1.140\n",
      "Step reward: -15.991313711225795, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.9605100461415\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 21.08351707458496\n",
      "Actor loss: 41.003353118896484\n",
      "Critic loss: 9.201857566833496\n",
      "Actor loss: 52.993141174316406\n",
      "Critic loss: 6.7715325355529785\n",
      "Actor loss: 55.914581298828125\n",
      "Critic loss: 58.56942367553711\n",
      "Actor loss: 40.46418380737305\n",
      "Critic loss: 1.427772045135498\n",
      "Actor loss: 52.8403205871582\n",
      "Critic loss: 6.164971828460693\n",
      "Actor loss: 44.825157165527344\n",
      "Critic loss: 7.147009372711182\n",
      "Actor loss: 50.19553756713867\n",
      "Critic loss: 5.490293979644775\n",
      "Actor loss: 42.24040985107422\n",
      "Critic loss: 6.51773738861084\n",
      "Actor loss: 44.262855529785156\n",
      "Critic loss: 2.3791556358337402\n",
      "Actor loss: 40.97230529785156\n",
      "Episode: 38\n",
      "Episode: 38, Step: 0\n",
      "Next Action: [-1.102\n",
      "Step reward: -11.992553122008317, Next State: [-1.\n",
      "Total episode reward: -11.992553122008317\n",
      "Episode: 38, Step: 1\n",
      "Next Action: [-0.995\n",
      "Step reward: -14.85848061199908, Next State: [-1. \n",
      "Total episode reward: -26.851033734007398\n",
      "Episode: 38, Step: 2\n",
      "Next Action: [-0.838\n",
      "Step reward: -15.47747135496863, Next State: [-1. \n",
      "Total episode reward: -42.32850508897603\n",
      "Episode: 38, Step: 3\n",
      "Next Action: [-0.776\n",
      "Step reward: -15.7752559586283, Next State: [-1.  \n",
      "Total episode reward: -58.10376104760433\n",
      "Episode: 38, Step: 4\n",
      "Next Action: [-1.020\n",
      "Step reward: -15.833223448099858, Next State: [-1.\n",
      "Total episode reward: -73.9369844957042\n",
      "Episode: 38, Step: 5\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.82296745247108, Next State: [-1. \n",
      "Total episode reward: -89.75995194817527\n",
      "Episode: 38, Step: 6\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.848590515724915, Next State: [-1.\n",
      "Total episode reward: -105.60854246390019\n",
      "Episode: 38, Step: 7\n",
      "Next Action: [-1.020\n",
      "Step reward: -15.905166569543054, Next State: [-1.\n",
      "Total episode reward: -121.51370903344325\n",
      "Episode: 38, Step: 8\n",
      "Next Action: [-1.007\n",
      "Step reward: -15.916555308373624, Next State: [-1.\n",
      "Total episode reward: -137.43026434181687\n",
      "Episode: 38, Step: 9\n",
      "Next Action: [-1.177\n",
      "Step reward: -15.93152968820866, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.36179403002552\n",
      "Episode: 38, Step: 10\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.960218172320367, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.3220122023459\n",
      "Episode: 38, Step: 11\n",
      "Next Action: [-0.843\n",
      "Step reward: -15.966420998641778, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.28843320098767\n",
      "Episode: 38, Step: 12\n",
      "Next Action: [-0.919\n",
      "Step reward: -15.985542138650354, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.273975339638\n",
      "Episode: 38, Step: 13\n",
      "Next Action: [-0.996\n",
      "Step reward: -15.996280422500245, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.27025576213825\n",
      "Episode: 38, Step: 14\n",
      "Next Action: [-0.786\n",
      "Step reward: -15.987255888869282, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.25751165100752\n",
      "Episode: 38, Step: 15\n",
      "Next Action: [-0.314\n",
      "Step reward: -15.992323327013041, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.24983497802057\n",
      "Episode: 38, Step: 16\n",
      "Next Action: [-0.665\n",
      "Step reward: -15.987201633894875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.23703661191547\n",
      "Episode: 38, Step: 17\n",
      "Next Action: [-0.715\n",
      "Step reward: -15.979238431321072, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.2162750432365\n",
      "Episode: 38, Step: 18\n",
      "Next Action: [-0.797\n",
      "Step reward: -15.971665201970554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.1879402452071\n",
      "Episode: 38, Step: 19\n",
      "Next Action: [-0.804\n",
      "Step reward: -15.969460202396194, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.15740044760327\n",
      "Episode: 38, Step: 20\n",
      "Next Action: [-0.745\n",
      "Step reward: -15.957154123634258, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.11455457123753\n",
      "Episode: 38, Step: 21\n",
      "Next Action: [-0.720\n",
      "Step reward: -15.964246269027019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.07880084026453\n",
      "Episode: 38, Step: 22\n",
      "Next Action: [-0.582\n",
      "Step reward: -15.976962319137536, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.0557631594021\n",
      "Episode: 38, Step: 23\n",
      "Next Action: [-0.433\n",
      "Step reward: -15.9912933934807, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.0470565528828\n",
      "Episode: 38, Step: 24\n",
      "Next Action: [-0.502\n",
      "Step reward: -15.984899718276765, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.0319562711596\n",
      "Episode: 38, Step: 25\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.994861609445039, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.0268178806046\n",
      "Episode: 38, Step: 26\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.980891264082572, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.00770914468717\n",
      "Episode: 38, Step: 27\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.979036869293292, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.98674601398045\n",
      "Episode: 38, Step: 28\n",
      "Next Action: [-0.933\n",
      "Step reward: -15.975645559949683, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.9623915739301\n",
      "Episode: 38, Step: 29\n",
      "Next Action: [-1.012\n",
      "Step reward: -15.975472439536734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.9378640134669\n",
      "Episode: 38, Step: 30\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.972335222885679, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.9101992363526\n",
      "Episode: 38, Step: 31\n",
      "Next Action: [-1.122\n",
      "Step reward: -15.969034138222222, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.8792333745748\n",
      "Episode: 38, Step: 32\n",
      "Next Action: [-1.123\n",
      "Step reward: -15.952234794804259, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.831468169379\n",
      "Episode: 38, Step: 33\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.930882890047549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.7623510594266\n",
      "Episode: 38, Step: 34\n",
      "Next Action: [-1.634\n",
      "Step reward: -15.927804227954649, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.6901552873812\n",
      "Episode: 38, Step: 35\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.92898050644832, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.6191357938295\n",
      "Episode: 38, Step: 36\n",
      "Next Action: [-1.171\n",
      "Step reward: -15.919138687597567, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.538274481427\n",
      "Episode: 38, Step: 37\n",
      "Next Action: [-1.092\n",
      "Step reward: -15.89309070019951, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.4313651816266\n",
      "Episode: 38, Step: 38\n",
      "Next Action: [-1.132\n",
      "Step reward: -15.889518368163694, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.3208835497903\n",
      "Episode: 38, Step: 39\n",
      "Next Action: [-1.426\n",
      "Step reward: -15.942994088033574, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.263877637824\n",
      "Episode: 38, Step: 40\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.928123050526057, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.19200068835\n",
      "Episode: 38, Step: 41\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.922585181961075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.1145858703111\n",
      "Episode: 38, Step: 42\n",
      "Next Action: [-0.860\n",
      "Step reward: -15.930477131790825, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.0450630021019\n",
      "Episode: 38, Step: 43\n",
      "Next Action: [-0.699\n",
      "Step reward: -15.941212689640675, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -695.9862756917425\n",
      "Episode: 38, Step: 44\n",
      "Next Action: [-0.734\n",
      "Step reward: -15.945841990151909, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -711.9321176818945\n",
      "Episode: 38, Step: 45\n",
      "Next Action: [-1.090\n",
      "Step reward: -15.93016707289881, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -727.8622847547933\n",
      "Episode: 38, Step: 46\n",
      "Next Action: [-1.011\n",
      "Step reward: -15.923338179150399, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -743.7856229339437\n",
      "Episode: 38, Step: 47\n",
      "Next Action: [-1.299\n",
      "Step reward: -15.915506364039343, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -759.701129297983\n",
      "Episode: 38, Step: 48\n",
      "Next Action: [-1.163\n",
      "Step reward: -15.938619939048909, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -775.6397492370319\n",
      "Episode: 38, Step: 49\n",
      "Next Action: [-1.309\n",
      "Step reward: -15.954608575027331, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -791.5943578120592\n",
      "Episode: 38, Step: 50\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.96539043989938, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -807.5597482519586\n",
      "Episode: 38, Step: 51\n",
      "Next Action: [-1.417\n",
      "Step reward: -15.973912693199727, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -823.5336609451583\n",
      "Episode: 38, Step: 52\n",
      "Next Action: [-1.485\n",
      "Step reward: -15.960581882138344, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -839.4942428272967\n",
      "Episode: 38, Step: 53\n",
      "Next Action: [-1.424\n",
      "Step reward: -15.974137152437532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -855.4683799797342\n",
      "Episode: 38, Step: 54\n",
      "Next Action: [-1.242\n",
      "Step reward: -15.989239410974143, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -871.4576193907084\n",
      "Episode: 38, Step: 55\n",
      "Next Action: [-1.558\n",
      "Step reward: -15.997572852687217, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.4551922433956\n",
      "Episode: 38, Step: 56\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.99868087594692, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.4538731193425\n",
      "Episode: 38, Step: 57\n",
      "Next Action: [-1.064\n",
      "Step reward: -15.978387350699938, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.4322604700424\n",
      "Episode: 38, Step: 58\n",
      "Next Action: [-1.236\n",
      "Step reward: -15.953444741682306, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.3857052117247\n",
      "Episode: 38, Step: 59\n",
      "Next Action: [-1.062\n",
      "Step reward: -15.964219439231275, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.349924650956\n",
      "Episode: 38, Step: 60\n",
      "Next Action: [-0.851\n",
      "Step reward: -15.968719571192276, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.3186442221484\n",
      "Episode: 38, Step: 61\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.96913157625676, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.2877757984052\n",
      "Episode: 38, Step: 62\n",
      "Next Action: [-1.266\n",
      "Step reward: -15.970109035010067, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.2578848334152\n",
      "Episode: 38, Step: 63\n",
      "Next Action: [-1.511\n",
      "Step reward: -15.985497972679179, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.2433828060944\n",
      "Episode: 38, Step: 64\n",
      "Next Action: [-1.560\n",
      "Step reward: -15.991318941193025, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.2347017472873\n",
      "Episode: 38, Step: 65\n",
      "Next Action: [-1.418\n",
      "Step reward: -15.996058479885267, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.2307602271726\n",
      "Episode: 38, Step: 66\n",
      "Next Action: [-1.145\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.2307602271726\n",
      "Episode: 38, Step: 67\n",
      "Next Action: [-1.295\n",
      "Step reward: -16.0, Next State: [-1. -1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.2307602271726\n",
      "Episode: 38, Step: 68\n",
      "Next Action: [-1.375\n",
      "Step reward: -15.995273355119604, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.2260335822923\n",
      "Episode: 38, Step: 69\n",
      "Next Action: [-1.514\n",
      "Step reward: -15.989590906458696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.215624488751\n",
      "Episode: 38, Step: 70\n",
      "Next Action: [-1.203\n",
      "Step reward: -15.976756160109582, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.1923806488605\n",
      "Episode: 38, Step: 71\n",
      "Next Action: [-0.843\n",
      "Step reward: -15.970726512898947, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1143.1631071617594\n",
      "Episode: 38, Step: 72\n",
      "Next Action: [-0.578\n",
      "Step reward: -15.968837262754635, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.1319444245141\n",
      "Episode: 38, Step: 73\n",
      "Next Action: [-0.589\n",
      "Step reward: -15.971504441532849, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.103448866047\n",
      "Episode: 38, Step: 74\n",
      "Next Action: [-0.798\n",
      "Step reward: -15.970740741919743, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.074189607967\n",
      "Episode: 38, Step: 75\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.98281673863641, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.0570063466032\n",
      "Episode: 38, Step: 76\n",
      "Next Action: [-1.089\n",
      "Step reward: -15.996707087505456, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.0537134341087\n",
      "Episode: 38, Step: 77\n",
      "Next Action: [-1.065\n",
      "Step reward: -15.989939530756628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.0436529648653\n",
      "Episode: 38, Step: 78\n",
      "Next Action: [-1.212\n",
      "Step reward: -15.993227394482851, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.036880359348\n",
      "Episode: 38, Step: 79\n",
      "Next Action: [-1.211\n",
      "Step reward: -15.98820784027364, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.0250881996217\n",
      "Episode: 38, Step: 80\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.982267414975619, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.0073556145974\n",
      "Episode: 38, Step: 81\n",
      "Next Action: [-0.677\n",
      "Step reward: -15.99115093839689, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.9985065529943\n",
      "Episode: 38, Step: 82\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.994402126242871, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.9929086792372\n",
      "Episode: 38, Step: 83\n",
      "Next Action: [-0.783\n",
      "Step reward: -15.987550007111592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.9804586863488\n",
      "Episode: 38, Step: 84\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.976686911142927, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.9571455974917\n",
      "Episode: 38, Step: 85\n",
      "Next Action: [-0.706\n",
      "Step reward: -15.977748775932698, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.9348943734244\n",
      "Episode: 38, Step: 86\n",
      "Next Action: [-1.143\n",
      "Step reward: -15.969654902057858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.9045492754822\n",
      "Episode: 38, Step: 87\n",
      "Next Action: [-1.501\n",
      "Step reward: -15.954182836494429, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.8587321119767\n",
      "Episode: 38, Step: 88\n",
      "Next Action: [-1.501\n",
      "Step reward: -15.968759767808434, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.8274918797852\n",
      "Episode: 38, Step: 89\n",
      "Next Action: [-1.579\n",
      "Step reward: -15.968778147509374, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.7962700272944\n",
      "Episode: 38, Step: 90\n",
      "Next Action: [-1.652\n",
      "Step reward: -15.968734806821349, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.7650048341159\n",
      "Episode: 38, Step: 91\n",
      "Next Action: [-1.275\n",
      "Step reward: -15.968783692338926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.7337885264549\n",
      "Episode: 38, Step: 92\n",
      "Next Action: [-1.195\n",
      "Step reward: -15.968913542827577, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.7027020692824\n",
      "Episode: 38, Step: 93\n",
      "Next Action: [-0.911\n",
      "Step reward: -15.963868792806855, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1494.6665708620892\n",
      "Episode: 38, Step: 94\n",
      "Next Action: [-0.996\n",
      "Step reward: -15.951133933012617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1510.6177047951019\n",
      "Episode: 38, Step: 95\n",
      "Next Action: [-1.242\n",
      "Step reward: -15.955524009536262, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1526.5732288046381\n",
      "Episode: 38, Step: 96\n",
      "Next Action: [-1.198\n",
      "Step reward: -15.955138944747207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1542.5283677493853\n",
      "Episode: 38, Step: 97\n",
      "Next Action: [-0.656\n",
      "Step reward: -15.92711048266912, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.4554782320545\n",
      "Episode: 38, Step: 98\n",
      "Next Action: [-0.595\n",
      "Step reward: -15.928667952631288, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1574.3841461846857\n",
      "Episode: 38, Step: 99\n",
      "Next Action: [-0.868\n",
      "Step reward: -15.906640783550897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1590.2907869682367\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 23.157875061035156\n",
      "Actor loss: 37.843605041503906\n",
      "Critic loss: 12.098777770996094\n",
      "Actor loss: 46.120567321777344\n",
      "Critic loss: 27.926578521728516\n",
      "Actor loss: 27.84006118774414\n",
      "Critic loss: 1.9600523710250854\n",
      "Actor loss: 45.37540817260742\n",
      "Critic loss: 1.886374592781067\n",
      "Actor loss: 51.566139221191406\n",
      "Critic loss: 1.426681399345398\n",
      "Actor loss: 47.846412658691406\n",
      "Critic loss: 2.2521579265594482\n",
      "Actor loss: 46.547752380371094\n",
      "Critic loss: 34.308738708496094\n",
      "Actor loss: 43.43461990356445\n",
      "Critic loss: 1.4318732023239136\n",
      "Actor loss: 55.51222610473633\n",
      "Critic loss: 1.752806544303894\n",
      "Actor loss: 53.272178649902344\n",
      "Episode: 39\n",
      "Episode: 39, Step: 0\n",
      "Next Action: [-0.676\n",
      "Step reward: -11.80377246328644, Next State: [-0.1\n",
      "Total episode reward: -11.80377246328644\n",
      "Episode: 39, Step: 1\n",
      "Next Action: [-0.862\n",
      "Step reward: -14.765834877029944, Next State: [-0.\n",
      "Total episode reward: -26.569607340316384\n",
      "Episode: 39, Step: 2\n",
      "Next Action: [-1.019\n",
      "Step reward: -15.633222584398885, Next State: [-1.\n",
      "Total episode reward: -42.20282992471527\n",
      "Episode: 39, Step: 3\n",
      "Next Action: [-0.971\n",
      "Step reward: -15.86337070970386, Next State: [-1. \n",
      "Total episode reward: -58.066200634419125\n",
      "Episode: 39, Step: 4\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.920716417387231, Next State: [-1.\n",
      "Total episode reward: -73.98691705180636\n",
      "Episode: 39, Step: 5\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.929124161766431, Next State: [-1.\n",
      "Total episode reward: -89.91604121357278\n",
      "Episode: 39, Step: 6\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.915527103992314, Next State: [-1.\n",
      "Total episode reward: -105.8315683175651\n",
      "Episode: 39, Step: 7\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.909810561923296, Next State: [-1.\n",
      "Total episode reward: -121.7413788794884\n",
      "Episode: 39, Step: 8\n",
      "Next Action: [-1.237\n",
      "Step reward: -15.905974001132428, Next State: [-1.\n",
      "Total episode reward: -137.64735288062082\n",
      "Episode: 39, Step: 9\n",
      "Next Action: [-0.822\n",
      "Step reward: -15.910694071340053, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.55804695196088\n",
      "Episode: 39, Step: 10\n",
      "Next Action: [-0.524\n",
      "Step reward: -15.924390635355445, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.48243758731633\n",
      "Episode: 39, Step: 11\n",
      "Next Action: [-0.509\n",
      "Step reward: -15.942113121407072, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.4245507087234\n",
      "Episode: 39, Step: 12\n",
      "Next Action: [-0.635\n",
      "Step reward: -15.981493319022338, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.40604402774574\n",
      "Episode: 39, Step: 13\n",
      "Next Action: [-4.767\n",
      "Step reward: -15.981844639287525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.38788866703325\n",
      "Episode: 39, Step: 14\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.980851928802423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.36874059583567\n",
      "Episode: 39, Step: 15\n",
      "Next Action: [-0.752\n",
      "Step reward: -15.973527836937294, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.34226843277298\n",
      "Episode: 39, Step: 16\n",
      "Next Action: [-0.591\n",
      "Step reward: -15.968664673577246, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.3109331063502\n",
      "Episode: 39, Step: 17\n",
      "Next Action: [-0.344\n",
      "Step reward: -15.964290972240503, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.2752240785907\n",
      "Episode: 39, Step: 18\n",
      "Next Action: [-0.529\n",
      "Step reward: -15.986103840625596, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.26132791921634\n",
      "Episode: 39, Step: 19\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.997874213156281, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.25920213237265\n",
      "Episode: 39, Step: 20\n",
      "Next Action: [-8.365\n",
      "Step reward: -15.988099315494207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.24730144786685\n",
      "Episode: 39, Step: 21\n",
      "Next Action: [-0.638\n",
      "Step reward: -15.989645405638678, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.2369468535055\n",
      "Episode: 39, Step: 22\n",
      "Next Action: [-0.574\n",
      "Step reward: -15.993616644666034, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.23056349817153\n",
      "Episode: 39, Step: 23\n",
      "Next Action: [-0.953\n",
      "Step reward: -15.99355656072098, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.22412005889254\n",
      "Episode: 39, Step: 24\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.990133503571585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.21425356246414\n",
      "Episode: 39, Step: 25\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.999747257584934, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.2140008200491\n",
      "Episode: 39, Step: 26\n",
      "Next Action: [-1.413\n",
      "Step reward: -15.999570631993913, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.21357145204297\n",
      "Episode: 39, Step: 27\n",
      "Next Action: [-0.953\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.21357145204297\n",
      "Episode: 39, Step: 28\n",
      "Next Action: [-0.855\n",
      "Step reward: -15.99103094005008, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.2046023920931\n",
      "Episode: 39, Step: 29\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.960774123447061, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.1653765155401\n",
      "Episode: 39, Step: 30\n",
      "Next Action: [-1.197\n",
      "Step reward: -15.962356481211108, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.1277329967512\n",
      "Episode: 39, Step: 31\n",
      "Next Action: [-1.156\n",
      "Step reward: -15.967049065203899, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.09478206195513\n",
      "Episode: 39, Step: 32\n",
      "Next Action: [-1.313\n",
      "Step reward: -15.96670628463152, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.0614883465867\n",
      "Episode: 39, Step: 33\n",
      "Next Action: [-1.295\n",
      "Step reward: -15.963231149852488, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.0247194964392\n",
      "Episode: 39, Step: 34\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.975950434332539, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.0006699307717\n",
      "Episode: 39, Step: 35\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.989645002176307, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.990314932948\n",
      "Episode: 39, Step: 36\n",
      "Next Action: [-1.288\n",
      "Step reward: -15.996653182933139, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.9869681158812\n",
      "Episode: 39, Step: 37\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.983067967832204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.9700360837134\n",
      "Episode: 39, Step: 38\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.971109627804532, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.9411457115178\n",
      "Episode: 39, Step: 39\n",
      "Next Action: [-1.232\n",
      "Step reward: -15.95274374843347, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.8938894599513\n",
      "Episode: 39, Step: 40\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.953313716293826, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.8472031762451\n",
      "Episode: 39, Step: 41\n",
      "Next Action: [-1.022\n",
      "Step reward: -15.969824591282084, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.8170277675272\n",
      "Episode: 39, Step: 42\n",
      "Next Action: [-9.878\n",
      "Step reward: -15.96867247113125, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.7857002386585\n",
      "Episode: 39, Step: 43\n",
      "Next Action: [-0.979\n",
      "Step reward: -15.969970125559321, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.7556703642178\n",
      "Episode: 39, Step: 44\n",
      "Next Action: [-1.182\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.7556703642178\n",
      "Episode: 39, Step: 45\n",
      "Next Action: [-1.232\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.7556703642178\n",
      "Episode: 39, Step: 46\n",
      "Next Action: [-1.487\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.7556703642178\n",
      "Episode: 39, Step: 47\n",
      "Next Action: [-1.449\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.7556703642178\n",
      "Episode: 39, Step: 48\n",
      "Next Action: [-1.023\n",
      "Step reward: -15.990361527768174, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.746031891986\n",
      "Episode: 39, Step: 49\n",
      "Next Action: [-0.965\n",
      "Step reward: -15.976371697469549, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.7224035894556\n",
      "Episode: 39, Step: 50\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.981601079198839, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.7040046686544\n",
      "Episode: 39, Step: 51\n",
      "Next Action: [-1.235\n",
      "Step reward: -15.980483249677892, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.6844879183324\n",
      "Episode: 39, Step: 52\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.97897974742332, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.6634676657557\n",
      "Episode: 39, Step: 53\n",
      "Next Action: [-1.254\n",
      "Step reward: -15.97671645432215, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.6401841200778\n",
      "Episode: 39, Step: 54\n",
      "Next Action: [-1.167\n",
      "Step reward: -15.988281112087204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.628465232165\n",
      "Episode: 39, Step: 55\n",
      "Next Action: [-1.099\n",
      "Step reward: -15.997267420162043, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.6257326523271\n",
      "Episode: 39, Step: 56\n",
      "Next Action: [-1.083\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.6257326523271\n",
      "Episode: 39, Step: 57\n",
      "Next Action: [-1.381\n",
      "Step reward: -15.986002443950499, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.6117350962776\n",
      "Episode: 39, Step: 58\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.968882730026717, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.5806178263043\n",
      "Episode: 39, Step: 59\n",
      "Next Action: [-1.434\n",
      "Step reward: -15.980284528944921, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.5609023552493\n",
      "Episode: 39, Step: 60\n",
      "Next Action: [-1.531\n",
      "Step reward: -16.0, Next State: [-1.  1.  1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.5609023552493\n",
      "Episode: 39, Step: 61\n",
      "Next Action: [-1.497\n",
      "Step reward: -15.995490954810329, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.5563933100595\n",
      "Episode: 39, Step: 62\n",
      "Next Action: [-1.243\n",
      "Step reward: -15.997317329348341, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.5537106394079\n",
      "Episode: 39, Step: 63\n",
      "Next Action: [-1.439\n",
      "Step reward: -15.99949856737988, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.5532092067878\n",
      "Episode: 39, Step: 64\n",
      "Next Action: [-1.479\n",
      "Step reward: -15.996299555756199, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.549508762544\n",
      "Episode: 39, Step: 65\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.995209416312242, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.5447181788563\n",
      "Episode: 39, Step: 66\n",
      "Next Action: [-1.435\n",
      "Step reward: -15.991008997693111, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.5357271765495\n",
      "Episode: 39, Step: 67\n",
      "Next Action: [-1.360\n",
      "Step reward: -15.984416051753977, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.5201432283034\n",
      "Episode: 39, Step: 68\n",
      "Next Action: [-1.502\n",
      "Step reward: -15.98019086658386, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.5003340948872\n",
      "Episode: 39, Step: 69\n",
      "Next Action: [-1.628\n",
      "Step reward: -15.99557848720211, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.4959125820892\n",
      "Episode: 39, Step: 70\n",
      "Next Action: [-1.749\n",
      "Step reward: -15.993899128068929, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.4898117101582\n",
      "Episode: 39, Step: 71\n",
      "Next Action: [-1.719\n",
      "Step reward: -15.991540723165176, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.4813524333233\n",
      "Episode: 39, Step: 72\n",
      "Next Action: [-1.572\n",
      "Step reward: -15.990421728086144, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.4717741614095\n",
      "Episode: 39, Step: 73\n",
      "Next Action: [-1.087\n",
      "Step reward: -15.97228324026118, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.4440574016708\n",
      "Episode: 39, Step: 74\n",
      "Next Action: [-1.010\n",
      "Step reward: -15.96875534505056, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.4128127467213\n",
      "Episode: 39, Step: 75\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.95119919389057, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.3640119406118\n",
      "Episode: 39, Step: 76\n",
      "Next Action: [-0.648\n",
      "Step reward: -15.952909846030037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.3169217866418\n",
      "Episode: 39, Step: 77\n",
      "Next Action: [-0.484\n",
      "Step reward: -15.963836836102569, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.2807586227443\n",
      "Episode: 39, Step: 78\n",
      "Next Action: [-0.377\n",
      "Step reward: -15.971244672542664, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.252003295287\n",
      "Episode: 39, Step: 79\n",
      "Next Action: [-0.565\n",
      "Step reward: -15.97840904236563, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.2304123376528\n",
      "Episode: 39, Step: 80\n",
      "Next Action: [-0.561\n",
      "Step reward: -15.990197437254373, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.2206097749072\n",
      "Episode: 39, Step: 81\n",
      "Next Action: [-5.990\n",
      "Step reward: -15.985527170919914, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.2061369458272\n",
      "Episode: 39, Step: 82\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.977686299000155, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.1838232448274\n",
      "Episode: 39, Step: 83\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.968159459229048, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.1519827040565\n",
      "Episode: 39, Step: 84\n",
      "Next Action: [-0.886\n",
      "Step reward: -15.933922424034694, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.0859051280913\n",
      "Episode: 39, Step: 85\n",
      "Next Action: [-0.592\n",
      "Step reward: -15.921842263875288, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.0077473919666\n",
      "Episode: 39, Step: 86\n",
      "Next Action: [-0.651\n",
      "Step reward: -15.92283000498907, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.9305773969556\n",
      "Episode: 39, Step: 87\n",
      "Next Action: [-0.563\n",
      "Step reward: -15.917139671774907, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.8477170687306\n",
      "Episode: 39, Step: 88\n",
      "Next Action: [-0.771\n",
      "Step reward: -15.932324648507757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.7800417172384\n",
      "Episode: 39, Step: 89\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.941860900039886, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.7219026172784\n",
      "Episode: 39, Step: 90\n",
      "Next Action: [-0.635\n",
      "Step reward: -15.956310031042962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.6782126483213\n",
      "Episode: 39, Step: 91\n",
      "Next Action: [-0.716\n",
      "Step reward: -15.966470353867185, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.6446830021885\n",
      "Episode: 39, Step: 92\n",
      "Next Action: [-0.770\n",
      "Step reward: -15.97268585128003, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.6173688534684\n",
      "Episode: 39, Step: 93\n",
      "Next Action: [-1.273\n",
      "Step reward: -15.97563995932101, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.5930088127893\n",
      "Episode: 39, Step: 94\n",
      "Next Action: [-0.976\n",
      "Step reward: -15.98000714146942, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.5730159542588\n",
      "Episode: 39, Step: 95\n",
      "Next Action: [-0.959\n",
      "Step reward: -15.974653336565206, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.547669290824\n",
      "Episode: 39, Step: 96\n",
      "Next Action: [-1.460\n",
      "Step reward: -15.974411270326257, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.5220805611502\n",
      "Episode: 39, Step: 97\n",
      "Next Action: [-1.338\n",
      "Step reward: -15.983025138710024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.5051056998602\n",
      "Episode: 39, Step: 98\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.98309875942434, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.4882044592846\n",
      "Episode: 39, Step: 99\n",
      "Next Action: [-1.307\n",
      "Step reward: -15.956128802977172, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.4443332622618\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 2.671861410140991\n",
      "Actor loss: 33.323020935058594\n",
      "Critic loss: 4.201001167297363\n",
      "Actor loss: 35.105377197265625\n",
      "Critic loss: 10.893081665039062\n",
      "Actor loss: 57.32476806640625\n",
      "Critic loss: 1.502097249031067\n",
      "Actor loss: 35.888790130615234\n",
      "Critic loss: 1.7168411016464233\n",
      "Actor loss: 49.94568634033203\n",
      "Critic loss: 1.654634714126587\n",
      "Actor loss: 53.7061767578125\n",
      "Critic loss: 24.416942596435547\n",
      "Actor loss: 28.991031646728516\n",
      "Critic loss: 1.6865452527999878\n",
      "Actor loss: 50.51512145996094\n",
      "Critic loss: 1.6203690767288208\n",
      "Actor loss: 57.68526077270508\n",
      "Critic loss: 2.227177858352661\n",
      "Actor loss: 56.24528121948242\n",
      "Episode: 40\n",
      "Episode: 40, Step: 0\n",
      "Next Action: [-1.448\n",
      "Step reward: -11.759249460182136, Next State: [-0.\n",
      "Total episode reward: -11.759249460182136\n",
      "Episode: 40, Step: 1\n",
      "Next Action: [-1.452\n",
      "Step reward: -14.819652892599452, Next State: [-1.\n",
      "Total episode reward: -26.57890235278159\n",
      "Episode: 40, Step: 2\n",
      "Next Action: [-1.803\n",
      "Step reward: -15.655650723601426, Next State: [-1.\n",
      "Total episode reward: -42.234553076383015\n",
      "Episode: 40, Step: 3\n",
      "Next Action: [-1.827\n",
      "Step reward: -15.821939534774085, Next State: [-1.\n",
      "Total episode reward: -58.0564926111571\n",
      "Episode: 40, Step: 4\n",
      "Next Action: [-1.641\n",
      "Step reward: -15.864335459309652, Next State: [-1.\n",
      "Total episode reward: -73.92082807046675\n",
      "Episode: 40, Step: 5\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.894277159184718, Next State: [-1.\n",
      "Total episode reward: -89.81510522965146\n",
      "Episode: 40, Step: 6\n",
      "Next Action: [-1.596\n",
      "Step reward: -15.920687645111501, Next State: [-1.\n",
      "Total episode reward: -105.73579287476296\n",
      "Episode: 40, Step: 7\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.947921463980945, Next State: [-1.\n",
      "Total episode reward: -121.68371433874391\n",
      "Episode: 40, Step: 8\n",
      "Next Action: [-1.433\n",
      "Step reward: -15.947425516731924, Next State: [-1.\n",
      "Total episode reward: -137.63113985547582\n",
      "Episode: 40, Step: 9\n",
      "Next Action: [-1.638\n",
      "Step reward: -15.958808677162759, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.58994853263857\n",
      "Episode: 40, Step: 10\n",
      "Next Action: [-1.627\n",
      "Step reward: -15.973606776548563, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.56355530918714\n",
      "Episode: 40, Step: 11\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.982293337548258, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.5458486467354\n",
      "Episode: 40, Step: 12\n",
      "Next Action: [-0.857\n",
      "Step reward: -15.99122165946957, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.53707030620498\n",
      "Episode: 40, Step: 13\n",
      "Next Action: [-0.783\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.53707030620498\n",
      "Episode: 40, Step: 14\n",
      "Next Action: [-0.870\n",
      "Step reward: -15.995565143128147, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.53263544933313\n",
      "Episode: 40, Step: 15\n",
      "Next Action: [-0.894\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.53263544933313\n",
      "Episode: 40, Step: 16\n",
      "Next Action: [-1.241\n",
      "Step reward: -15.980148672937064, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.5127841222702\n",
      "Episode: 40, Step: 17\n",
      "Next Action: [-1.402\n",
      "Step reward: -15.976969877021752, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.48975399929196\n",
      "Episode: 40, Step: 18\n",
      "Next Action: [-1.309\n",
      "Step reward: -15.975466876257201, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.46522087554916\n",
      "Episode: 40, Step: 19\n",
      "Next Action: [-1.005\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.46522087554916\n",
      "Episode: 40, Step: 20\n",
      "Next Action: [-1.307\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.46522087554916\n",
      "Episode: 40, Step: 21\n",
      "Next Action: [-1.395\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.46522087554916\n",
      "Episode: 40, Step: 22\n",
      "Next Action: [-1.362\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.46522087554916\n",
      "Episode: 40, Step: 23\n",
      "Next Action: [-1.410\n",
      "Step reward: -15.96895132055901, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.4341721961082\n",
      "Episode: 40, Step: 24\n",
      "Next Action: [-1.378\n",
      "Step reward: -15.945890125531333, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.3800623216395\n",
      "Episode: 40, Step: 25\n",
      "Next Action: [-1.424\n",
      "Step reward: -15.94041346856351, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.32047579020303\n",
      "Episode: 40, Step: 26\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.941746901782617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.26222269198564\n",
      "Episode: 40, Step: 27\n",
      "Next Action: [-1.346\n",
      "Step reward: -15.952415173074295, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.21463786505996\n",
      "Episode: 40, Step: 28\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.962161756273293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.17679962133326\n",
      "Episode: 40, Step: 29\n",
      "Next Action: [-1.159\n",
      "Step reward: -15.93464672822666, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.1114463495599\n",
      "Episode: 40, Step: 30\n",
      "Next Action: [-1.398\n",
      "Step reward: -15.922599067610276, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.0340454171702\n",
      "Episode: 40, Step: 31\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.903293054547541, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.93733847171774\n",
      "Episode: 40, Step: 32\n",
      "Next Action: [-0.976\n",
      "Step reward: -15.909055021705926, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.8463934934236\n",
      "Episode: 40, Step: 33\n",
      "Next Action: [-1.059\n",
      "Step reward: -15.928632984306521, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.7750264777302\n",
      "Episode: 40, Step: 34\n",
      "Next Action: [-1.050\n",
      "Step reward: -15.950721672896607, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.7257481506268\n",
      "Episode: 40, Step: 35\n",
      "Next Action: [-1.165\n",
      "Step reward: -15.983141965379744, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.7088901160066\n",
      "Episode: 40, Step: 36\n",
      "Next Action: [-0.860\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.7088901160066\n",
      "Episode: 40, Step: 37\n",
      "Next Action: [-1.128\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.7088901160066\n",
      "Episode: 40, Step: 38\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.993060049031916, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.7019501650385\n",
      "Episode: 40, Step: 39\n",
      "Next Action: [-0.693\n",
      "Step reward: -15.993336134357026, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.6952862993955\n",
      "Episode: 40, Step: 40\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.983748857875264, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.6790351572708\n",
      "Episode: 40, Step: 41\n",
      "Next Action: [-0.560\n",
      "Step reward: -15.979867194942324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.6589023522132\n",
      "Episode: 40, Step: 42\n",
      "Next Action: [-0.500\n",
      "Step reward: -15.999581600193917, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.6584839524071\n",
      "Episode: 40, Step: 43\n",
      "Next Action: [-0.329\n",
      "Step reward: -15.986775034162276, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.6452589865694\n",
      "Episode: 40, Step: 44\n",
      "Next Action: [-0.313\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.6452589865694\n",
      "Episode: 40, Step: 45\n",
      "Next Action: [-0.590\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.6452589865694\n",
      "Episode: 40, Step: 46\n",
      "Next Action: [-0.646\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.6452589865694\n",
      "Episode: 40, Step: 47\n",
      "Next Action: [-0.281\n",
      "Step reward: -15.989317972731014, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.6345769593004\n",
      "Episode: 40, Step: 48\n",
      "Next Action: [-0.454\n",
      "Step reward: -15.996158135430637, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.630735094731\n",
      "Episode: 40, Step: 49\n",
      "Next Action: [-0.529\n",
      "Step reward: -15.991295301517576, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.6220303962485\n",
      "Episode: 40, Step: 50\n",
      "Next Action: [-0.745\n",
      "Step reward: -15.986423308042545, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.608453704291\n",
      "Episode: 40, Step: 51\n",
      "Next Action: [-0.585\n",
      "Step reward: -15.992752752553445, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.6012064568445\n",
      "Episode: 40, Step: 52\n",
      "Next Action: [-0.627\n",
      "Step reward: -15.998863832725059, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.6000702895695\n",
      "Episode: 40, Step: 53\n",
      "Next Action: [-0.740\n",
      "Step reward: -15.996638030412962, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.5967083199826\n",
      "Episode: 40, Step: 54\n",
      "Next Action: [-0.701\n",
      "Step reward: -15.989410870864186, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.5861191908467\n",
      "Episode: 40, Step: 55\n",
      "Next Action: [-0.800\n",
      "Step reward: -15.992047928820204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.5781671196669\n",
      "Episode: 40, Step: 56\n",
      "Next Action: [-0.933\n",
      "Step reward: -15.98458512446254, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.5627522441295\n",
      "Episode: 40, Step: 57\n",
      "Next Action: [-1.000\n",
      "Step reward: -15.989665471892339, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.5524177160218\n",
      "Episode: 40, Step: 58\n",
      "Next Action: [-1.047\n",
      "Step reward: -15.98726962919855, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.5396873452203\n",
      "Episode: 40, Step: 59\n",
      "Next Action: [-1.326\n",
      "Step reward: -15.97760861361521, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.5172959588355\n",
      "Episode: 40, Step: 60\n",
      "Next Action: [-1.172\n",
      "Step reward: -15.974746218826173, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.4920421776617\n",
      "Episode: 40, Step: 61\n",
      "Next Action: [-0.895\n",
      "Step reward: -15.981054343168685, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.4730965208304\n",
      "Episode: 40, Step: 62\n",
      "Next Action: [-0.500\n",
      "Step reward: -15.967442670539928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.4405391913704\n",
      "Episode: 40, Step: 63\n",
      "Next Action: [-0.748\n",
      "Step reward: -15.964246944200937, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.4047861355713\n",
      "Episode: 40, Step: 64\n",
      "Next Action: [-0.946\n",
      "Step reward: -15.964068787167749, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.368854922739\n",
      "Episode: 40, Step: 65\n",
      "Next Action: [-0.890\n",
      "Step reward: -15.968020591542414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.3368755142815\n",
      "Episode: 40, Step: 66\n",
      "Next Action: [-0.755\n",
      "Step reward: -15.969835927952445, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.306711442234\n",
      "Episode: 40, Step: 67\n",
      "Next Action: [-0.812\n",
      "Step reward: -15.972070789396971, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.2787822316309\n",
      "Episode: 40, Step: 68\n",
      "Next Action: [-0.998\n",
      "Step reward: -15.974615664771168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.253397896402\n",
      "Episode: 40, Step: 69\n",
      "Next Action: [-0.766\n",
      "Step reward: -15.96181702749917, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.2152149239012\n",
      "Episode: 40, Step: 70\n",
      "Next Action: [-0.465\n",
      "Step reward: -15.961494002040352, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.1767089259415\n",
      "Episode: 40, Step: 71\n",
      "Next Action: [-0.561\n",
      "Step reward: -15.977563069405353, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.1542719953468\n",
      "Episode: 40, Step: 72\n",
      "Next Action: [-0.544\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.1542719953468\n",
      "Episode: 40, Step: 73\n",
      "Next Action: [-0.605\n",
      "Step reward: -15.993141280500781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.1474132758476\n",
      "Episode: 40, Step: 74\n",
      "Next Action: [-0.808\n",
      "Step reward: -15.989223607518126, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.1366368833658\n",
      "Episode: 40, Step: 75\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.997639570723376, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.134276454089\n",
      "Episode: 40, Step: 76\n",
      "Next Action: [-1.010\n",
      "Step reward: -15.998799137557738, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.1330755916467\n",
      "Episode: 40, Step: 77\n",
      "Next Action: [-1.301\n",
      "Step reward: -15.999953742320097, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.1330293339668\n",
      "Episode: 40, Step: 78\n",
      "Next Action: [-1.222\n",
      "Step reward: -15.994540706695801, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.1275700406625\n",
      "Episode: 40, Step: 79\n",
      "Next Action: [-1.339\n",
      "Step reward: -15.982542664867337, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.11011270553\n",
      "Episode: 40, Step: 80\n",
      "Next Action: [-1.636\n",
      "Step reward: -15.96564491231758, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.0757576178476\n",
      "Episode: 40, Step: 81\n",
      "Next Action: [-1.869\n",
      "Step reward: -15.981671790491735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.0574294083392\n",
      "Episode: 40, Step: 82\n",
      "Next Action: [-1.864\n",
      "Step reward: -15.982477618798912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.039907027138\n",
      "Episode: 40, Step: 83\n",
      "Next Action: [-1.972\n",
      "Step reward: -15.97467332458727, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.0145803517253\n",
      "Episode: 40, Step: 84\n",
      "Next Action: [-1.988\n",
      "Step reward: -15.973704980302204, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.9882853320275\n",
      "Episode: 40, Step: 85\n",
      "Next Action: [-1.845\n",
      "Step reward: -15.981878812988972, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.9701641450165\n",
      "Episode: 40, Step: 86\n",
      "Next Action: [-1.373\n",
      "Step reward: -15.996856597138539, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.967020742155\n",
      "Episode: 40, Step: 87\n",
      "Next Action: [-1.039\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.967020742155\n",
      "Episode: 40, Step: 88\n",
      "Next Action: [-0.891\n",
      "Step reward: -15.996716522368937, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.9637372645238\n",
      "Episode: 40, Step: 89\n",
      "Next Action: [-1.111\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.9637372645238\n",
      "Episode: 40, Step: 90\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.973520016604978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.9372572811287\n",
      "Episode: 40, Step: 91\n",
      "Next Action: [-1.061\n",
      "Step reward: -15.969672032995906, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.9069293141247\n",
      "Episode: 40, Step: 92\n",
      "Next Action: [-1.459\n",
      "Step reward: -15.977124301501943, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.8840536156267\n",
      "Episode: 40, Step: 93\n",
      "Next Action: [-1.756\n",
      "Step reward: -15.996250208875162, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.8803038245019\n",
      "Episode: 40, Step: 94\n",
      "Next Action: [-1.477\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.8803038245019\n",
      "Episode: 40, Step: 95\n",
      "Next Action: [-1.473\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.8803038245019\n",
      "Episode: 40, Step: 96\n",
      "Next Action: [-1.646\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.8803038245019\n",
      "Episode: 40, Step: 97\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.99652410372633, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.8768279282283\n",
      "Episode: 40, Step: 98\n",
      "Next Action: [-1.423\n",
      "Step reward: -15.98561231364257, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.862440241871\n",
      "Episode: 40, Step: 99\n",
      "Next Action: [-1.129\n",
      "Step reward: -15.973198937832674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.8356391797035\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 2.0115087032318115\n",
      "Actor loss: 52.3555793762207\n",
      "Critic loss: 52.47239685058594\n",
      "Actor loss: 26.622121810913086\n",
      "Critic loss: 1.6929856538772583\n",
      "Actor loss: 50.69831848144531\n",
      "Critic loss: 1.4717073440551758\n",
      "Actor loss: 51.46111297607422\n",
      "Critic loss: 9.826312065124512\n",
      "Actor loss: 43.091678619384766\n",
      "Critic loss: 1.8463070392608643\n",
      "Actor loss: 54.49120330810547\n",
      "Critic loss: 22.984546661376953\n",
      "Actor loss: 60.96152877807617\n",
      "Critic loss: 1.324880599975586\n",
      "Actor loss: 54.852325439453125\n",
      "Critic loss: 5.219485282897949\n",
      "Actor loss: 57.83125686645508\n",
      "Critic loss: 2.1466665267944336\n",
      "Actor loss: 50.600730895996094\n",
      "Episode: 41\n",
      "Episode: 41, Step: 0\n",
      "Next Action: [-1.144\n",
      "Step reward: -11.779500552329864, Next State: [-0.\n",
      "Total episode reward: -11.779500552329864\n",
      "Episode: 41, Step: 1\n",
      "Next Action: [-1.127\n",
      "Step reward: -14.65938073030773, Next State: [-1. \n",
      "Total episode reward: -26.438881282637595\n",
      "Episode: 41, Step: 2\n",
      "Next Action: [-1.023\n",
      "Step reward: -15.376001513688594, Next State: [-1.\n",
      "Total episode reward: -41.81488279632619\n",
      "Episode: 41, Step: 3\n",
      "Next Action: [-1.184\n",
      "Step reward: -15.677308630709424, Next State: [-1.\n",
      "Total episode reward: -57.492191427035614\n",
      "Episode: 41, Step: 4\n",
      "Next Action: [-0.933\n",
      "Step reward: -15.869590074840085, Next State: [-1.\n",
      "Total episode reward: -73.3617815018757\n",
      "Episode: 41, Step: 5\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.931088698446125, Next State: [-1.\n",
      "Total episode reward: -89.29287020032183\n",
      "Episode: 41, Step: 6\n",
      "Next Action: [-0.744\n",
      "Step reward: -15.92871619633251, Next State: [-1. \n",
      "Total episode reward: -105.22158639665435\n",
      "Episode: 41, Step: 7\n",
      "Next Action: [-0.711\n",
      "Step reward: -15.936025062147218, Next State: [-1.\n",
      "Total episode reward: -121.15761145880157\n",
      "Episode: 41, Step: 8\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.941285124215536, Next State: [-1.\n",
      "Total episode reward: -137.0988965830171\n",
      "Episode: 41, Step: 9\n",
      "Next Action: [-0.898\n",
      "Step reward: -15.963238430951899, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.062135013969\n",
      "Episode: 41, Step: 10\n",
      "Next Action: [-1.074\n",
      "Step reward: -15.979784110078763, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.04191912404775\n",
      "Episode: 41, Step: 11\n",
      "Next Action: [-1.474\n",
      "Step reward: -15.990473177481865, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.03239230152963\n",
      "Episode: 41, Step: 12\n",
      "Next Action: [-1.043\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.03239230152963\n",
      "Episode: 41, Step: 13\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.995916875100628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.02830917663024\n",
      "Episode: 41, Step: 14\n",
      "Next Action: [-1.035\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.02830917663024\n",
      "Episode: 41, Step: 15\n",
      "Next Action: [-0.725\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.02830917663024\n",
      "Episode: 41, Step: 16\n",
      "Next Action: [-1.051\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.0283091766303\n",
      "Episode: 41, Step: 17\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.99873212059164, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.0270412972219\n",
      "Episode: 41, Step: 18\n",
      "Next Action: [-0.712\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.0270412972219\n",
      "Episode: 41, Step: 19\n",
      "Next Action: [-0.487\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.0270412972219\n",
      "Episode: 41, Step: 20\n",
      "Next Action: [-0.363\n",
      "Step reward: -15.984506133015104, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.011547430237\n",
      "Episode: 41, Step: 21\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.993575541262805, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.0051229714998\n",
      "Episode: 41, Step: 22\n",
      "Next Action: [-1.062\n",
      "Step reward: -15.996259498961868, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.0013824704617\n",
      "Episode: 41, Step: 23\n",
      "Next Action: [-1.210\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.0013824704617\n",
      "Episode: 41, Step: 24\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.996810657818065, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.99819312827975\n",
      "Episode: 41, Step: 25\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.980871260472949, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.9790643887527\n",
      "Episode: 41, Step: 26\n",
      "Next Action: [-0.684\n",
      "Step reward: -15.965674198847637, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.94473858760034\n",
      "Episode: 41, Step: 27\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.965767710010068, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.9105062976104\n",
      "Episode: 41, Step: 28\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.954252722669995, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.8647590202804\n",
      "Episode: 41, Step: 29\n",
      "Next Action: [-0.644\n",
      "Step reward: -15.959760133613644, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.8245191538941\n",
      "Episode: 41, Step: 30\n",
      "Next Action: [-0.447\n",
      "Step reward: -15.948372920955023, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.7728920748491\n",
      "Episode: 41, Step: 31\n",
      "Next Action: [-0.520\n",
      "Step reward: -15.938523784937146, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.71141585978626\n",
      "Episode: 41, Step: 32\n",
      "Next Action: [-0.753\n",
      "Step reward: -15.922451206101675, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.6338670658879\n",
      "Episode: 41, Step: 33\n",
      "Next Action: [-0.819\n",
      "Step reward: -15.933351014907183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.5672180807951\n",
      "Episode: 41, Step: 34\n",
      "Next Action: [-0.666\n",
      "Step reward: -15.950821142056247, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.5180392228514\n",
      "Episode: 41, Step: 35\n",
      "Next Action: [-0.595\n",
      "Step reward: -15.970352244845428, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.4883914676968\n",
      "Episode: 41, Step: 36\n",
      "Next Action: [-4.498\n",
      "Step reward: -15.973533965180595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.4619254328775\n",
      "Episode: 41, Step: 37\n",
      "Next Action: [-0.731\n",
      "Step reward: -15.975239574621162, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.4371650074986\n",
      "Episode: 41, Step: 38\n",
      "Next Action: [-0.669\n",
      "Step reward: -15.963809080433059, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.4009740879317\n",
      "Episode: 41, Step: 39\n",
      "Next Action: [-0.706\n",
      "Step reward: -15.960969317736724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.3619434056684\n",
      "Episode: 41, Step: 40\n",
      "Next Action: [-0.594\n",
      "Step reward: -15.963220686204647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.325164091873\n",
      "Episode: 41, Step: 41\n",
      "Next Action: [-0.436\n",
      "Step reward: -15.97352320181816, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.2986872936912\n",
      "Episode: 41, Step: 42\n",
      "Next Action: [-0.588\n",
      "Step reward: -15.984827236453114, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.2835145301443\n",
      "Episode: 41, Step: 43\n",
      "Next Action: [-0.663\n",
      "Step reward: -15.987405791314963, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.2709203214592\n",
      "Episode: 41, Step: 44\n",
      "Next Action: [-1.287\n",
      "Step reward: -15.999257957764797, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.2701782792241\n",
      "Episode: 41, Step: 45\n",
      "Next Action: [-1.198\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.2701782792241\n",
      "Episode: 41, Step: 46\n",
      "Next Action: [-1.154\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.2701782792241\n",
      "Episode: 41, Step: 47\n",
      "Next Action: [-0.647\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.2701782792241\n",
      "Episode: 41, Step: 48\n",
      "Next Action: [-0.989\n",
      "Step reward: -15.98766732179693, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.257845601021\n",
      "Episode: 41, Step: 49\n",
      "Next Action: [-0.964\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.257845601021\n",
      "Episode: 41, Step: 50\n",
      "Next Action: [-0.758\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.257845601021\n",
      "Episode: 41, Step: 51\n",
      "Next Action: [-0.765\n",
      "Step reward: -15.99821895698431, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.2560645580053\n",
      "Episode: 41, Step: 52\n",
      "Next Action: [-0.513\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.2560645580053\n",
      "Episode: 41, Step: 53\n",
      "Next Action: [-0.655\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.2560645580053\n",
      "Episode: 41, Step: 54\n",
      "Next Action: [-4.399\n",
      "Step reward: -15.978377311066849, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.2344418690722\n",
      "Episode: 41, Step: 55\n",
      "Next Action: [-0.418\n",
      "Step reward: -15.960973492813066, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.1954153618852\n",
      "Episode: 41, Step: 56\n",
      "Next Action: [-0.698\n",
      "Step reward: -15.957903095804236, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.1533184576895\n",
      "Episode: 41, Step: 57\n",
      "Next Action: [-0.582\n",
      "Step reward: -15.95792226916185, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.1112407268513\n",
      "Episode: 41, Step: 58\n",
      "Next Action: [-0.607\n",
      "Step reward: -15.93457448260386, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.0458152094551\n",
      "Episode: 41, Step: 59\n",
      "Next Action: [-0.540\n",
      "Step reward: -15.92331756454459, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.9691327739997\n",
      "Episode: 41, Step: 60\n",
      "Next Action: [-9.666\n",
      "Step reward: -15.92780284763393, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.8969356216337\n",
      "Episode: 41, Step: 61\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.958418645911415, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.8553542675451\n",
      "Episode: 41, Step: 62\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.968207713297865, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.8235619808429\n",
      "Episode: 41, Step: 63\n",
      "Next Action: [-1.013\n",
      "Step reward: -15.97621660023091, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.7997785810738\n",
      "Episode: 41, Step: 64\n",
      "Next Action: [-0.685\n",
      "Step reward: -15.986272429978964, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.786051011053\n",
      "Episode: 41, Step: 65\n",
      "Next Action: [-0.947\n",
      "Step reward: -15.995908931973696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.7819599430265\n",
      "Episode: 41, Step: 66\n",
      "Next Action: [-1.031\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.7819599430265\n",
      "Episode: 41, Step: 67\n",
      "Next Action: [-1.070\n",
      "Step reward: -15.994887941057746, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.7768478840842\n",
      "Episode: 41, Step: 68\n",
      "Next Action: [-1.065\n",
      "Step reward: -15.999214617544517, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.7760625016288\n",
      "Episode: 41, Step: 69\n",
      "Next Action: [-0.961\n",
      "Step reward: -15.998773691116183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.774836192745\n",
      "Episode: 41, Step: 70\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.992847665025428, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.7676838577704\n",
      "Episode: 41, Step: 71\n",
      "Next Action: [-0.437\n",
      "Step reward: -15.997535179303146, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1143.7652190370736\n",
      "Episode: 41, Step: 72\n",
      "Next Action: [-8.652\n",
      "Step reward: -15.990235362552696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.7554543996264\n",
      "Episode: 41, Step: 73\n",
      "Next Action: [-0.782\n",
      "Step reward: -15.99422186981532, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.7496762694418\n",
      "Episode: 41, Step: 74\n",
      "Next Action: [-0.757\n",
      "Step reward: -15.997005052017121, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.746681321459\n",
      "Episode: 41, Step: 75\n",
      "Next Action: [-0.511\n",
      "Step reward: -15.977129106875346, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.7238104283342\n",
      "Episode: 41, Step: 76\n",
      "Next Action: [-0.583\n",
      "Step reward: -15.965974268262338, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.6897846965965\n",
      "Episode: 41, Step: 77\n",
      "Next Action: [-0.609\n",
      "Step reward: -15.983574016001313, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.6733587125977\n",
      "Episode: 41, Step: 78\n",
      "Next Action: [-0.645\n",
      "Step reward: -15.988423064087508, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.6617817766853\n",
      "Episode: 41, Step: 79\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.992513969388215, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.6542957460736\n",
      "Episode: 41, Step: 80\n",
      "Next Action: [-1.268\n",
      "Step reward: -15.981473988300332, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.635769734374\n",
      "Episode: 41, Step: 81\n",
      "Next Action: [-1.330\n",
      "Step reward: -15.973088917854472, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.6088586522285\n",
      "Episode: 41, Step: 82\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.971374533711627, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.5802331859402\n",
      "Episode: 41, Step: 83\n",
      "Next Action: [-1.321\n",
      "Step reward: -15.956304277792718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.5365374637329\n",
      "Episode: 41, Step: 84\n",
      "Next Action: [-1.405\n",
      "Step reward: -15.966951339400556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.5034888031335\n",
      "Episode: 41, Step: 85\n",
      "Next Action: [-1.302\n",
      "Step reward: -15.968134867559375, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.471623670693\n",
      "Episode: 41, Step: 86\n",
      "Next Action: [-1.401\n",
      "Step reward: -15.982273662023353, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.4538973327162\n",
      "Episode: 41, Step: 87\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.98683228345626, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.4407296161726\n",
      "Episode: 41, Step: 88\n",
      "Next Action: [-1.069\n",
      "Step reward: -15.993985958352805, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.4347155745254\n",
      "Episode: 41, Step: 89\n",
      "Next Action: [-1.421\n",
      "Step reward: -15.977790582207021, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.4125061567324\n",
      "Episode: 41, Step: 90\n",
      "Next Action: [-1.499\n",
      "Step reward: -15.966782379223337, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.3792885359558\n",
      "Episode: 41, Step: 91\n",
      "Next Action: [-1.505\n",
      "Step reward: -15.964068347601495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.3433568835574\n",
      "Episode: 41, Step: 92\n",
      "Next Action: [-1.570\n",
      "Step reward: -15.951978112239836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.2953349957972\n",
      "Episode: 41, Step: 93\n",
      "Next Action: [-1.625\n",
      "Step reward: -15.95282497056956, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.2481599663668\n",
      "Episode: 41, Step: 94\n",
      "Next Action: [-1.411\n",
      "Step reward: -15.968902388074254, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.217062354441\n",
      "Episode: 41, Step: 95\n",
      "Next Action: [-1.701\n",
      "Step reward: -15.968746568770037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.185808923211\n",
      "Episode: 41, Step: 96\n",
      "Next Action: [-1.657\n",
      "Step reward: -15.972448371961912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.158257295173\n",
      "Episode: 41, Step: 97\n",
      "Next Action: [-1.577\n",
      "Step reward: -15.97415516665098, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.1324124618238\n",
      "Episode: 41, Step: 98\n",
      "Next Action: [-1.326\n",
      "Step reward: -15.991755740818444, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.1241682026423\n",
      "Episode: 41, Step: 99\n",
      "Next Action: [-1.351\n",
      "Step reward: -15.991363475294433, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.1155316779368\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.5498461723327637\n",
      "Actor loss: 43.93663024902344\n",
      "Critic loss: 21.267284393310547\n",
      "Actor loss: 39.719749450683594\n",
      "Critic loss: 16.415010452270508\n",
      "Actor loss: 52.775306701660156\n",
      "Critic loss: 193.07273864746094\n",
      "Actor loss: 37.8086051940918\n",
      "Critic loss: 1.997165560722351\n",
      "Actor loss: 28.935802459716797\n",
      "Critic loss: 13.040794372558594\n",
      "Actor loss: 25.96013832092285\n",
      "Critic loss: 13.72020149230957\n",
      "Actor loss: 38.750450134277344\n",
      "Critic loss: 54.28456115722656\n",
      "Actor loss: 24.559146881103516\n",
      "Critic loss: 32.346858978271484\n",
      "Actor loss: 26.43609619140625\n",
      "Critic loss: 21.314462661743164\n",
      "Actor loss: 38.336456298828125\n",
      "Episode: 42\n",
      "Episode: 42, Step: 0\n",
      "Next Action: [-1.566\n",
      "Step reward: -11.960599081065995, Next State: [-1.\n",
      "Total episode reward: -11.960599081065995\n",
      "Episode: 42, Step: 1\n",
      "Next Action: [-1.198\n",
      "Step reward: -14.800544465328759, Next State: [-1.\n",
      "Total episode reward: -26.761143546394756\n",
      "Episode: 42, Step: 2\n",
      "Next Action: [-1.058\n",
      "Step reward: -15.522932239326945, Next State: [-1.\n",
      "Total episode reward: -42.2840757857217\n",
      "Episode: 42, Step: 3\n",
      "Next Action: [-1.125\n",
      "Step reward: -15.825162280718354, Next State: [-1.\n",
      "Total episode reward: -58.10923806644005\n",
      "Episode: 42, Step: 4\n",
      "Next Action: [-1.194\n",
      "Step reward: -15.911739698690734, Next State: [-1.\n",
      "Total episode reward: -74.02097776513078\n",
      "Episode: 42, Step: 5\n",
      "Next Action: [-1.552\n",
      "Step reward: -15.913585141488518, Next State: [-1.\n",
      "Total episode reward: -89.9345629066193\n",
      "Episode: 42, Step: 6\n",
      "Next Action: [-1.439\n",
      "Step reward: -15.92232074827422, Next State: [-1. \n",
      "Total episode reward: -105.85688365489352\n",
      "Episode: 42, Step: 7\n",
      "Next Action: [-1.503\n",
      "Step reward: -15.956016047285573, Next State: [-1.\n",
      "Total episode reward: -121.8128997021791\n",
      "Episode: 42, Step: 8\n",
      "Next Action: [-1.470\n",
      "Step reward: -15.97040698238972, Next State: [-1. \n",
      "Total episode reward: -137.78330668456883\n",
      "Episode: 42, Step: 9\n",
      "Next Action: [-1.310\n",
      "Step reward: -15.999963171193533, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.78326985576237\n",
      "Episode: 42, Step: 10\n",
      "Next Action: [-1.261\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.78326985576237\n",
      "Episode: 42, Step: 11\n",
      "Next Action: [-0.983\n",
      "Step reward: -15.99539081526243, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.7786606710248\n",
      "Episode: 42, Step: 12\n",
      "Next Action: [-1.134\n",
      "Step reward: -15.981343430099853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.76000410112465\n",
      "Episode: 42, Step: 13\n",
      "Next Action: [-1.073\n",
      "Step reward: -15.992619816417223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.75262391754188\n",
      "Episode: 42, Step: 14\n",
      "Next Action: [-0.995\n",
      "Step reward: -15.995588005512625, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.7482119230545\n",
      "Episode: 42, Step: 15\n",
      "Next Action: [-1.093\n",
      "Step reward: -15.968270078677918, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.7164820017324\n",
      "Episode: 42, Step: 16\n",
      "Next Action: [-1.116\n",
      "Step reward: -15.96039595819329, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.6768779599257\n",
      "Episode: 42, Step: 17\n",
      "Next Action: [-1.361\n",
      "Step reward: -15.957132040917758, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.6340100008435\n",
      "Episode: 42, Step: 18\n",
      "Next Action: [-1.285\n",
      "Step reward: -15.96020140924086, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.59421141008437\n",
      "Episode: 42, Step: 19\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.958818959853682, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.553030369938\n",
      "Episode: 42, Step: 20\n",
      "Next Action: [-0.973\n",
      "Step reward: -15.988225512334658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.5412558822727\n",
      "Episode: 42, Step: 21\n",
      "Next Action: [-0.781\n",
      "Step reward: -15.998359043256587, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.5396149255293\n",
      "Episode: 42, Step: 22\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.996357875439019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.5359728009683\n",
      "Episode: 42, Step: 23\n",
      "Next Action: [-1.193\n",
      "Step reward: -15.985797775101112, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.5217705760694\n",
      "Episode: 42, Step: 24\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.920639188290904, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.4424097643603\n",
      "Episode: 42, Step: 25\n",
      "Next Action: [-0.590\n",
      "Step reward: -15.932900257613388, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.3753100219737\n",
      "Episode: 42, Step: 26\n",
      "Next Action: [-0.603\n",
      "Step reward: -15.958600446798942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.3339104687726\n",
      "Episode: 42, Step: 27\n",
      "Next Action: [-0.957\n",
      "Step reward: -15.951534584832928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.28544505360554\n",
      "Episode: 42, Step: 28\n",
      "Next Action: [-1.129\n",
      "Step reward: -15.948813291550723, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.23425834515626\n",
      "Episode: 42, Step: 29\n",
      "Next Action: [-1.306\n",
      "Step reward: -15.958343827690056, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.1926021728463\n",
      "Episode: 42, Step: 30\n",
      "Next Action: [-1.576\n",
      "Step reward: -15.965973284221183, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.1585754570675\n",
      "Episode: 42, Step: 31\n",
      "Next Action: [-1.505\n",
      "Step reward: -15.97949261729276, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.13806807436026\n",
      "Episode: 42, Step: 32\n",
      "Next Action: [-1.390\n",
      "Step reward: -15.965486339604825, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.1035544139651\n",
      "Episode: 42, Step: 33\n",
      "Next Action: [-1.431\n",
      "Step reward: -15.936323864828426, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.0398782787935\n",
      "Episode: 42, Step: 34\n",
      "Next Action: [-1.434\n",
      "Step reward: -15.926028619343487, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.965906898137\n",
      "Episode: 42, Step: 35\n",
      "Next Action: [-1.449\n",
      "Step reward: -15.927247114952209, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.8931540130892\n",
      "Episode: 42, Step: 36\n",
      "Next Action: [-1.580\n",
      "Step reward: -15.942035976399161, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.8351899894884\n",
      "Episode: 42, Step: 37\n",
      "Next Action: [-1.116\n",
      "Step reward: -15.939966223670316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.7751562131588\n",
      "Episode: 42, Step: 38\n",
      "Next Action: [-1.045\n",
      "Step reward: -15.922090872579123, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.6972470857379\n",
      "Episode: 42, Step: 39\n",
      "Next Action: [-1.196\n",
      "Step reward: -15.921750671059703, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.6189977567976\n",
      "Episode: 42, Step: 40\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.953240234217539, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.5722379910152\n",
      "Episode: 42, Step: 41\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.967977421002997, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.5402154120181\n",
      "Episode: 42, Step: 42\n",
      "Next Action: [-1.616\n",
      "Step reward: -15.967192587842085, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.5074079998602\n",
      "Episode: 42, Step: 43\n",
      "Next Action: [-1.254\n",
      "Step reward: -15.961063414564281, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.4684714144245\n",
      "Episode: 42, Step: 44\n",
      "Next Action: [-1.197\n",
      "Step reward: -15.955537613082793, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.4240090275073\n",
      "Episode: 42, Step: 45\n",
      "Next Action: [-0.932\n",
      "Step reward: -15.966506864593505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.3905158921008\n",
      "Episode: 42, Step: 46\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.968034804425765, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.3585506965265\n",
      "Episode: 42, Step: 47\n",
      "Next Action: [-0.849\n",
      "Step reward: -15.968726382118119, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.3272770786447\n",
      "Episode: 42, Step: 48\n",
      "Next Action: [-0.787\n",
      "Step reward: -15.96878707110402, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.2960641497486\n",
      "Episode: 42, Step: 49\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.965656133470272, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.2617202832189\n",
      "Episode: 42, Step: 50\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.958931188132883, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.2206514713517\n",
      "Episode: 42, Step: 51\n",
      "Next Action: [-1.302\n",
      "Step reward: -15.940180982054114, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.1608324534059\n",
      "Episode: 42, Step: 52\n",
      "Next Action: [-1.228\n",
      "Step reward: -15.943127661234316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.1039601146402\n",
      "Episode: 42, Step: 53\n",
      "Next Action: [-1.144\n",
      "Step reward: -15.954084464789075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.0580445794293\n",
      "Episode: 42, Step: 54\n",
      "Next Action: [-1.173\n",
      "Step reward: -15.95339397711517, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.0114385565445\n",
      "Episode: 42, Step: 55\n",
      "Next Action: [-1.586\n",
      "Step reward: -15.937005740639856, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -887.9484442971843\n",
      "Episode: 42, Step: 56\n",
      "Next Action: [-1.417\n",
      "Step reward: -15.959095455927518, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -903.9075397531118\n",
      "Episode: 42, Step: 57\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.962725927649556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -919.8702656807613\n",
      "Episode: 42, Step: 58\n",
      "Next Action: [-0.920\n",
      "Step reward: -15.943336016225706, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -935.813601696987\n",
      "Episode: 42, Step: 59\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.930409554756531, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -951.7440112517436\n",
      "Episode: 42, Step: 60\n",
      "Next Action: [-0.805\n",
      "Step reward: -15.907439817312781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -967.6514510690564\n",
      "Episode: 42, Step: 61\n",
      "Next Action: [-0.891\n",
      "Step reward: -15.928730147978875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -983.5801812170353\n",
      "Episode: 42, Step: 62\n",
      "Next Action: [-0.694\n",
      "Step reward: -15.958063390972686, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -999.538244608008\n",
      "Episode: 42, Step: 63\n",
      "Next Action: [-0.343\n",
      "Step reward: -15.971448425808653, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1015.5096930338166\n",
      "Episode: 42, Step: 64\n",
      "Next Action: [-0.627\n",
      "Step reward: -15.973224142703105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1031.4829171765198\n",
      "Episode: 42, Step: 65\n",
      "Next Action: [-0.784\n",
      "Step reward: -15.959928178644379, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1047.442845355164\n",
      "Episode: 42, Step: 66\n",
      "Next Action: [-0.929\n",
      "Step reward: -15.949890976060525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1063.3927363312246\n",
      "Episode: 42, Step: 67\n",
      "Next Action: [-1.009\n",
      "Step reward: -15.954425025872442, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1079.347161357097\n",
      "Episode: 42, Step: 68\n",
      "Next Action: [-1.002\n",
      "Step reward: -15.971348915961443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1095.3185102730586\n",
      "Episode: 42, Step: 69\n",
      "Next Action: [-0.653\n",
      "Step reward: -15.973111436587212, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1111.2916217096458\n",
      "Episode: 42, Step: 70\n",
      "Next Action: [-0.714\n",
      "Step reward: -15.985807999974215, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1127.27742970962\n",
      "Episode: 42, Step: 71\n",
      "Next Action: [-0.821\n",
      "Step reward: -15.98505978626927, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1143.2624894958892\n",
      "Episode: 42, Step: 72\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.980585137925559, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1159.2430746338148\n",
      "Episode: 42, Step: 73\n",
      "Next Action: [-0.764\n",
      "Step reward: -15.97074236578668, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1175.2138169996015\n",
      "Episode: 42, Step: 74\n",
      "Next Action: [-0.916\n",
      "Step reward: -15.968737160680877, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.1825541602823\n",
      "Episode: 42, Step: 75\n",
      "Next Action: [-0.829\n",
      "Step reward: -15.969210790772852, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.1517649510552\n",
      "Episode: 42, Step: 76\n",
      "Next Action: [-0.699\n",
      "Step reward: -15.970461186822842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.122226137878\n",
      "Episode: 42, Step: 77\n",
      "Next Action: [-1.144\n",
      "Step reward: -15.974531115047208, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.0967572529253\n",
      "Episode: 42, Step: 78\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.968984590850551, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.0657418437759\n",
      "Episode: 42, Step: 79\n",
      "Next Action: [-1.341\n",
      "Step reward: -15.968409991114083, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.03415183489\n",
      "Episode: 42, Step: 80\n",
      "Next Action: [-1.469\n",
      "Step reward: -15.958223234735398, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.9923750696255\n",
      "Episode: 42, Step: 81\n",
      "Next Action: [-1.446\n",
      "Step reward: -15.961749834188113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.9541249038136\n",
      "Episode: 42, Step: 82\n",
      "Next Action: [-1.193\n",
      "Step reward: -15.957453338013606, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1318.9115782418273\n",
      "Episode: 42, Step: 83\n",
      "Next Action: [-1.304\n",
      "Step reward: -15.983545888201862, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1334.8951241300292\n",
      "Episode: 42, Step: 84\n",
      "Next Action: [-1.403\n",
      "Step reward: -15.970272819458293, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1350.8653969494876\n",
      "Episode: 42, Step: 85\n",
      "Next Action: [-1.702\n",
      "Step reward: -15.958633954141979, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1366.8240309036296\n",
      "Episode: 42, Step: 86\n",
      "Next Action: [-1.365\n",
      "Step reward: -15.96157412723578, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1382.7856050308653\n",
      "Episode: 42, Step: 87\n",
      "Next Action: [-1.261\n",
      "Step reward: -15.969870442185591, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1398.7554754730509\n",
      "Episode: 42, Step: 88\n",
      "Next Action: [-1.165\n",
      "Step reward: -15.982854846462633, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1414.7383303195136\n",
      "Episode: 42, Step: 89\n",
      "Next Action: [-1.137\n",
      "Step reward: -15.97160745310086, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1430.7099377726145\n",
      "Episode: 42, Step: 90\n",
      "Next Action: [-1.026\n",
      "Step reward: -15.966179348968124, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1446.6761171215826\n",
      "Episode: 42, Step: 91\n",
      "Next Action: [-1.047\n",
      "Step reward: -15.969410784206246, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1462.6455279057889\n",
      "Episode: 42, Step: 92\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.95100673236078, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1478.5965346381497\n",
      "Episode: 42, Step: 93\n",
      "Next Action: [-0.771\n",
      "Step reward: -15.964878685327713, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1494.5614133234774\n",
      "Episode: 42, Step: 94\n",
      "Next Action: [-0.818\n",
      "Step reward: -15.964929902222115, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1510.5263432256995\n",
      "Episode: 42, Step: 95\n",
      "Next Action: [-1.109\n",
      "Step reward: -15.963951893691455, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1526.490295119391\n",
      "Episode: 42, Step: 96\n",
      "Next Action: [-1.110\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1542.490295119391\n",
      "Episode: 42, Step: 97\n",
      "Next Action: [-0.821\n",
      "Step reward: -15.984640804166263, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1558.4749359235573\n",
      "Episode: 42, Step: 98\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.984923936222454, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1574.4598598597797\n",
      "Episode: 42, Step: 99\n",
      "Next Action: [-0.970\n",
      "Step reward: -15.959393410246014, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1590.4192532700258\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 4.558987617492676\n",
      "Actor loss: 26.485881805419922\n",
      "Critic loss: 41.15492630004883\n",
      "Actor loss: 22.562667846679688\n",
      "Critic loss: 16.136919021606445\n",
      "Actor loss: 42.44181442260742\n",
      "Critic loss: 16.288537979125977\n",
      "Actor loss: 38.390838623046875\n",
      "Critic loss: 19.782514572143555\n",
      "Actor loss: 38.12080764770508\n",
      "Critic loss: 1.9286904335021973\n",
      "Actor loss: 48.838130950927734\n",
      "Critic loss: 5.405735492706299\n",
      "Actor loss: 46.33595657348633\n",
      "Critic loss: 16.892032623291016\n",
      "Actor loss: 42.855865478515625\n",
      "Critic loss: 6.730700492858887\n",
      "Actor loss: 64.52446746826172\n",
      "Critic loss: 2.4645540714263916\n",
      "Actor loss: 36.457733154296875\n",
      "Episode: 43\n",
      "Episode: 43, Step: 0\n",
      "Next Action: [-1.092\n",
      "Step reward: -11.759982664557757, Next State: [-1.\n",
      "Total episode reward: -11.759982664557757\n",
      "Episode: 43, Step: 1\n",
      "Next Action: [-1.267\n",
      "Step reward: -15.007001554623027, Next State: [-1.\n",
      "Total episode reward: -26.766984219180785\n",
      "Episode: 43, Step: 2\n",
      "Next Action: [-1.534\n",
      "Step reward: -15.654308382454428, Next State: [-1.\n",
      "Total episode reward: -42.421292601635216\n",
      "Episode: 43, Step: 3\n",
      "Next Action: [-1.693\n",
      "Step reward: -15.798515900102396, Next State: [-1.\n",
      "Total episode reward: -58.21980850173761\n",
      "Episode: 43, Step: 4\n",
      "Next Action: [-1.554\n",
      "Step reward: -15.818446859370448, Next State: [-1.\n",
      "Total episode reward: -74.03825536110806\n",
      "Episode: 43, Step: 5\n",
      "Next Action: [-1.290\n",
      "Step reward: -15.883065460246126, Next State: [-1.\n",
      "Total episode reward: -89.92132082135419\n",
      "Episode: 43, Step: 6\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.916210025410338, Next State: [-1.\n",
      "Total episode reward: -105.83753084676452\n",
      "Episode: 43, Step: 7\n",
      "Next Action: [-0.993\n",
      "Step reward: -15.938457449072612, Next State: [-1.\n",
      "Total episode reward: -121.77598829583714\n",
      "Episode: 43, Step: 8\n",
      "Next Action: [-0.611\n",
      "Step reward: -15.979196669003223, Next State: [-1.\n",
      "Total episode reward: -137.75518496484037\n",
      "Episode: 43, Step: 9\n",
      "Next Action: [-0.620\n",
      "Step reward: -15.993326149134646, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.748511113975\n",
      "Episode: 43, Step: 10\n",
      "Next Action: [-0.909\n",
      "Step reward: -15.979160209391726, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.72767132336674\n",
      "Episode: 43, Step: 11\n",
      "Next Action: [-1.017\n",
      "Step reward: -15.95318964642217, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.6808609697889\n",
      "Episode: 43, Step: 12\n",
      "Next Action: [-1.103\n",
      "Step reward: -15.946392113228208, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.6272530830171\n",
      "Episode: 43, Step: 13\n",
      "Next Action: [-1.336\n",
      "Step reward: -15.975092495470932, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.60234557848804\n",
      "Episode: 43, Step: 14\n",
      "Next Action: [-1.196\n",
      "Step reward: -15.990656799006505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.59300237749454\n",
      "Episode: 43, Step: 15\n",
      "Next Action: [-1.094\n",
      "Step reward: -15.99439587673574, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.5873982542303\n",
      "Episode: 43, Step: 16\n",
      "Next Action: [-1.042\n",
      "Step reward: -15.985827962785583, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.5732262170159\n",
      "Episode: 43, Step: 17\n",
      "Next Action: [-0.948\n",
      "Step reward: -15.970596742915781, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.5438229599317\n",
      "Episode: 43, Step: 18\n",
      "Next Action: [-1.069\n",
      "Step reward: -15.982665231834186, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.52648819176585\n",
      "Episode: 43, Step: 19\n",
      "Next Action: [-1.037\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.52648819176585\n",
      "Episode: 43, Step: 20\n",
      "Next Action: [-1.011\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.52648819176585\n",
      "Episode: 43, Step: 21\n",
      "Next Action: [-1.269\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.52648819176585\n",
      "Episode: 43, Step: 22\n",
      "Next Action: [-1.327\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.52648819176585\n",
      "Episode: 43, Step: 23\n",
      "Next Action: [-0.989\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.52648819176585\n",
      "Episode: 43, Step: 24\n",
      "Next Action: [-1.038\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.52648819176585\n",
      "Episode: 43, Step: 25\n",
      "Next Action: [-1.280\n",
      "Step reward: -15.998716487617884, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.5252046793837\n",
      "Episode: 43, Step: 26\n",
      "Next Action: [-1.259\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.5252046793837\n",
      "Episode: 43, Step: 27\n",
      "Next Action: [-1.140\n",
      "Step reward: -15.996998413083839, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.5222030924675\n",
      "Episode: 43, Step: 28\n",
      "Next Action: [-1.061\n",
      "Step reward: -15.990550216405378, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.5127533088729\n",
      "Episode: 43, Step: 29\n",
      "Next Action: [-1.320\n",
      "Step reward: -15.988503648764345, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.5012569576372\n",
      "Episode: 43, Step: 30\n",
      "Next Action: [-1.187\n",
      "Step reward: -15.990745911678042, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.4920028693153\n",
      "Episode: 43, Step: 31\n",
      "Next Action: [-1.208\n",
      "Step reward: -15.976560568976351, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.46856343829165\n",
      "Episode: 43, Step: 32\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.973186921957097, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.4417503602488\n",
      "Episode: 43, Step: 33\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.965381060940981, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.4071314211898\n",
      "Episode: 43, Step: 34\n",
      "Next Action: [-1.118\n",
      "Step reward: -15.969375823640704, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.3765072448305\n",
      "Episode: 43, Step: 35\n",
      "Next Action: [-1.422\n",
      "Step reward: -15.964341176245775, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.3408484210762\n",
      "Episode: 43, Step: 36\n",
      "Next Action: [-1.480\n",
      "Step reward: -15.948522051636798, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.289370472713\n",
      "Episode: 43, Step: 37\n",
      "Next Action: [-1.273\n",
      "Step reward: -15.943430169438747, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.2328006421518\n",
      "Episode: 43, Step: 38\n",
      "Next Action: [-1.323\n",
      "Step reward: -15.947112782793837, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.1799134249457\n",
      "Episode: 43, Step: 39\n",
      "Next Action: [-1.351\n",
      "Step reward: -15.975188388203842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.1551018131495\n",
      "Episode: 43, Step: 40\n",
      "Next Action: [-1.260\n",
      "Step reward: -15.97799432657292, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.1330961397224\n",
      "Episode: 43, Step: 41\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.979706273722861, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.1128024134453\n",
      "Episode: 43, Step: 42\n",
      "Next Action: [-1.170\n",
      "Step reward: -15.988804655910673, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.101607069356\n",
      "Episode: 43, Step: 43\n",
      "Next Action: [-1.098\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.101607069356\n",
      "Episode: 43, Step: 44\n",
      "Next Action: [-0.876\n",
      "Step reward: -15.999098126356614, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.1007051957126\n",
      "Episode: 43, Step: 45\n",
      "Next Action: [-0.902\n",
      "Step reward: -15.993813712933877, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.0945189086465\n",
      "Episode: 43, Step: 46\n",
      "Next Action: [-0.819\n",
      "Step reward: -15.976676190722987, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.0711950993694\n",
      "Episode: 43, Step: 47\n",
      "Next Action: [-0.694\n",
      "Step reward: -15.949270899877218, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.0204659992467\n",
      "Episode: 43, Step: 48\n",
      "Next Action: [-0.793\n",
      "Step reward: -15.938550912117446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.9590169113641\n",
      "Episode: 43, Step: 49\n",
      "Next Action: [-1.052\n",
      "Step reward: -15.955719670430682, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.9147365817947\n",
      "Episode: 43, Step: 50\n",
      "Next Action: [-0.980\n",
      "Step reward: -15.949110665188384, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.8638472469831\n",
      "Episode: 43, Step: 51\n",
      "Next Action: [-1.259\n",
      "Step reward: -15.957092006382503, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.8209392533656\n",
      "Episode: 43, Step: 52\n",
      "Next Action: [-1.148\n",
      "Step reward: -15.962851151622473, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.783790404988\n",
      "Episode: 43, Step: 53\n",
      "Next Action: [-1.352\n",
      "Step reward: -15.960482103095172, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.7442725080832\n",
      "Episode: 43, Step: 54\n",
      "Next Action: [-1.289\n",
      "Step reward: -15.963883783886462, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.7081562919697\n",
      "Episode: 43, Step: 55\n",
      "Next Action: [-1.278\n",
      "Step reward: -15.98134864817115, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.6895049401408\n",
      "Episode: 43, Step: 56\n",
      "Next Action: [-1.200\n",
      "Step reward: -15.971026956212736, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.6605318963535\n",
      "Episode: 43, Step: 57\n",
      "Next Action: [-1.172\n",
      "Step reward: -15.999778172892086, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.6603100692456\n",
      "Episode: 43, Step: 58\n",
      "Next Action: [-1.038\n",
      "Step reward: -15.984490211972716, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.6448002812183\n",
      "Episode: 43, Step: 59\n",
      "Next Action: [-1.271\n",
      "Step reward: -15.969641730661113, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.6144420118794\n",
      "Episode: 43, Step: 60\n",
      "Next Action: [-1.226\n",
      "Step reward: -15.950094018986416, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.5645360308658\n",
      "Episode: 43, Step: 61\n",
      "Next Action: [-1.066\n",
      "Step reward: -15.951921546487888, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.5164575773538\n",
      "Episode: 43, Step: 62\n",
      "Next Action: [-1.072\n",
      "Step reward: -15.948505930271288, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.464963507625\n",
      "Episode: 43, Step: 63\n",
      "Next Action: [-1.250\n",
      "Step reward: -15.951427090419735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.4163905980448\n",
      "Episode: 43, Step: 64\n",
      "Next Action: [-1.350\n",
      "Step reward: -15.965341007470485, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.3817316055154\n",
      "Episode: 43, Step: 65\n",
      "Next Action: [-1.070\n",
      "Step reward: -15.950383679076833, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.3321152845922\n",
      "Episode: 43, Step: 66\n",
      "Next Action: [-0.522\n",
      "Step reward: -15.9656646370283, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.2977799216205\n",
      "Episode: 43, Step: 67\n",
      "Next Action: [-0.834\n",
      "Step reward: -15.95941364939916, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.2571935710198\n",
      "Episode: 43, Step: 68\n",
      "Next Action: [-0.306\n",
      "Step reward: -15.9669751400506, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.2241687110704\n",
      "Episode: 43, Step: 69\n",
      "Next Action: [-0.464\n",
      "Step reward: -15.961599280946059, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.1857679920165\n",
      "Episode: 43, Step: 70\n",
      "Next Action: [-0.104\n",
      "Step reward: -15.97269850129423, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.1584664933107\n",
      "Episode: 43, Step: 71\n",
      "Next Action: [ 0.049\n",
      "Step reward: -15.96864518965742, Next State: [-0.9\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.127111682968\n",
      "Episode: 43, Step: 72\n",
      "Next Action: [ 0.246\n",
      "Step reward: -15.953623544538097, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.080735227506\n",
      "Episode: 43, Step: 73\n",
      "Next Action: [ 0.216\n",
      "Step reward: -15.941598730488725, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.0223339579948\n",
      "Episode: 43, Step: 74\n",
      "Next Action: [ 0.215\n",
      "Step reward: -15.93543992198095, Next State: [-0.2\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1191.9577738799758\n",
      "Episode: 43, Step: 75\n",
      "Next Action: [ 0.379\n",
      "Step reward: -15.936021782817773, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1207.8937956627935\n",
      "Episode: 43, Step: 76\n",
      "Next Action: [ 0.422\n",
      "Step reward: -15.93840933479329, Next State: [ 0.5\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1223.8322049975868\n",
      "Episode: 43, Step: 77\n",
      "Next Action: [ 0.422\n",
      "Step reward: -15.979028451967832, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1239.8112334495547\n",
      "Episode: 43, Step: 78\n",
      "Next Action: [ 0.223\n",
      "Step reward: -15.968410693341513, Next State: [ 1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1255.7796441428961\n",
      "Episode: 43, Step: 79\n",
      "Next Action: [ 0.229\n",
      "Step reward: -15.97160587070677, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1271.7512500136029\n",
      "Episode: 43, Step: 80\n",
      "Next Action: [ 0.015\n",
      "Step reward: -15.97164485443493, Next State: [ 1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1287.7228948680379\n",
      "Episode: 43, Step: 81\n",
      "Next Action: [-0.188\n",
      "Step reward: -15.9653054517362, Next State: [ 0.81\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1303.688200319774\n",
      "Episode: 43, Step: 82\n",
      "Next Action: [-0.334\n",
      "Step reward: -15.955614128937215, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1319.6438144487113\n",
      "Episode: 43, Step: 83\n",
      "Next Action: [-0.266\n",
      "Step reward: -15.946437124640891, Next State: [ 0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1335.5902515733521\n",
      "Episode: 43, Step: 84\n",
      "Next Action: [-0.542\n",
      "Step reward: -15.9408771343209, Next State: [-0.33\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1351.5311287076731\n",
      "Episode: 43, Step: 85\n",
      "Next Action: [-0.851\n",
      "Step reward: -15.969496544029337, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1367.5006252517026\n",
      "Episode: 43, Step: 86\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.9951065556726, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1383.4957318073753\n",
      "Episode: 43, Step: 87\n",
      "Next Action: [-0.822\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.4957318073753\n",
      "Episode: 43, Step: 88\n",
      "Next Action: [-0.902\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.4957318073753\n",
      "Episode: 43, Step: 89\n",
      "Next Action: [-0.601\n",
      "Step reward: -15.994324171067538, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.4900559784428\n",
      "Episode: 43, Step: 90\n",
      "Next Action: [-7.489\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.4900559784428\n",
      "Episode: 43, Step: 91\n",
      "Next Action: [-0.792\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.4900559784428\n",
      "Episode: 43, Step: 92\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.998439831215496, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.4884958096584\n",
      "Episode: 43, Step: 93\n",
      "Next Action: [-1.134\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.4884958096584\n",
      "Episode: 43, Step: 94\n",
      "Next Action: [-1.297\n",
      "Step reward: -15.993524766974728, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.4820205766332\n",
      "Episode: 43, Step: 95\n",
      "Next Action: [-1.424\n",
      "Step reward: -15.986424153196776, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.46844472983\n",
      "Episode: 43, Step: 96\n",
      "Next Action: [-1.694\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.46844472983\n",
      "Episode: 43, Step: 97\n",
      "Next Action: [-1.414\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.46844472983\n",
      "Episode: 43, Step: 98\n",
      "Next Action: [-1.462\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.46844472983\n",
      "Episode: 43, Step: 99\n",
      "Next Action: [-1.629\n",
      "Step reward: -15.985399701580944, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.453844431411\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 3.5625813007354736\n",
      "Actor loss: 61.18169021606445\n",
      "Critic loss: 5.677947998046875\n",
      "Actor loss: 65.04425048828125\n",
      "Critic loss: 3.335198402404785\n",
      "Actor loss: 40.24713897705078\n",
      "Critic loss: 58.74299621582031\n",
      "Actor loss: 68.59529113769531\n",
      "Critic loss: 28.595067977905273\n",
      "Actor loss: 61.334373474121094\n",
      "Critic loss: 3.5305378437042236\n",
      "Actor loss: 54.47833251953125\n",
      "Critic loss: 3.3131256103515625\n",
      "Actor loss: 31.61692237854004\n",
      "Critic loss: 5.327003002166748\n",
      "Actor loss: 44.71794891357422\n",
      "Critic loss: 7.147469520568848\n",
      "Actor loss: 45.16432571411133\n",
      "Critic loss: 3.237126350402832\n",
      "Actor loss: 43.2794303894043\n",
      "Episode: 44\n",
      "Episode: 44, Step: 0\n",
      "Next Action: [-1.594\n",
      "Step reward: -11.076036376669567, Next State: [-4.\n",
      "Total episode reward: -11.076036376669567\n",
      "Episode: 44, Step: 1\n",
      "Next Action: [-1.453\n",
      "Step reward: -14.630453416250093, Next State: [-1.\n",
      "Total episode reward: -25.706489792919662\n",
      "Episode: 44, Step: 2\n",
      "Next Action: [-1.320\n",
      "Step reward: -15.549193324586174, Next State: [-1.\n",
      "Total episode reward: -41.25568311750584\n",
      "Episode: 44, Step: 3\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.780512699047186, Next State: [-1.\n",
      "Total episode reward: -57.036195816553025\n",
      "Episode: 44, Step: 4\n",
      "Next Action: [-0.802\n",
      "Step reward: -15.843463689170209, Next State: [-1.\n",
      "Total episode reward: -72.87965950572324\n",
      "Episode: 44, Step: 5\n",
      "Next Action: [-0.875\n",
      "Step reward: -15.885529323741164, Next State: [-1.\n",
      "Total episode reward: -88.7651888294644\n",
      "Episode: 44, Step: 6\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.92983580474772, Next State: [-1. \n",
      "Total episode reward: -104.69502463421213\n",
      "Episode: 44, Step: 7\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.951977186295483, Next State: [-1.\n",
      "Total episode reward: -120.64700182050761\n",
      "Episode: 44, Step: 8\n",
      "Next Action: [-1.558\n",
      "Step reward: -15.929499779517384, Next State: [-1.\n",
      "Total episode reward: -136.57650160002498\n",
      "Episode: 44, Step: 9\n",
      "Next Action: [-1.519\n",
      "Step reward: -15.936735239077295, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -152.5132368391023\n",
      "Episode: 44, Step: 10\n",
      "Next Action: [-1.199\n",
      "Step reward: -15.942009006700802, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -168.4552458458031\n",
      "Episode: 44, Step: 11\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.967283640239593, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -184.4225294860427\n",
      "Episode: 44, Step: 12\n",
      "Next Action: [-0.978\n",
      "Step reward: -15.963599753030273, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -200.38612923907297\n",
      "Episode: 44, Step: 13\n",
      "Next Action: [-1.068\n",
      "Step reward: -15.969153549418799, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -216.35528278849176\n",
      "Episode: 44, Step: 14\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.97594555829734, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -232.3312283467891\n",
      "Episode: 44, Step: 15\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.960757550531168, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -248.29198589732025\n",
      "Episode: 44, Step: 16\n",
      "Next Action: [-0.695\n",
      "Step reward: -15.968708880954742, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -264.26069477827497\n",
      "Episode: 44, Step: 17\n",
      "Next Action: [-0.344\n",
      "Step reward: -15.974132455362561, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -280.23482723363753\n",
      "Episode: 44, Step: 18\n",
      "Next Action: [-0.111\n",
      "Step reward: -15.980410607343945, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -296.2152378409815\n",
      "Episode: 44, Step: 19\n",
      "Next Action: [-0.411\n",
      "Step reward: -15.99146804544873, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -312.20670588643026\n",
      "Episode: 44, Step: 20\n",
      "Next Action: [-0.738\n",
      "Step reward: -15.989452994463493, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -328.19615888089373\n",
      "Episode: 44, Step: 21\n",
      "Next Action: [-0.483\n",
      "Step reward: -15.974046171373601, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -344.1702050522673\n",
      "Episode: 44, Step: 22\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.968936780716282, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -360.13914183298357\n",
      "Episode: 44, Step: 23\n",
      "Next Action: [-0.575\n",
      "Step reward: -15.973083192235432, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -376.112225025219\n",
      "Episode: 44, Step: 24\n",
      "Next Action: [-0.709\n",
      "Step reward: -15.965355171110753, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -392.07758019632973\n",
      "Episode: 44, Step: 25\n",
      "Next Action: [-0.751\n",
      "Step reward: -15.954858969417648, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -408.0324391657474\n",
      "Episode: 44, Step: 26\n",
      "Next Action: [-0.574\n",
      "Step reward: -15.916461358030123, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -423.9489005237775\n",
      "Episode: 44, Step: 27\n",
      "Next Action: [-0.669\n",
      "Step reward: -15.918704150575145, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -439.86760467435266\n",
      "Episode: 44, Step: 28\n",
      "Next Action: [-0.560\n",
      "Step reward: -15.946593961528523, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -455.8141986358812\n",
      "Episode: 44, Step: 29\n",
      "Next Action: [-0.637\n",
      "Step reward: -15.964156673412734, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -471.77835530929394\n",
      "Episode: 44, Step: 30\n",
      "Next Action: [-0.469\n",
      "Step reward: -15.952763304431524, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -487.73111861372547\n",
      "Episode: 44, Step: 31\n",
      "Next Action: [-0.749\n",
      "Step reward: -15.93295349908121, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -503.6640721128067\n",
      "Episode: 44, Step: 32\n",
      "Next Action: [-0.633\n",
      "Step reward: -15.937388561761143, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -519.6014606745679\n",
      "Episode: 44, Step: 33\n",
      "Next Action: [-0.540\n",
      "Step reward: -15.958675123296358, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -535.5601357978642\n",
      "Episode: 44, Step: 34\n",
      "Next Action: [-0.843\n",
      "Step reward: -15.927429361858412, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -551.4875651597226\n",
      "Episode: 44, Step: 35\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.929261773428056, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -567.4168269331507\n",
      "Episode: 44, Step: 36\n",
      "Next Action: [-0.752\n",
      "Step reward: -15.92353031435548, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -583.3403572475062\n",
      "Episode: 44, Step: 37\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.914281058307372, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -599.2546383058135\n",
      "Episode: 44, Step: 38\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.926701484594334, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -615.1813397904078\n",
      "Episode: 44, Step: 39\n",
      "Next Action: [-1.149\n",
      "Step reward: -15.949783899884437, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -631.1311236902923\n",
      "Episode: 44, Step: 40\n",
      "Next Action: [-0.818\n",
      "Step reward: -15.965474915704933, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -647.0965986059972\n",
      "Episode: 44, Step: 41\n",
      "Next Action: [-0.520\n",
      "Step reward: -15.974565551959234, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -663.0711641579564\n",
      "Episode: 44, Step: 42\n",
      "Next Action: [-0.404\n",
      "Step reward: -15.9916710166926, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -679.0628351746491\n",
      "Episode: 44, Step: 43\n",
      "Next Action: [-0.363\n",
      "Step reward: -15.972290565787024, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -695.0351257404361\n",
      "Episode: 44, Step: 44\n",
      "Next Action: [-0.518\n",
      "Step reward: -15.953210824292842, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -710.9883365647289\n",
      "Episode: 44, Step: 45\n",
      "Next Action: [-0.258\n",
      "Step reward: -15.960292191080303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -726.9486287558092\n",
      "Episode: 44, Step: 46\n",
      "Next Action: [-0.535\n",
      "Step reward: -15.950398041211688, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -742.8990267970208\n",
      "Episode: 44, Step: 47\n",
      "Next Action: [-0.624\n",
      "Step reward: -15.968198700105358, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -758.8672254971262\n",
      "Episode: 44, Step: 48\n",
      "Next Action: [-0.654\n",
      "Step reward: -15.961610815571479, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -774.8288363126977\n",
      "Episode: 44, Step: 49\n",
      "Next Action: [-0.956\n",
      "Step reward: -15.963397889376434, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -790.7922342020742\n",
      "Episode: 44, Step: 50\n",
      "Next Action: [-1.234\n",
      "Step reward: -15.96872143684848, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -806.7609556389226\n",
      "Episode: 44, Step: 51\n",
      "Next Action: [-0.972\n",
      "Step reward: -15.967723537144556, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -822.7286791760672\n",
      "Episode: 44, Step: 52\n",
      "Next Action: [-0.766\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -838.7286791760672\n",
      "Episode: 44, Step: 53\n",
      "Next Action: [-0.907\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -854.7286791760672\n",
      "Episode: 44, Step: 54\n",
      "Next Action: [-0.701\n",
      "Step reward: -15.997562776660265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -870.7262419527275\n",
      "Episode: 44, Step: 55\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.985246472757451, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -886.711488425485\n",
      "Episode: 44, Step: 56\n",
      "Next Action: [-0.697\n",
      "Step reward: -15.961539231975875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -902.6730276574608\n",
      "Episode: 44, Step: 57\n",
      "Next Action: [-0.382\n",
      "Step reward: -15.97880753950405, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -918.6518351969648\n",
      "Episode: 44, Step: 58\n",
      "Next Action: [-0.604\n",
      "Step reward: -15.97820135320377, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -934.6300365501686\n",
      "Episode: 44, Step: 59\n",
      "Next Action: [-0.654\n",
      "Step reward: -15.988032761419367, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -950.618069311588\n",
      "Episode: 44, Step: 60\n",
      "Next Action: [-0.973\n",
      "Step reward: -15.979570371504812, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -966.5976396830928\n",
      "Episode: 44, Step: 61\n",
      "Next Action: [-0.833\n",
      "Step reward: -15.96380526621781, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -982.5614449493106\n",
      "Episode: 44, Step: 62\n",
      "Next Action: [-0.899\n",
      "Step reward: -15.961365731712123, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -998.5228106810227\n",
      "Episode: 44, Step: 63\n",
      "Next Action: [-0.639\n",
      "Step reward: -15.965322173644674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1014.4881328546675\n",
      "Episode: 44, Step: 64\n",
      "Next Action: [-0.564\n",
      "Step reward: -15.969515647713546, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1030.457648502381\n",
      "Episode: 44, Step: 65\n",
      "Next Action: [-0.686\n",
      "Step reward: -15.970880420682258, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1046.4285289230634\n",
      "Episode: 44, Step: 66\n",
      "Next Action: [-0.499\n",
      "Step reward: -15.973923963457384, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1062.4024528865207\n",
      "Episode: 44, Step: 67\n",
      "Next Action: [-0.279\n",
      "Step reward: -15.966238469543672, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1078.3686913560643\n",
      "Episode: 44, Step: 68\n",
      "Next Action: [-0.367\n",
      "Step reward: -15.985877030743318, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1094.3545683868076\n",
      "Episode: 44, Step: 69\n",
      "Next Action: [-0.435\n",
      "Step reward: -15.985589136114461, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1110.340157522922\n",
      "Episode: 44, Step: 70\n",
      "Next Action: [-0.294\n",
      "Step reward: -15.987873886555109, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1126.328031409477\n",
      "Episode: 44, Step: 71\n",
      "Next Action: [-0.458\n",
      "Step reward: -15.997982175756029, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1142.326013585233\n",
      "Episode: 44, Step: 72\n",
      "Next Action: [-0.104\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1158.326013585233\n",
      "Episode: 44, Step: 73\n",
      "Next Action: [-0.417\n",
      "Step reward: -15.999527418402845, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1174.3255410036359\n",
      "Episode: 44, Step: 74\n",
      "Next Action: [-0.314\n",
      "Step reward: -15.992860906771696, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1190.3184019104076\n",
      "Episode: 44, Step: 75\n",
      "Next Action: [-0.410\n",
      "Step reward: -15.984314082931084, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1206.3027159933388\n",
      "Episode: 44, Step: 76\n",
      "Next Action: [-4.735\n",
      "Step reward: -15.983999346450778, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1222.2867153397895\n",
      "Episode: 44, Step: 77\n",
      "Next Action: [-0.386\n",
      "Step reward: -15.94354332135119, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1238.2302586611406\n",
      "Episode: 44, Step: 78\n",
      "Next Action: [-0.219\n",
      "Step reward: -15.93596037016094, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1254.1662190313016\n",
      "Episode: 44, Step: 79\n",
      "Next Action: [-0.282\n",
      "Step reward: -15.954258786488136, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1270.1204778177896\n",
      "Episode: 44, Step: 80\n",
      "Next Action: [-0.393\n",
      "Step reward: -15.963568551486112, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1286.0840463692757\n",
      "Episode: 44, Step: 81\n",
      "Next Action: [-0.795\n",
      "Step reward: -15.954986439559576, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1302.0390328088354\n",
      "Episode: 44, Step: 82\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.930879356154355, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1317.9699121649896\n",
      "Episode: 44, Step: 83\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.921991328213993, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1333.8919034932037\n",
      "Episode: 44, Step: 84\n",
      "Next Action: [-1.054\n",
      "Step reward: -15.919873491488138, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1349.8117769846917\n",
      "Episode: 44, Step: 85\n",
      "Next Action: [-0.950\n",
      "Step reward: -15.954748328663234, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1365.766525313355\n",
      "Episode: 44, Step: 86\n",
      "Next Action: [-0.964\n",
      "Step reward: -15.969635556865649, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1381.7361608702206\n",
      "Episode: 44, Step: 87\n",
      "Next Action: [-1.000\n",
      "Step reward: -15.98738690786894, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1397.7235477780896\n",
      "Episode: 44, Step: 88\n",
      "Next Action: [-0.923\n",
      "Step reward: -15.98470841956799, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1413.7082561976576\n",
      "Episode: 44, Step: 89\n",
      "Next Action: [-0.795\n",
      "Step reward: -15.981449319775017, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1429.6897055174327\n",
      "Episode: 44, Step: 90\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.97983992192046, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1445.6695454393532\n",
      "Episode: 44, Step: 91\n",
      "Next Action: [-0.877\n",
      "Step reward: -15.95149707497248, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1461.6210425143256\n",
      "Episode: 44, Step: 92\n",
      "Next Action: [-0.999\n",
      "Step reward: -15.929913822554472, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1477.55095633688\n",
      "Episode: 44, Step: 93\n",
      "Next Action: [-1.053\n",
      "Step reward: -15.913735674904812, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1493.4646920117848\n",
      "Episode: 44, Step: 94\n",
      "Next Action: [-0.914\n",
      "Step reward: -15.903177896611446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1509.3678699083962\n",
      "Episode: 44, Step: 95\n",
      "Next Action: [-0.878\n",
      "Step reward: -15.924275781227974, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1525.2921456896242\n",
      "Episode: 44, Step: 96\n",
      "Next Action: [-0.901\n",
      "Step reward: -15.951751162138777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1541.243896851763\n",
      "Episode: 44, Step: 97\n",
      "Next Action: [-0.902\n",
      "Step reward: -15.951532855511996, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1557.195429707275\n",
      "Episode: 44, Step: 98\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.971307949700439, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1573.1667376569756\n",
      "Episode: 44, Step: 99\n",
      "Next Action: [-1.020\n",
      "Step reward: -15.961912672043647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1589.1286503290191\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 1.958639144897461\n",
      "Actor loss: 43.91365432739258\n",
      "Critic loss: 4.168467998504639\n",
      "Actor loss: 30.218414306640625\n",
      "Critic loss: 17.609193801879883\n",
      "Actor loss: 38.69386672973633\n",
      "Critic loss: 4.736780166625977\n",
      "Actor loss: 45.614341735839844\n",
      "Critic loss: 8.832578659057617\n",
      "Actor loss: 42.89421081542969\n",
      "Critic loss: 50.575439453125\n",
      "Actor loss: 32.4487419128418\n",
      "Critic loss: 19.772188186645508\n",
      "Actor loss: 41.33395004272461\n",
      "Critic loss: 12.031658172607422\n",
      "Actor loss: 47.00614547729492\n",
      "Critic loss: 9.095187187194824\n",
      "Actor loss: 51.961814880371094\n",
      "Critic loss: 1.5720226764678955\n",
      "Actor loss: 56.29387283325195\n",
      "Episode: 45\n",
      "Episode: 45, Step: 0\n",
      "Next Action: [-1.297\n",
      "Step reward: -12.172216798447378, Next State: [-0.\n",
      "Total episode reward: -12.172216798447378\n",
      "Episode: 45, Step: 1\n",
      "Next Action: [-1.085\n",
      "Step reward: -14.88271848367133, Next State: [-1. \n",
      "Total episode reward: -27.054935282118706\n",
      "Episode: 45, Step: 2\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.586081863312497, Next State: [-1.\n",
      "Total episode reward: -42.641017145431206\n",
      "Episode: 45, Step: 3\n",
      "Next Action: [-0.636\n",
      "Step reward: -15.844862892196002, Next State: [-1.\n",
      "Total episode reward: -58.48588003762721\n",
      "Episode: 45, Step: 4\n",
      "Next Action: [-0.571\n",
      "Step reward: -15.944920174216891, Next State: [-1.\n",
      "Total episode reward: -74.4308002118441\n",
      "Episode: 45, Step: 5\n",
      "Next Action: [-0.377\n",
      "Step reward: -15.96989122830311, Next State: [-1. \n",
      "Total episode reward: -90.40069144014721\n",
      "Episode: 45, Step: 6\n",
      "Next Action: [-0.287\n",
      "Step reward: -15.971250903743355, Next State: [-1.\n",
      "Total episode reward: -106.37194234389057\n",
      "Episode: 45, Step: 7\n",
      "Next Action: [-0.076\n",
      "Step reward: -15.973050234328323, Next State: [-1.\n",
      "Total episode reward: -122.34499257821889\n",
      "Episode: 45, Step: 8\n",
      "Next Action: [-0.409\n",
      "Step reward: -15.979800248752403, Next State: [-1.\n",
      "Total episode reward: -138.3247928269713\n",
      "Episode: 45, Step: 9\n",
      "Next Action: [-6.048\n",
      "Step reward: -15.987142825920037, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.31193565289135\n",
      "Episode: 45, Step: 10\n",
      "Next Action: [-0.750\n",
      "Step reward: -15.99872004544275, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.3106556983341\n",
      "Episode: 45, Step: 11\n",
      "Next Action: [-0.880\n",
      "Step reward: -15.990261791906919, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.30091749024103\n",
      "Episode: 45, Step: 12\n",
      "Next Action: [-0.795\n",
      "Step reward: -15.992480498831068, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.2933979890721\n",
      "Episode: 45, Step: 13\n",
      "Next Action: [-0.454\n",
      "Step reward: -15.985488796760194, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.2788867858323\n",
      "Episode: 45, Step: 14\n",
      "Next Action: [-0.826\n",
      "Step reward: -15.982206324753186, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.26109311058548\n",
      "Episode: 45, Step: 15\n",
      "Next Action: [-0.800\n",
      "Step reward: -15.98756862401724, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.2486617346027\n",
      "Episode: 45, Step: 16\n",
      "Next Action: [-0.722\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.2486617346027\n",
      "Episode: 45, Step: 17\n",
      "Next Action: [-1.109\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.2486617346027\n",
      "Episode: 45, Step: 18\n",
      "Next Action: [-1.094\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.2486617346027\n",
      "Episode: 45, Step: 19\n",
      "Next Action: [-0.476\n",
      "Step reward: -15.995693264391784, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.2443549989945\n",
      "Episode: 45, Step: 20\n",
      "Next Action: [-0.603\n",
      "Step reward: -15.964244158944808, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.20859915793926\n",
      "Episode: 45, Step: 21\n",
      "Next Action: [-0.309\n",
      "Step reward: -15.952620703500685, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.1612198614399\n",
      "Episode: 45, Step: 22\n",
      "Next Action: [-0.262\n",
      "Step reward: -15.959484759019004, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.1207046204589\n",
      "Episode: 45, Step: 23\n",
      "Next Action: [-0.172\n",
      "Step reward: -15.969322977430917, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.09002759788984\n",
      "Episode: 45, Step: 24\n",
      "Next Action: [-0.176\n",
      "Step reward: -15.955203327301211, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.04523092519105\n",
      "Episode: 45, Step: 25\n",
      "Next Action: [-0.307\n",
      "Step reward: -15.959155635361608, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.00438656055263\n",
      "Episode: 45, Step: 26\n",
      "Next Action: [-0.294\n",
      "Step reward: -15.977568479836142, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -425.98195504038875\n",
      "Episode: 45, Step: 27\n",
      "Next Action: [-0.263\n",
      "Step reward: -15.980196895892316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -441.9621519362811\n",
      "Episode: 45, Step: 28\n",
      "Next Action: [-0.290\n",
      "Step reward: -15.992571352865744, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -457.95472328914684\n",
      "Episode: 45, Step: 29\n",
      "Next Action: [-0.463\n",
      "Step reward: -15.994738906714899, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -473.9494621958617\n",
      "Episode: 45, Step: 30\n",
      "Next Action: [-0.556\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -489.9494621958617\n",
      "Episode: 45, Step: 31\n",
      "Next Action: [-0.599\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -505.9494621958617\n",
      "Episode: 45, Step: 32\n",
      "Next Action: [-0.675\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -521.9494621958618\n",
      "Episode: 45, Step: 33\n",
      "Next Action: [-0.411\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -537.9494621958618\n",
      "Episode: 45, Step: 34\n",
      "Next Action: [-0.103\n",
      "Step reward: -15.999716758111166, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.9491789539729\n",
      "Episode: 45, Step: 35\n",
      "Next Action: [-0.064\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.9491789539729\n",
      "Episode: 45, Step: 36\n",
      "Next Action: [ 0.007\n",
      "Step reward: -15.999557542902116, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.9487364968751\n",
      "Episode: 45, Step: 37\n",
      "Next Action: [ 0.036\n",
      "Step reward: -15.991212042668144, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.9399485395433\n",
      "Episode: 45, Step: 38\n",
      "Next Action: [-0.179\n",
      "Step reward: -15.98872750716477, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.928676046708\n",
      "Episode: 45, Step: 39\n",
      "Next Action: [-0.316\n",
      "Step reward: -15.968348619784377, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.8970246664924\n",
      "Episode: 45, Step: 40\n",
      "Next Action: [-0.149\n",
      "Step reward: -15.956266621643104, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.8532912881354\n",
      "Episode: 45, Step: 41\n",
      "Next Action: [-0.180\n",
      "Step reward: -15.969041274910676, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.8223325630461\n",
      "Episode: 45, Step: 42\n",
      "Next Action: [-1.841\n",
      "Step reward: -15.968934423911348, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.7912669869575\n",
      "Episode: 45, Step: 43\n",
      "Next Action: [-0.437\n",
      "Step reward: -15.97355418667538, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.7648211736329\n",
      "Episode: 45, Step: 44\n",
      "Next Action: [-0.383\n",
      "Step reward: -15.979754008570731, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.7445751822036\n",
      "Episode: 45, Step: 45\n",
      "Next Action: [-0.391\n",
      "Step reward: -15.988643275331604, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.7332184575351\n",
      "Episode: 45, Step: 46\n",
      "Next Action: [-0.796\n",
      "Step reward: -15.979124401134431, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.7123428586696\n",
      "Episode: 45, Step: 47\n",
      "Next Action: [-9.265\n",
      "Step reward: -15.976077917073475, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.688420775743\n",
      "Episode: 45, Step: 48\n",
      "Next Action: [-1.083\n",
      "Step reward: -15.972134479621332, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.6605552553643\n",
      "Episode: 45, Step: 49\n",
      "Next Action: [-1.025\n",
      "Step reward: -15.970630857051368, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.6311861124157\n",
      "Episode: 45, Step: 50\n",
      "Next Action: [-0.972\n",
      "Step reward: -15.97463430450498, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.6058204169207\n",
      "Episode: 45, Step: 51\n",
      "Next Action: [-1.082\n",
      "Step reward: -15.99420948105884, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.6000298979795\n",
      "Episode: 45, Step: 52\n",
      "Next Action: [-1.007\n",
      "Step reward: -15.995053745700776, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.5950836436804\n",
      "Episode: 45, Step: 53\n",
      "Next Action: [-1.086\n",
      "Step reward: -15.977334993681907, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.5724186373623\n",
      "Episode: 45, Step: 54\n",
      "Next Action: [-1.232\n",
      "Step reward: -15.970047957932263, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.5424665952945\n",
      "Episode: 45, Step: 55\n",
      "Next Action: [-1.038\n",
      "Step reward: -15.97054570596484, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.5130123012593\n",
      "Episode: 45, Step: 56\n",
      "Next Action: [-0.824\n",
      "Step reward: -15.963258856203096, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.4762711574624\n",
      "Episode: 45, Step: 57\n",
      "Next Action: [-0.783\n",
      "Step reward: -15.970139040186359, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.4464101976488\n",
      "Episode: 45, Step: 58\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.980226398174864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.4266365958237\n",
      "Episode: 45, Step: 59\n",
      "Next Action: [-0.901\n",
      "Step reward: -15.98180685346814, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.4084434492918\n",
      "Episode: 45, Step: 60\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.993906616259386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.4023500655512\n",
      "Episode: 45, Step: 61\n",
      "Next Action: [-0.614\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.4023500655512\n",
      "Episode: 45, Step: 62\n",
      "Next Action: [-0.444\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.4023500655512\n",
      "Episode: 45, Step: 63\n",
      "Next Action: [-0.648\n",
      "Step reward: -15.997809080531804, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.4001591460831\n",
      "Episode: 45, Step: 64\n",
      "Next Action: [-0.894\n",
      "Step reward: -15.992402022442175, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.3925611685252\n",
      "Episode: 45, Step: 65\n",
      "Next Action: [-0.709\n",
      "Step reward: -15.991413498575504, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.3839746671006\n",
      "Episode: 45, Step: 66\n",
      "Next Action: [-0.914\n",
      "Step reward: -15.995020212446455, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.378994879547\n",
      "Episode: 45, Step: 67\n",
      "Next Action: [-0.688\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.378994879547\n",
      "Episode: 45, Step: 68\n",
      "Next Action: [-0.743\n",
      "Step reward: -15.98792462157389, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.366919501121\n",
      "Episode: 45, Step: 69\n",
      "Next Action: [-0.621\n",
      "Step reward: -15.985643191089732, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.3525626922108\n",
      "Episode: 45, Step: 70\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.98948316735755, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.3420458595683\n",
      "Episode: 45, Step: 71\n",
      "Next Action: [-0.916\n",
      "Step reward: -15.99399867853452, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.336044538103\n",
      "Episode: 45, Step: 72\n",
      "Next Action: [-0.771\n",
      "Step reward: -15.98038533561963, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.3164298737224\n",
      "Episode: 45, Step: 73\n",
      "Next Action: [-0.834\n",
      "Step reward: -15.971616685671057, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.2880465593935\n",
      "Episode: 45, Step: 74\n",
      "Next Action: [-1.366\n",
      "Step reward: -15.96911437777777, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.2571609371712\n",
      "Episode: 45, Step: 75\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.9620636125779, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.219224549749\n",
      "Episode: 45, Step: 76\n",
      "Next Action: [-1.162\n",
      "Step reward: -15.979387086511961, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.198611636261\n",
      "Episode: 45, Step: 77\n",
      "Next Action: [-0.993\n",
      "Step reward: -15.99212297952803, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.1907346157889\n",
      "Episode: 45, Step: 78\n",
      "Next Action: [-0.661\n",
      "Step reward: -15.999627128174978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.1903617439639\n",
      "Episode: 45, Step: 79\n",
      "Next Action: [-0.447\n",
      "Step reward: -15.99625603711904, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.186617781083\n",
      "Episode: 45, Step: 80\n",
      "Next Action: [-0.632\n",
      "Step reward: -15.9946604711346, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.1812782522177\n",
      "Episode: 45, Step: 81\n",
      "Next Action: [-0.598\n",
      "Step reward: -15.999469548918155, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.1807478011358\n",
      "Episode: 45, Step: 82\n",
      "Next Action: [-0.763\n",
      "Step reward: -15.986942633424507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.1676904345602\n",
      "Episode: 45, Step: 83\n",
      "Next Action: [-0.790\n",
      "Step reward: -15.983132059681084, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.1508224942413\n",
      "Episode: 45, Step: 84\n",
      "Next Action: [-0.828\n",
      "Step reward: -15.979688814871148, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.1305113091125\n",
      "Episode: 45, Step: 85\n",
      "Next Action: [-0.817\n",
      "Step reward: -15.97665927067362, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.107170579786\n",
      "Episode: 45, Step: 86\n",
      "Next Action: [-1.000\n",
      "Step reward: -15.98081941332796, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.087989993114\n",
      "Episode: 45, Step: 87\n",
      "Next Action: [-1.092\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.087989993114\n",
      "Episode: 45, Step: 88\n",
      "Next Action: [-1.105\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1417.087989993114\n",
      "Episode: 45, Step: 89\n",
      "Next Action: [-0.977\n",
      "Step reward: -15.996147765421075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1433.084137758535\n",
      "Episode: 45, Step: 90\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.988971051237302, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1449.0731088097723\n",
      "Episode: 45, Step: 91\n",
      "Next Action: [-0.872\n",
      "Step reward: -15.99227255892127, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1465.0653813686936\n",
      "Episode: 45, Step: 92\n",
      "Next Action: [-0.908\n",
      "Step reward: -15.9850418189407, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1481.0504231876344\n",
      "Episode: 45, Step: 93\n",
      "Next Action: [-0.507\n",
      "Step reward: -15.96130909108511, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1497.0117322787194\n",
      "Episode: 45, Step: 94\n",
      "Next Action: [-0.676\n",
      "Step reward: -15.966894570893896, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.9786268496132\n",
      "Episode: 45, Step: 95\n",
      "Next Action: [-0.842\n",
      "Step reward: -15.976896253211484, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.9555231028246\n",
      "Episode: 45, Step: 96\n",
      "Next Action: [-0.766\n",
      "Step reward: -15.980637397900157, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.9361605007248\n",
      "Episode: 45, Step: 97\n",
      "Next Action: [-0.863\n",
      "Step reward: -15.966222851231112, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.902383351956\n",
      "Episode: 45, Step: 98\n",
      "Next Action: [-1.032\n",
      "Step reward: -15.969420927886544, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.8718042798425\n",
      "Episode: 45, Step: 99\n",
      "Next Action: [-1.112\n",
      "Step reward: -15.949148351306855, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.8209526311493\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 21.475370407104492\n",
      "Actor loss: 43.32006072998047\n",
      "Critic loss: 2.0345208644866943\n",
      "Actor loss: 38.805335998535156\n",
      "Critic loss: 1.5186915397644043\n",
      "Actor loss: 57.55229949951172\n",
      "Critic loss: 10.630349159240723\n",
      "Actor loss: 69.40685272216797\n",
      "Critic loss: 28.34119987487793\n",
      "Actor loss: 68.24454498291016\n",
      "Critic loss: 5.268617153167725\n",
      "Actor loss: 50.546878814697266\n",
      "Critic loss: 8.177726745605469\n",
      "Actor loss: 39.6747932434082\n",
      "Critic loss: 16.197736740112305\n",
      "Actor loss: 38.6151008605957\n",
      "Critic loss: 2.990753173828125\n",
      "Actor loss: 52.9868049621582\n",
      "Critic loss: 14.619856834411621\n",
      "Actor loss: 61.06290817260742\n",
      "Episode: 46\n",
      "Episode: 46, Step: 0\n",
      "Next Action: [-1.009\n",
      "Step reward: -11.460537853081709, Next State: [-3.\n",
      "Total episode reward: -11.460537853081709\n",
      "Episode: 46, Step: 1\n",
      "Next Action: [-1.340\n",
      "Step reward: -14.728841222410884, Next State: [-1.\n",
      "Total episode reward: -26.189379075492592\n",
      "Episode: 46, Step: 2\n",
      "Next Action: [-1.163\n",
      "Step reward: -15.51260905570963, Next State: [-1. \n",
      "Total episode reward: -41.70198813120222\n",
      "Episode: 46, Step: 3\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.782704979975382, Next State: [-1.\n",
      "Total episode reward: -57.484693111177606\n",
      "Episode: 46, Step: 4\n",
      "Next Action: [-1.254\n",
      "Step reward: -15.90438132807839, Next State: [-1. \n",
      "Total episode reward: -73.38907443925599\n",
      "Episode: 46, Step: 5\n",
      "Next Action: [-1.356\n",
      "Step reward: -15.999947709623992, Next State: [-1.\n",
      "Total episode reward: -89.38902214887999\n",
      "Episode: 46, Step: 6\n",
      "Next Action: [-1.608\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Total episode reward: -105.38902214887999\n",
      "Episode: 46, Step: 7\n",
      "Next Action: [-1.801\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Total episode reward: -121.38902214887999\n",
      "Episode: 46, Step: 8\n",
      "Next Action: [-1.553\n",
      "Step reward: -15.99656576638808, Next State: [-1. \n",
      "Total episode reward: -137.38558791526808\n",
      "Episode: 46, Step: 9\n",
      "Next Action: [-1.691\n",
      "Step reward: -15.979917369629119, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.36550528489718\n",
      "Episode: 46, Step: 10\n",
      "Next Action: [-1.777\n",
      "Step reward: -15.98281916853137, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -169.34832445342855\n",
      "Episode: 46, Step: 11\n",
      "Next Action: [-1.678\n",
      "Step reward: -15.98748385825924, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -185.3358083116878\n",
      "Episode: 46, Step: 12\n",
      "Next Action: [-1.827\n",
      "Step reward: -15.982876164759096, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -201.3186844764469\n",
      "Episode: 46, Step: 13\n",
      "Next Action: [-1.886\n",
      "Step reward: -15.975806460552592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -217.2944909369995\n",
      "Episode: 46, Step: 14\n",
      "Next Action: [-2.009\n",
      "Step reward: -15.973932282992356, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -233.26842321999186\n",
      "Episode: 46, Step: 15\n",
      "Next Action: [-1.714\n",
      "Step reward: -15.984411332957754, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -249.25283455294962\n",
      "Episode: 46, Step: 16\n",
      "Next Action: [-1.708\n",
      "Step reward: -15.96308233377913, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -265.21591688672873\n",
      "Episode: 46, Step: 17\n",
      "Next Action: [-1.736\n",
      "Step reward: -15.959804641569882, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -281.1757215282986\n",
      "Episode: 46, Step: 18\n",
      "Next Action: [-1.470\n",
      "Step reward: -15.976629632561863, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -297.15235116086046\n",
      "Episode: 46, Step: 19\n",
      "Next Action: [-1.366\n",
      "Step reward: -15.978618301055363, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -313.1309694619158\n",
      "Episode: 46, Step: 20\n",
      "Next Action: [-1.580\n",
      "Step reward: -15.97914085408855, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -329.11011031600435\n",
      "Episode: 46, Step: 21\n",
      "Next Action: [-1.653\n",
      "Step reward: -15.98044026606781, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -345.0905505820722\n",
      "Episode: 46, Step: 22\n",
      "Next Action: [-1.537\n",
      "Step reward: -15.996042734771116, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -361.0865933168433\n",
      "Episode: 46, Step: 23\n",
      "Next Action: [-1.371\n",
      "Step reward: -15.992236576263526, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -377.0788298931068\n",
      "Episode: 46, Step: 24\n",
      "Next Action: [-1.672\n",
      "Step reward: -15.983876212110152, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -393.06270610521693\n",
      "Episode: 46, Step: 25\n",
      "Next Action: [-1.596\n",
      "Step reward: -15.959867520354596, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -409.0225736255715\n",
      "Episode: 46, Step: 26\n",
      "Next Action: [-1.690\n",
      "Step reward: -15.970891643839797, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -424.99346526941133\n",
      "Episode: 46, Step: 27\n",
      "Next Action: [-1.590\n",
      "Step reward: -15.972347449515265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -440.9658127189266\n",
      "Episode: 46, Step: 28\n",
      "Next Action: [-1.568\n",
      "Step reward: -15.970140126517327, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -456.9359528454439\n",
      "Episode: 46, Step: 29\n",
      "Next Action: [-1.200\n",
      "Step reward: -15.955173595132026, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -472.89112644057593\n",
      "Episode: 46, Step: 30\n",
      "Next Action: [-7.312\n",
      "Step reward: -15.951125050653149, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -488.8422514912291\n",
      "Episode: 46, Step: 31\n",
      "Next Action: [-0.845\n",
      "Step reward: -15.95436797347102, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -504.7966194647001\n",
      "Episode: 46, Step: 32\n",
      "Next Action: [-0.953\n",
      "Step reward: -15.960705171952274, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -520.7573246366524\n",
      "Episode: 46, Step: 33\n",
      "Next Action: [-6.612\n",
      "Step reward: -15.985857544695481, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -536.7431821813478\n",
      "Episode: 46, Step: 34\n",
      "Next Action: [-0.685\n",
      "Step reward: -15.984404857803757, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -552.7275870391516\n",
      "Episode: 46, Step: 35\n",
      "Next Action: [-0.806\n",
      "Step reward: -15.978646129210366, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -568.7062331683619\n",
      "Episode: 46, Step: 36\n",
      "Next Action: [-1.008\n",
      "Step reward: -15.963088717838103, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -584.6693218862\n",
      "Episode: 46, Step: 37\n",
      "Next Action: [-0.968\n",
      "Step reward: -15.956655002209255, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -600.6259768884092\n",
      "Episode: 46, Step: 38\n",
      "Next Action: [-1.028\n",
      "Step reward: -15.967680946835825, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -616.593657835245\n",
      "Episode: 46, Step: 39\n",
      "Next Action: [-0.956\n",
      "Step reward: -15.997660869833762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -632.5913187050788\n",
      "Episode: 46, Step: 40\n",
      "Next Action: [-1.289\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -648.5913187050788\n",
      "Episode: 46, Step: 41\n",
      "Next Action: [-1.239\n",
      "Step reward: -15.998019356026315, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -664.5893380611052\n",
      "Episode: 46, Step: 42\n",
      "Next Action: [-0.991\n",
      "Step reward: -15.986800442676152, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -680.5761385037813\n",
      "Episode: 46, Step: 43\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.971783189909303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -696.5479216936906\n",
      "Episode: 46, Step: 44\n",
      "Next Action: [-0.674\n",
      "Step reward: -15.968866589913414, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -712.516788283604\n",
      "Episode: 46, Step: 45\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.972802450589947, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -728.4895907341939\n",
      "Episode: 46, Step: 46\n",
      "Next Action: [-0.780\n",
      "Step reward: -15.9743503281016, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -744.4639410622956\n",
      "Episode: 46, Step: 47\n",
      "Next Action: [-0.762\n",
      "Step reward: -15.970202139531658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -760.4341432018273\n",
      "Episode: 46, Step: 48\n",
      "Next Action: [-0.639\n",
      "Step reward: -15.955358688685438, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -776.3895018905127\n",
      "Episode: 46, Step: 49\n",
      "Next Action: [-0.593\n",
      "Step reward: -15.96577170797855, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -792.3552735984913\n",
      "Episode: 46, Step: 50\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.963114220652713, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -808.318387819144\n",
      "Episode: 46, Step: 51\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.945975290518415, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -824.2643631096624\n",
      "Episode: 46, Step: 52\n",
      "Next Action: [-1.428\n",
      "Step reward: -15.939158917758602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -840.2035220274211\n",
      "Episode: 46, Step: 53\n",
      "Next Action: [-1.726\n",
      "Step reward: -15.941956342938525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -856.1454783703596\n",
      "Episode: 46, Step: 54\n",
      "Next Action: [-1.655\n",
      "Step reward: -15.968286369288554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -872.1137647396481\n",
      "Episode: 46, Step: 55\n",
      "Next Action: [-1.791\n",
      "Step reward: -15.987519467246585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -888.1012842068947\n",
      "Episode: 46, Step: 56\n",
      "Next Action: [-1.790\n",
      "Step reward: -15.989248730626507, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -904.0905329375212\n",
      "Episode: 46, Step: 57\n",
      "Next Action: [-1.806\n",
      "Step reward: -15.992024687955052, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -920.0825576254763\n",
      "Episode: 46, Step: 58\n",
      "Next Action: [-2.000\n",
      "Step reward: -15.990728379376813, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -936.0732860048531\n",
      "Episode: 46, Step: 59\n",
      "Next Action: [-1.817\n",
      "Step reward: -15.998202508136373, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -952.0714885129895\n",
      "Episode: 46, Step: 60\n",
      "Next Action: [-1.654\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -968.0714885129895\n",
      "Episode: 46, Step: 61\n",
      "Next Action: [-1.421\n",
      "Step reward: -15.999065564418602, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -984.0705540774081\n",
      "Episode: 46, Step: 62\n",
      "Next Action: [-1.364\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1000.0705540774081\n",
      "Episode: 46, Step: 63\n",
      "Next Action: [-1.313\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1016.0705540774081\n",
      "Episode: 46, Step: 64\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.999046984696864, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1032.069601062105\n",
      "Episode: 46, Step: 65\n",
      "Next Action: [-1.035\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1048.069601062105\n",
      "Episode: 46, Step: 66\n",
      "Next Action: [-1.343\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1064.069601062105\n",
      "Episode: 46, Step: 67\n",
      "Next Action: [-1.292\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1080.069601062105\n",
      "Episode: 46, Step: 68\n",
      "Next Action: [-1.578\n",
      "Step reward: -15.991621946831534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1096.0612230089366\n",
      "Episode: 46, Step: 69\n",
      "Next Action: [-1.549\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.0612230089366\n",
      "Episode: 46, Step: 70\n",
      "Next Action: [-1.334\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.0612230089366\n",
      "Episode: 46, Step: 71\n",
      "Next Action: [-1.500\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.0612230089366\n",
      "Episode: 46, Step: 72\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.997783910136963, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.0590069190737\n",
      "Episode: 46, Step: 73\n",
      "Next Action: [-1.724\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.0590069190737\n",
      "Episode: 46, Step: 74\n",
      "Next Action: [-1.821\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.0590069190737\n",
      "Episode: 46, Step: 75\n",
      "Next Action: [-1.629\n",
      "Step reward: -15.999518260736266, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.05852517981\n",
      "Episode: 46, Step: 76\n",
      "Next Action: [-1.286\n",
      "Step reward: -15.997691397106786, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.0562165769168\n",
      "Episode: 46, Step: 77\n",
      "Next Action: [-1.234\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.0562165769168\n",
      "Episode: 46, Step: 78\n",
      "Next Action: [-1.344\n",
      "Step reward: -15.994386950322042, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.0506035272388\n",
      "Episode: 46, Step: 79\n",
      "Next Action: [-1.148\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.0506035272388\n",
      "Episode: 46, Step: 80\n",
      "Next Action: [-1.280\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.0506035272388\n",
      "Episode: 46, Step: 81\n",
      "Next Action: [-1.326\n",
      "Step reward: -15.998540247374468, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.0491437746132\n",
      "Episode: 46, Step: 82\n",
      "Next Action: [-1.020\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.0491437746132\n",
      "Episode: 46, Step: 83\n",
      "Next Action: [-1.079\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.0491437746132\n",
      "Episode: 46, Step: 84\n",
      "Next Action: [-0.954\n",
      "Step reward: -15.993498831941745, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.0426426065549\n",
      "Episode: 46, Step: 85\n",
      "Next Action: [-0.819\n",
      "Step reward: -15.998506882285723, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.0411494888406\n",
      "Episode: 46, Step: 86\n",
      "Next Action: [-1.049\n",
      "Step reward: -15.98820392029552, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.029353409136\n",
      "Episode: 46, Step: 87\n",
      "Next Action: [-1.413\n",
      "Step reward: -15.970468178813348, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1399.9998215879496\n",
      "Episode: 46, Step: 88\n",
      "Next Action: [-1.359\n",
      "Step reward: -15.966932516411068, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1415.9667541043607\n",
      "Episode: 46, Step: 89\n",
      "Next Action: [-1.189\n",
      "Step reward: -15.989436100364342, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1431.956190204725\n",
      "Episode: 46, Step: 90\n",
      "Next Action: [-1.374\n",
      "Step reward: -15.994772269770184, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1447.9509624744953\n",
      "Episode: 46, Step: 91\n",
      "Next Action: [-1.652\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1463.9509624744953\n",
      "Episode: 46, Step: 92\n",
      "Next Action: [-1.384\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1479.9509624744953\n",
      "Episode: 46, Step: 93\n",
      "Next Action: [-1.136\n",
      "Step reward: -15.960876295899668, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1495.911838770395\n",
      "Episode: 46, Step: 94\n",
      "Next Action: [-1.251\n",
      "Step reward: -15.92791473727578, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1511.8397535076708\n",
      "Episode: 46, Step: 95\n",
      "Next Action: [-1.414\n",
      "Step reward: -15.92258563469292, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1527.7623391423638\n",
      "Episode: 46, Step: 96\n",
      "Next Action: [-1.191\n",
      "Step reward: -15.931041729528866, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1543.6933808718927\n",
      "Episode: 46, Step: 97\n",
      "Next Action: [-1.580\n",
      "Step reward: -15.95594287646515, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1559.6493237483578\n",
      "Episode: 46, Step: 98\n",
      "Next Action: [-1.550\n",
      "Step reward: -15.970634142460746, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1575.6199578908186\n",
      "Episode: 46, Step: 99\n",
      "Next Action: [-1.345\n",
      "Step reward: -15.974961943783459, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1591.594919834602\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 16.70882225036621\n",
      "Actor loss: 57.41025924682617\n",
      "Critic loss: 13.355552673339844\n",
      "Actor loss: 36.196571350097656\n",
      "Critic loss: 4.871210098266602\n",
      "Actor loss: 50.53544998168945\n",
      "Critic loss: 20.444307327270508\n",
      "Actor loss: 28.766082763671875\n",
      "Critic loss: 11.381653785705566\n",
      "Actor loss: 42.29012680053711\n",
      "Critic loss: 5.674725532531738\n",
      "Actor loss: 54.969993591308594\n",
      "Critic loss: 5.1418232917785645\n",
      "Actor loss: 28.719585418701172\n",
      "Critic loss: 1.7183645963668823\n",
      "Actor loss: 52.094024658203125\n",
      "Critic loss: 11.230791091918945\n",
      "Actor loss: 40.16681671142578\n",
      "Critic loss: 4.407834053039551\n",
      "Actor loss: 28.442312240600586\n",
      "Episode: 47\n",
      "Episode: 47, Step: 0\n",
      "Next Action: [-1.306\n",
      "Step reward: -11.799141626514963, Next State: [-1.\n",
      "Total episode reward: -11.799141626514963\n",
      "Episode: 47, Step: 1\n",
      "Next Action: [-1.399\n",
      "Step reward: -15.092213926712898, Next State: [-1.\n",
      "Total episode reward: -26.89135555322786\n",
      "Episode: 47, Step: 2\n",
      "Next Action: [-1.485\n",
      "Step reward: -15.757510574672809, Next State: [-1.\n",
      "Total episode reward: -42.64886612790067\n",
      "Episode: 47, Step: 3\n",
      "Next Action: [-1.245\n",
      "Step reward: -15.91084205313241, Next State: [-1. \n",
      "Total episode reward: -58.55970818103307\n",
      "Episode: 47, Step: 4\n",
      "Next Action: [-1.502\n",
      "Step reward: -15.952254236779556, Next State: [-1.\n",
      "Total episode reward: -74.51196241781263\n",
      "Episode: 47, Step: 5\n",
      "Next Action: [-1.349\n",
      "Step reward: -15.983761439878867, Next State: [-1.\n",
      "Total episode reward: -90.49572385769149\n",
      "Episode: 47, Step: 6\n",
      "Next Action: [-1.457\n",
      "Step reward: -15.996015849459093, Next State: [-1.\n",
      "Total episode reward: -106.49173970715059\n",
      "Episode: 47, Step: 7\n",
      "Next Action: [-1.364\n",
      "Step reward: -15.974866949208339, Next State: [-1.\n",
      "Total episode reward: -122.46660665635892\n",
      "Episode: 47, Step: 8\n",
      "Next Action: [-1.244\n",
      "Step reward: -15.956358188202923, Next State: [-1.\n",
      "Total episode reward: -138.42296484456185\n",
      "Episode: 47, Step: 9\n",
      "Next Action: [-1.432\n",
      "Step reward: -15.946052641268185, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.36901748583003\n",
      "Episode: 47, Step: 10\n",
      "Next Action: [-1.645\n",
      "Step reward: -15.984172688353858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.35319017418388\n",
      "Episode: 47, Step: 11\n",
      "Next Action: [-1.607\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.35319017418388\n",
      "Episode: 47, Step: 12\n",
      "Next Action: [-1.555\n",
      "Step reward: -15.999488971771534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.35267914595542\n",
      "Episode: 47, Step: 13\n",
      "Next Action: [-1.369\n",
      "Step reward: -15.997731052395588, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.350410198351\n",
      "Episode: 47, Step: 14\n",
      "Next Action: [-1.246\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.350410198351\n",
      "Episode: 47, Step: 15\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.996822252470322, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.3472324508213\n",
      "Episode: 47, Step: 16\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.985020187468471, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.3322526382898\n",
      "Episode: 47, Step: 17\n",
      "Next Action: [-1.377\n",
      "Step reward: -15.980759147855316, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.3130117861451\n",
      "Episode: 47, Step: 18\n",
      "Next Action: [-1.199\n",
      "Step reward: -15.974364780057853, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.28737656620297\n",
      "Episode: 47, Step: 19\n",
      "Next Action: [-1.053\n",
      "Step reward: -15.979431204865525, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.2668077710685\n",
      "Episode: 47, Step: 20\n",
      "Next Action: [-1.113\n",
      "Step reward: -15.98978843931125, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.25659621037977\n",
      "Episode: 47, Step: 21\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.995569786927248, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.25216599730703\n",
      "Episode: 47, Step: 22\n",
      "Next Action: [-1.144\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.25216599730703\n",
      "Episode: 47, Step: 23\n",
      "Next Action: [-0.758\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.25216599730703\n",
      "Episode: 47, Step: 24\n",
      "Next Action: [-0.884\n",
      "Step reward: -15.994134039063841, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.2463000363709\n",
      "Episode: 47, Step: 25\n",
      "Next Action: [-0.660\n",
      "Step reward: -15.983874410491234, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.2301744468621\n",
      "Episode: 47, Step: 26\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.970951394211015, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.2011258410731\n",
      "Episode: 47, Step: 27\n",
      "Next Action: [-0.787\n",
      "Step reward: -15.967472961213875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.168598802287\n",
      "Episode: 47, Step: 28\n",
      "Next Action: [-0.738\n",
      "Step reward: -15.965446843492854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.13404564577985\n",
      "Episode: 47, Step: 29\n",
      "Next Action: [-0.794\n",
      "Step reward: -15.972120196261873, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.1061658420417\n",
      "Episode: 47, Step: 30\n",
      "Next Action: [-0.438\n",
      "Step reward: -15.97921156611377, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.08537740815547\n",
      "Episode: 47, Step: 31\n",
      "Next Action: [-0.548\n",
      "Step reward: -15.978421960563733, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.0637993687192\n",
      "Episode: 47, Step: 32\n",
      "Next Action: [-0.337\n",
      "Step reward: -15.97497507590794, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.0387744446272\n",
      "Episode: 47, Step: 33\n",
      "Next Action: [-0.719\n",
      "Step reward: -15.973543569470305, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.0123180140974\n",
      "Episode: 47, Step: 34\n",
      "Next Action: [-0.855\n",
      "Step reward: -15.965369784452315, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -553.9776877985497\n",
      "Episode: 47, Step: 35\n",
      "Next Action: [-1.097\n",
      "Step reward: -15.978564399366336, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -569.956252197916\n",
      "Episode: 47, Step: 36\n",
      "Next Action: [-1.180\n",
      "Step reward: -15.979712752397326, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -585.9359649503133\n",
      "Episode: 47, Step: 37\n",
      "Next Action: [-1.442\n",
      "Step reward: -15.992643937269351, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -601.9286088875826\n",
      "Episode: 47, Step: 38\n",
      "Next Action: [-1.750\n",
      "Step reward: -15.990275224064755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -617.9188841116473\n",
      "Episode: 47, Step: 39\n",
      "Next Action: [-1.916\n",
      "Step reward: -15.964508293480922, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -633.8833924051282\n",
      "Episode: 47, Step: 40\n",
      "Next Action: [-1.974\n",
      "Step reward: -15.949750126917674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -649.8331425320459\n",
      "Episode: 47, Step: 41\n",
      "Next Action: [-1.855\n",
      "Step reward: -15.942185124247889, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -665.7753276562938\n",
      "Episode: 47, Step: 42\n",
      "Next Action: [-2.324\n",
      "Step reward: -15.94636147313278, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -681.7216891294266\n",
      "Episode: 47, Step: 43\n",
      "Next Action: [-2.240\n",
      "Step reward: -15.971293245515955, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.6929823749426\n",
      "Episode: 47, Step: 44\n",
      "Next Action: [-1.982\n",
      "Step reward: -15.987164502909158, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.6801468778518\n",
      "Episode: 47, Step: 45\n",
      "Next Action: [-2.042\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.6801468778518\n",
      "Episode: 47, Step: 46\n",
      "Next Action: [-2.143\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.6801468778518\n",
      "Episode: 47, Step: 47\n",
      "Next Action: [-1.869\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.6801468778518\n",
      "Episode: 47, Step: 48\n",
      "Next Action: [-1.853\n",
      "Step reward: -15.992188750429142, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.672335628281\n",
      "Episode: 47, Step: 49\n",
      "Next Action: [-1.476\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.672335628281\n",
      "Episode: 47, Step: 50\n",
      "Next Action: [-1.518\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.672335628281\n",
      "Episode: 47, Step: 51\n",
      "Next Action: [-1.409\n",
      "Step reward: -15.9923474290285, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.6646830573095\n",
      "Episode: 47, Step: 52\n",
      "Next Action: [-1.308\n",
      "Step reward: -15.979153269882712, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.6438363271922\n",
      "Episode: 47, Step: 53\n",
      "Next Action: [-0.929\n",
      "Step reward: -15.971305772734386, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.6151420999265\n",
      "Episode: 47, Step: 54\n",
      "Next Action: [-0.907\n",
      "Step reward: -15.967710115056642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.5828522149832\n",
      "Episode: 47, Step: 55\n",
      "Next Action: [-1.135\n",
      "Step reward: -15.969388971373046, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.5522411863562\n",
      "Episode: 47, Step: 56\n",
      "Next Action: [-1.276\n",
      "Step reward: -15.972676316759674, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.524917503116\n",
      "Episode: 47, Step: 57\n",
      "Next Action: [-1.152\n",
      "Step reward: -15.982478572749294, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.5073960758652\n",
      "Episode: 47, Step: 58\n",
      "Next Action: [-1.030\n",
      "Step reward: -15.989516876616724, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.4969129524819\n",
      "Episode: 47, Step: 59\n",
      "Next Action: [-0.692\n",
      "Step reward: -15.997559148915697, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.4944721013976\n",
      "Episode: 47, Step: 60\n",
      "Next Action: [-0.848\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.4944721013976\n",
      "Episode: 47, Step: 61\n",
      "Next Action: [-0.755\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.4944721013976\n",
      "Episode: 47, Step: 62\n",
      "Next Action: [-0.642\n",
      "Step reward: -15.992876451550009, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.4873485529477\n",
      "Episode: 47, Step: 63\n",
      "Next Action: [-0.422\n",
      "Step reward: -15.975417442861016, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.4627659958087\n",
      "Episode: 47, Step: 64\n",
      "Next Action: [-2.162\n",
      "Step reward: -15.966168283628743, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.4289342794375\n",
      "Episode: 47, Step: 65\n",
      "Next Action: [-0.386\n",
      "Step reward: -15.970761238552642, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.39969551799\n",
      "Episode: 47, Step: 66\n",
      "Next Action: [-0.083\n",
      "Step reward: -15.956535163456838, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.356230681447\n",
      "Episode: 47, Step: 67\n",
      "Next Action: [ 0.025\n",
      "Step reward: -15.936822946229347, Next State: [-0.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.2930536276763\n",
      "Episode: 47, Step: 68\n",
      "Next Action: [-0.129\n",
      "Step reward: -15.959001249893364, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.2520548775697\n",
      "Episode: 47, Step: 69\n",
      "Next Action: [-0.050\n",
      "Step reward: -15.962318849402472, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.214373726972\n",
      "Episode: 47, Step: 70\n",
      "Next Action: [-0.210\n",
      "Step reward: -15.982014677228655, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.1963884042007\n",
      "Episode: 47, Step: 71\n",
      "Next Action: [-0.671\n",
      "Step reward: -15.97874609575697, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.1751344999577\n",
      "Episode: 47, Step: 72\n",
      "Next Action: [-0.549\n",
      "Step reward: -15.9643815072824, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.1395160072402\n",
      "Episode: 47, Step: 73\n",
      "Next Action: [-5.413\n",
      "Step reward: -15.939124355479994, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.0786403627203\n",
      "Episode: 47, Step: 74\n",
      "Next Action: [-0.427\n",
      "Step reward: -15.930652910603717, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.009293273324\n",
      "Episode: 47, Step: 75\n",
      "Next Action: [-0.615\n",
      "Step reward: -15.924390489336236, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.9336837626602\n",
      "Episode: 47, Step: 76\n",
      "Next Action: [-0.708\n",
      "Step reward: -15.945935426039956, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.8796191887002\n",
      "Episode: 47, Step: 77\n",
      "Next Action: [-0.801\n",
      "Step reward: -15.97069500648979, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.8503141951899\n",
      "Episode: 47, Step: 78\n",
      "Next Action: [-0.866\n",
      "Step reward: -15.969615986209542, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.8199301813995\n",
      "Episode: 47, Step: 79\n",
      "Next Action: [-0.590\n",
      "Step reward: -15.984263015869706, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.8041931972691\n",
      "Episode: 47, Step: 80\n",
      "Next Action: [-0.631\n",
      "Step reward: -15.980969293973223, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.7851624912423\n",
      "Episode: 47, Step: 81\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.96174076131953, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.7469032525619\n",
      "Episode: 47, Step: 82\n",
      "Next Action: [-0.980\n",
      "Step reward: -15.949014324332188, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.695917576894\n",
      "Episode: 47, Step: 83\n",
      "Next Action: [-0.925\n",
      "Step reward: -15.94945575976788, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.645373336662\n",
      "Episode: 47, Step: 84\n",
      "Next Action: [-0.848\n",
      "Step reward: -15.95043810957425, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.5958114462362\n",
      "Episode: 47, Step: 85\n",
      "Next Action: [-0.935\n",
      "Step reward: -15.93754781476314, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.5333592609993\n",
      "Episode: 47, Step: 86\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.929284864597058, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.4626441255964\n",
      "Episode: 47, Step: 87\n",
      "Next Action: [-0.988\n",
      "Step reward: -15.94065911522795, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1400.4033032408242\n",
      "Episode: 47, Step: 88\n",
      "Next Action: [-0.849\n",
      "Step reward: -15.965845103514658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.369148344339\n",
      "Episode: 47, Step: 89\n",
      "Next Action: [-0.866\n",
      "Step reward: -15.98056564678717, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.3497139911262\n",
      "Episode: 47, Step: 90\n",
      "Next Action: [-1.273\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.3497139911262\n",
      "Episode: 47, Step: 91\n",
      "Next Action: [-1.106\n",
      "Step reward: -15.996349609881522, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.3460636010077\n",
      "Episode: 47, Step: 92\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.993930236129584, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.3399938371372\n",
      "Episode: 47, Step: 93\n",
      "Next Action: [-1.128\n",
      "Step reward: -15.99598410652251, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.3359779436596\n",
      "Episode: 47, Step: 94\n",
      "Next Action: [-8.977\n",
      "Step reward: -15.993751751374075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.3297296950336\n",
      "Episode: 47, Step: 95\n",
      "Next Action: [-1.029\n",
      "Step reward: -15.998958407372909, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.3286881024064\n",
      "Episode: 47, Step: 96\n",
      "Next Action: [-0.819\n",
      "Step reward: -15.998189061998106, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.3268771644046\n",
      "Episode: 47, Step: 97\n",
      "Next Action: [-0.785\n",
      "Step reward: -15.989744391457736, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.3166215558622\n",
      "Episode: 47, Step: 98\n",
      "Next Action: [-0.853\n",
      "Step reward: -15.969610297895919, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.2862318537582\n",
      "Episode: 47, Step: 99\n",
      "Next Action: [-0.832\n",
      "Step reward: -15.97770353533312, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.2639353890913\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 1.7976247072219849\n",
      "Actor loss: 50.1558837890625\n",
      "Critic loss: 24.06908416748047\n",
      "Actor loss: 30.13576889038086\n",
      "Critic loss: 18.63364601135254\n",
      "Actor loss: 39.7631950378418\n",
      "Critic loss: 53.88335037231445\n",
      "Actor loss: 25.293659210205078\n",
      "Critic loss: 4.532890796661377\n",
      "Actor loss: 49.41547393798828\n",
      "Critic loss: 2.0796260833740234\n",
      "Actor loss: 57.771812438964844\n",
      "Critic loss: 7.249698162078857\n",
      "Actor loss: 49.6845588684082\n",
      "Critic loss: 2.520291328430176\n",
      "Actor loss: 35.1575813293457\n",
      "Critic loss: 2.613184928894043\n",
      "Actor loss: 59.629852294921875\n",
      "Critic loss: 4.2121381759643555\n",
      "Actor loss: 53.10997772216797\n",
      "Episode: 48\n",
      "Episode: 48, Step: 0\n",
      "Next Action: [-0.646\n",
      "Step reward: -12.41627395523991, Next State: [ 0.2\n",
      "Total episode reward: -12.41627395523991\n",
      "Episode: 48, Step: 1\n",
      "Next Action: [-0.841\n",
      "Step reward: -15.385672333298746, Next State: [-0.\n",
      "Total episode reward: -27.80194628853866\n",
      "Episode: 48, Step: 2\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.790740604191456, Next State: [-1.\n",
      "Total episode reward: -43.592686892730114\n",
      "Episode: 48, Step: 3\n",
      "Next Action: [-1.009\n",
      "Step reward: -15.92888081539765, Next State: [-1. \n",
      "Total episode reward: -59.52156770812776\n",
      "Episode: 48, Step: 4\n",
      "Next Action: [-1.133\n",
      "Step reward: -15.958449541931401, Next State: [-1.\n",
      "Total episode reward: -75.48001725005916\n",
      "Episode: 48, Step: 5\n",
      "Next Action: [-1.407\n",
      "Step reward: -15.975019225658608, Next State: [-1.\n",
      "Total episode reward: -91.45503647571778\n",
      "Episode: 48, Step: 6\n",
      "Next Action: [-1.464\n",
      "Step reward: -15.984892372383301, Next State: [-1.\n",
      "Total episode reward: -107.43992884810108\n",
      "Episode: 48, Step: 7\n",
      "Next Action: [-1.553\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Total episode reward: -123.43992884810108\n",
      "Episode: 48, Step: 8\n",
      "Next Action: [-1.662\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Total episode reward: -139.4399288481011\n",
      "Episode: 48, Step: 9\n",
      "Next Action: [-1.447\n",
      "Step reward: -15.998851448225857, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -155.43878029632694\n",
      "Episode: 48, Step: 10\n",
      "Next Action: [-1.517\n",
      "Step reward: -15.977389009495361, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -171.4161693058223\n",
      "Episode: 48, Step: 11\n",
      "Next Action: [-1.305\n",
      "Step reward: -15.953285296201331, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -187.36945460202364\n",
      "Episode: 48, Step: 12\n",
      "Next Action: [-1.233\n",
      "Step reward: -15.948375107519489, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -203.31782970954313\n",
      "Episode: 48, Step: 13\n",
      "Next Action: [-1.117\n",
      "Step reward: -15.94756426436432, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -219.26539397390746\n",
      "Episode: 48, Step: 14\n",
      "Next Action: [-1.067\n",
      "Step reward: -15.950984652903985, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -235.21637862681146\n",
      "Episode: 48, Step: 15\n",
      "Next Action: [-1.368\n",
      "Step reward: -15.921221990981028, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -251.1376006177925\n",
      "Episode: 48, Step: 16\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.915317276435236, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -267.05291789422773\n",
      "Episode: 48, Step: 17\n",
      "Next Action: [-1.241\n",
      "Step reward: -15.914562990616648, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.9674808848444\n",
      "Episode: 48, Step: 18\n",
      "Next Action: [-1.193\n",
      "Step reward: -15.909361116358683, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.8768420012031\n",
      "Episode: 48, Step: 19\n",
      "Next Action: [-1.720\n",
      "Step reward: -15.9302002149364, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.8070422161395\n",
      "Episode: 48, Step: 20\n",
      "Next Action: [-1.632\n",
      "Step reward: -15.924767156576491, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.731809372716\n",
      "Episode: 48, Step: 21\n",
      "Next Action: [-1.607\n",
      "Step reward: -15.94021453230107, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.67202390501706\n",
      "Episode: 48, Step: 22\n",
      "Next Action: [-1.652\n",
      "Step reward: -15.964328956464467, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.6363528614815\n",
      "Episode: 48, Step: 23\n",
      "Next Action: [-1.778\n",
      "Step reward: -15.971367878399553, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.6077207398811\n",
      "Episode: 48, Step: 24\n",
      "Next Action: [-1.348\n",
      "Step reward: -15.96939131414334, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.57711205402444\n",
      "Episode: 48, Step: 25\n",
      "Next Action: [-1.077\n",
      "Step reward: -15.964664161817923, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.5417762158424\n",
      "Episode: 48, Step: 26\n",
      "Next Action: [-1.019\n",
      "Step reward: -15.971724578404253, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.5135007942466\n",
      "Episode: 48, Step: 27\n",
      "Next Action: [-1.235\n",
      "Step reward: -15.981126432585105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.4946272268317\n",
      "Episode: 48, Step: 28\n",
      "Next Action: [-1.321\n",
      "Step reward: -15.982865363654303, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.477492590486\n",
      "Episode: 48, Step: 29\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.988483018942219, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.46597560942826\n",
      "Episode: 48, Step: 30\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.986795360590534, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.4527709700188\n",
      "Episode: 48, Step: 31\n",
      "Next Action: [-0.971\n",
      "Step reward: -15.99700675860404, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.4497777286228\n",
      "Episode: 48, Step: 32\n",
      "Next Action: [-0.897\n",
      "Step reward: -15.988885838746718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.4386635673695\n",
      "Episode: 48, Step: 33\n",
      "Next Action: [-1.103\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.4386635673695\n",
      "Episode: 48, Step: 34\n",
      "Next Action: [-0.776\n",
      "Step reward: -15.993788364046756, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.4324519314163\n",
      "Episode: 48, Step: 35\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.992533764424676, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.424985695841\n",
      "Episode: 48, Step: 36\n",
      "Next Action: [-1.520\n",
      "Step reward: -15.98233385863304, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.407319554474\n",
      "Episode: 48, Step: 37\n",
      "Next Action: [-1.515\n",
      "Step reward: -15.97540657607946, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.3827261305535\n",
      "Episode: 48, Step: 38\n",
      "Next Action: [-1.264\n",
      "Step reward: -15.970397606884905, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -618.3531237374384\n",
      "Episode: 48, Step: 39\n",
      "Next Action: [-1.190\n",
      "Step reward: -15.976858610390105, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -634.3299823478285\n",
      "Episode: 48, Step: 40\n",
      "Next Action: [-1.346\n",
      "Step reward: -15.984312734015939, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -650.3142950818444\n",
      "Episode: 48, Step: 41\n",
      "Next Action: [-1.169\n",
      "Step reward: -15.973937428196855, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -666.2882325100413\n",
      "Episode: 48, Step: 42\n",
      "Next Action: [-1.207\n",
      "Step reward: -15.968910158799902, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -682.2571426688412\n",
      "Episode: 48, Step: 43\n",
      "Next Action: [-0.967\n",
      "Step reward: -15.960407922933827, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -698.2175505917751\n",
      "Episode: 48, Step: 44\n",
      "Next Action: [-1.344\n",
      "Step reward: -15.981625665592324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -714.1991762573674\n",
      "Episode: 48, Step: 45\n",
      "Next Action: [-1.055\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -730.1991762573674\n",
      "Episode: 48, Step: 46\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.999533721246769, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -746.1987099786141\n",
      "Episode: 48, Step: 47\n",
      "Next Action: [-1.210\n",
      "Step reward: -15.99538942736781, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -762.194099405982\n",
      "Episode: 48, Step: 48\n",
      "Next Action: [-0.912\n",
      "Step reward: -15.973905200318764, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -778.1680046063008\n",
      "Episode: 48, Step: 49\n",
      "Next Action: [-0.930\n",
      "Step reward: -15.969232896588592, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -794.1372375028893\n",
      "Episode: 48, Step: 50\n",
      "Next Action: [-0.587\n",
      "Step reward: -15.951962665036074, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -810.0892001679254\n",
      "Episode: 48, Step: 51\n",
      "Next Action: [-0.800\n",
      "Step reward: -15.94089011463311, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -826.0300902825585\n",
      "Episode: 48, Step: 52\n",
      "Next Action: [-1.021\n",
      "Step reward: -15.941484188428873, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.9715744709873\n",
      "Episode: 48, Step: 53\n",
      "Next Action: [-1.154\n",
      "Step reward: -15.969647965579675, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.941222436567\n",
      "Episode: 48, Step: 54\n",
      "Next Action: [-1.021\n",
      "Step reward: -15.957948059183252, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.8991704957502\n",
      "Episode: 48, Step: 55\n",
      "Next Action: [-1.108\n",
      "Step reward: -15.962042484586282, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.8612129803365\n",
      "Episode: 48, Step: 56\n",
      "Next Action: [-1.057\n",
      "Step reward: -15.96613864332879, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.8273516236653\n",
      "Episode: 48, Step: 57\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.9861392799667, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.813490903632\n",
      "Episode: 48, Step: 58\n",
      "Next Action: [-1.095\n",
      "Step reward: -15.971839118959219, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.7853300225912\n",
      "Episode: 48, Step: 59\n",
      "Next Action: [-1.467\n",
      "Step reward: -15.973417544071552, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.7587475666628\n",
      "Episode: 48, Step: 60\n",
      "Next Action: [-1.415\n",
      "Step reward: -15.982034200030647, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.7407817666934\n",
      "Episode: 48, Step: 61\n",
      "Next Action: [-1.437\n",
      "Step reward: -15.999304721426912, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.7400864881204\n",
      "Episode: 48, Step: 62\n",
      "Next Action: [-1.426\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.7400864881204\n",
      "Episode: 48, Step: 63\n",
      "Next Action: [-1.337\n",
      "Step reward: -15.996635532087595, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.7367220202079\n",
      "Episode: 48, Step: 64\n",
      "Next Action: [-1.304\n",
      "Step reward: -15.99172682157589, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.7284488417838\n",
      "Episode: 48, Step: 65\n",
      "Next Action: [-1.244\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.7284488417838\n",
      "Episode: 48, Step: 66\n",
      "Next Action: [-1.440\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.7284488417838\n",
      "Episode: 48, Step: 67\n",
      "Next Action: [-1.255\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.7284488417838\n",
      "Episode: 48, Step: 68\n",
      "Next Action: [-1.263\n",
      "Step reward: -15.993523792441156, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.721972634225\n",
      "Episode: 48, Step: 69\n",
      "Next Action: [-1.053\n",
      "Step reward: -15.987879090122076, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.7098517243471\n",
      "Episode: 48, Step: 70\n",
      "Next Action: [-1.020\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.7098517243471\n",
      "Episode: 48, Step: 71\n",
      "Next Action: [-1.342\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.7098517243471\n",
      "Episode: 48, Step: 72\n",
      "Next Action: [-1.471\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.7098517243471\n",
      "Episode: 48, Step: 73\n",
      "Next Action: [-1.506\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.7098517243471\n",
      "Episode: 48, Step: 74\n",
      "Next Action: [-1.312\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.7098517243471\n",
      "Episode: 48, Step: 75\n",
      "Next Action: [-1.250\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.7098517243471\n",
      "Episode: 48, Step: 76\n",
      "Next Action: [-1.218\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.7098517243471\n",
      "Episode: 48, Step: 77\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.987403047592373, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.6972547719395\n",
      "Episode: 48, Step: 78\n",
      "Next Action: [-0.920\n",
      "Step reward: -15.991862967624673, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.6891177395642\n",
      "Episode: 48, Step: 79\n",
      "Next Action: [-1.011\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.6891177395642\n",
      "Episode: 48, Step: 80\n",
      "Next Action: [-1.310\n",
      "Step reward: -15.992067039152355, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.6811847787164\n",
      "Episode: 48, Step: 81\n",
      "Next Action: [-1.265\n",
      "Step reward: -15.995401747197372, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.6765865259138\n",
      "Episode: 48, Step: 82\n",
      "Next Action: [-1.228\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.6765865259138\n",
      "Episode: 48, Step: 83\n",
      "Next Action: [-1.109\n",
      "Step reward: -15.999251026152399, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.6758375520662\n",
      "Episode: 48, Step: 84\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.994313955960479, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.6701515080267\n",
      "Episode: 48, Step: 85\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.980575934806694, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.6507274428334\n",
      "Episode: 48, Step: 86\n",
      "Next Action: [-1.160\n",
      "Step reward: -15.980682603423759, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.631410046257\n",
      "Episode: 48, Step: 87\n",
      "Next Action: [-0.797\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.631410046257\n",
      "Episode: 48, Step: 88\n",
      "Next Action: [-0.649\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1417.631410046257\n",
      "Episode: 48, Step: 89\n",
      "Next Action: [-0.906\n",
      "Step reward: -15.993420598598536, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1433.6248306448556\n",
      "Episode: 48, Step: 90\n",
      "Next Action: [-0.810\n",
      "Step reward: -15.979019881785002, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1449.6038505266406\n",
      "Episode: 48, Step: 91\n",
      "Next Action: [-0.854\n",
      "Step reward: -15.971653067656794, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1465.5755035942973\n",
      "Episode: 48, Step: 92\n",
      "Next Action: [-1.038\n",
      "Step reward: -15.967019741982666, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1481.54252333628\n",
      "Episode: 48, Step: 93\n",
      "Next Action: [-1.463\n",
      "Step reward: -15.960733180450758, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1497.5032565167307\n",
      "Episode: 48, Step: 94\n",
      "Next Action: [-1.568\n",
      "Step reward: -15.960986866073597, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1513.4642433828044\n",
      "Episode: 48, Step: 95\n",
      "Next Action: [-1.503\n",
      "Step reward: -15.966438541682809, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1529.4306819244873\n",
      "Episode: 48, Step: 96\n",
      "Next Action: [-1.661\n",
      "Step reward: -15.974272163858593, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1545.404954088346\n",
      "Episode: 48, Step: 97\n",
      "Next Action: [-1.769\n",
      "Step reward: -15.975803893767868, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1561.3807579821137\n",
      "Episode: 48, Step: 98\n",
      "Next Action: [-1.587\n",
      "Step reward: -15.975566011309986, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1577.3563239934238\n",
      "Episode: 48, Step: 99\n",
      "Next Action: [-1.747\n",
      "Step reward: -15.978218315289459, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1593.3345423087133\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 21.700021743774414\n",
      "Actor loss: 42.60730743408203\n",
      "Critic loss: 3.197547435760498\n",
      "Actor loss: 58.309967041015625\n",
      "Critic loss: 1.9851970672607422\n",
      "Actor loss: 38.938575744628906\n",
      "Critic loss: 2.936661720275879\n",
      "Actor loss: 41.72636032104492\n",
      "Critic loss: 1.7826777696609497\n",
      "Actor loss: 59.95391845703125\n",
      "Critic loss: 28.718530654907227\n",
      "Actor loss: 42.52490234375\n",
      "Critic loss: 17.359704971313477\n",
      "Actor loss: 70.4237060546875\n",
      "Critic loss: 22.933271408081055\n",
      "Actor loss: 67.93928527832031\n",
      "Critic loss: 1.7632085084915161\n",
      "Actor loss: 63.343040466308594\n",
      "Critic loss: 4.573066234588623\n",
      "Actor loss: 58.49439239501953\n",
      "Episode: 49\n",
      "Episode: 49, Step: 0\n",
      "Next Action: [-1.762\n",
      "Step reward: -12.008757457235081, Next State: [-1.\n",
      "Total episode reward: -12.008757457235081\n",
      "Episode: 49, Step: 1\n",
      "Next Action: [-1.755\n",
      "Step reward: -15.286756514408063, Next State: [-1.\n",
      "Total episode reward: -27.295513971643146\n",
      "Episode: 49, Step: 2\n",
      "Next Action: [-2.059\n",
      "Step reward: -15.708271813686556, Next State: [-1.\n",
      "Total episode reward: -43.003785785329704\n",
      "Episode: 49, Step: 3\n",
      "Next Action: [-1.926\n",
      "Step reward: -15.824576426861997, Next State: [-1.\n",
      "Total episode reward: -58.8283622121917\n",
      "Episode: 49, Step: 4\n",
      "Next Action: [-1.435\n",
      "Step reward: -15.911550910447398, Next State: [-1.\n",
      "Total episode reward: -74.7399131226391\n",
      "Episode: 49, Step: 5\n",
      "Next Action: [-1.272\n",
      "Step reward: -15.950654724920378, Next State: [-1.\n",
      "Total episode reward: -90.69056784755948\n",
      "Episode: 49, Step: 6\n",
      "Next Action: [-1.479\n",
      "Step reward: -15.951047041909568, Next State: [-1.\n",
      "Total episode reward: -106.64161488946905\n",
      "Episode: 49, Step: 7\n",
      "Next Action: [-1.279\n",
      "Step reward: -15.948415087201274, Next State: [-1.\n",
      "Total episode reward: -122.59002997667032\n",
      "Episode: 49, Step: 8\n",
      "Next Action: [-1.248\n",
      "Step reward: -15.981704863187996, Next State: [-1.\n",
      "Total episode reward: -138.57173483985832\n",
      "Episode: 49, Step: 9\n",
      "Next Action: [-1.484\n",
      "Step reward: -15.985788239755415, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.55752307961373\n",
      "Episode: 49, Step: 10\n",
      "Next Action: [-1.415\n",
      "Step reward: -15.969731846874101, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.52725492648784\n",
      "Episode: 49, Step: 11\n",
      "Next Action: [-1.269\n",
      "Step reward: -15.975758981364768, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.50301390785262\n",
      "Episode: 49, Step: 12\n",
      "Next Action: [-1.449\n",
      "Step reward: -15.95655084819586, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.45956475604848\n",
      "Episode: 49, Step: 13\n",
      "Next Action: [-1.492\n",
      "Step reward: -15.952341768278695, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.41190652432718\n",
      "Episode: 49, Step: 14\n",
      "Next Action: [-1.423\n",
      "Step reward: -15.949975793597226, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.3618823179244\n",
      "Episode: 49, Step: 15\n",
      "Next Action: [-1.143\n",
      "Step reward: -15.954589703849578, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.31647202177396\n",
      "Episode: 49, Step: 16\n",
      "Next Action: [-0.966\n",
      "Step reward: -15.9726943223602, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.2891663441342\n",
      "Episode: 49, Step: 17\n",
      "Next Action: [-6.047\n",
      "Step reward: -15.968471000430078, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.2576373445643\n",
      "Episode: 49, Step: 18\n",
      "Next Action: [-0.488\n",
      "Step reward: -15.967249556230813, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.22488690079507\n",
      "Episode: 49, Step: 19\n",
      "Next Action: [-0.693\n",
      "Step reward: -15.963873637389902, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.18876053818497\n",
      "Episode: 49, Step: 20\n",
      "Next Action: [-0.925\n",
      "Step reward: -15.966684560963772, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.1554450991487\n",
      "Episode: 49, Step: 21\n",
      "Next Action: [-1.033\n",
      "Step reward: -15.981187820472575, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.1366329196213\n",
      "Episode: 49, Step: 22\n",
      "Next Action: [-1.086\n",
      "Step reward: -15.974533792710671, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.11116671233196\n",
      "Episode: 49, Step: 23\n",
      "Next Action: [-1.073\n",
      "Step reward: -15.997308155764417, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.1084748680964\n",
      "Episode: 49, Step: 24\n",
      "Next Action: [-1.011\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.1084748680964\n",
      "Episode: 49, Step: 25\n",
      "Next Action: [-1.019\n",
      "Step reward: -15.987453368902482, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.0959282369989\n",
      "Episode: 49, Step: 26\n",
      "Next Action: [-0.754\n",
      "Step reward: -15.997967201009777, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.09389543800864\n",
      "Episode: 49, Step: 27\n",
      "Next Action: [-1.009\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.09389543800864\n",
      "Episode: 49, Step: 28\n",
      "Next Action: [-1.387\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.09389543800864\n",
      "Episode: 49, Step: 29\n",
      "Next Action: [-1.374\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.09389543800864\n",
      "Episode: 49, Step: 30\n",
      "Next Action: [-1.498\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.09389543800864\n",
      "Episode: 49, Step: 31\n",
      "Next Action: [-1.294\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.09389543800864\n",
      "Episode: 49, Step: 32\n",
      "Next Action: [-1.186\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.0938954380086\n",
      "Episode: 49, Step: 33\n",
      "Next Action: [-1.166\n",
      "Step reward: -15.996431954885207, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.0903273928939\n",
      "Episode: 49, Step: 34\n",
      "Next Action: [-1.465\n",
      "Step reward: -15.991783642036896, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.0821110349308\n",
      "Episode: 49, Step: 35\n",
      "Next Action: [-1.304\n",
      "Step reward: -15.989927415812835, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.0720384507437\n",
      "Episode: 49, Step: 36\n",
      "Next Action: [-1.162\n",
      "Step reward: -15.994950482861535, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.0669889336052\n",
      "Episode: 49, Step: 37\n",
      "Next Action: [-0.953\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.0669889336052\n",
      "Episode: 49, Step: 38\n",
      "Next Action: [-0.894\n",
      "Step reward: -15.993177033788665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -618.0601659673939\n",
      "Episode: 49, Step: 39\n",
      "Next Action: [-0.780\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -634.0601659673939\n",
      "Episode: 49, Step: 40\n",
      "Next Action: [-0.999\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -650.0601659673939\n",
      "Episode: 49, Step: 41\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.98705808174427, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -666.0472240491382\n",
      "Episode: 49, Step: 42\n",
      "Next Action: [-0.884\n",
      "Step reward: -15.979662420885584, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -682.0268864700238\n",
      "Episode: 49, Step: 43\n",
      "Next Action: [-0.902\n",
      "Step reward: -15.972014135650323, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -697.9989006056741\n",
      "Episode: 49, Step: 44\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.968566054284373, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -713.9674666599584\n",
      "Episode: 49, Step: 45\n",
      "Next Action: [-0.850\n",
      "Step reward: -15.947768299908665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.9152349598671\n",
      "Episode: 49, Step: 46\n",
      "Next Action: [-0.508\n",
      "Step reward: -15.926030999007498, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.8412659588746\n",
      "Episode: 49, Step: 47\n",
      "Next Action: [-1.018\n",
      "Step reward: -15.922446038426836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.7637119973015\n",
      "Episode: 49, Step: 48\n",
      "Next Action: [-0.766\n",
      "Step reward: -15.945352828774466, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.709064826076\n",
      "Episode: 49, Step: 49\n",
      "Next Action: [-0.803\n",
      "Step reward: -15.927494751593171, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.6365595776691\n",
      "Episode: 49, Step: 50\n",
      "Next Action: [-0.880\n",
      "Step reward: -15.941018705494704, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.5775782831638\n",
      "Episode: 49, Step: 51\n",
      "Next Action: [-0.987\n",
      "Step reward: -15.96890723523632, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.5464855184001\n",
      "Episode: 49, Step: 52\n",
      "Next Action: [-0.789\n",
      "Step reward: -15.970045738669617, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.5165312570697\n",
      "Episode: 49, Step: 53\n",
      "Next Action: [-0.719\n",
      "Step reward: -15.99086525770342, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.5073965147731\n",
      "Episode: 49, Step: 54\n",
      "Next Action: [-0.847\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.5073965147731\n",
      "Episode: 49, Step: 55\n",
      "Next Action: [-0.692\n",
      "Step reward: -15.992832254727391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.5002287695005\n",
      "Episode: 49, Step: 56\n",
      "Next Action: [-1.222\n",
      "Step reward: -15.989736291674308, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.4899650611748\n",
      "Episode: 49, Step: 57\n",
      "Next Action: [-1.036\n",
      "Step reward: -15.990781226870075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.4807462880449\n",
      "Episode: 49, Step: 58\n",
      "Next Action: [-1.128\n",
      "Step reward: -15.990824105127722, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.4715703931726\n",
      "Episode: 49, Step: 59\n",
      "Next Action: [-1.005\n",
      "Step reward: -15.99446273253166, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.4660331257043\n",
      "Episode: 49, Step: 60\n",
      "Next Action: [-1.521\n",
      "Step reward: -15.99809651670856, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.4641296424128\n",
      "Episode: 49, Step: 61\n",
      "Next Action: [-1.326\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.4641296424128\n",
      "Episode: 49, Step: 62\n",
      "Next Action: [-1.410\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.4641296424128\n",
      "Episode: 49, Step: 63\n",
      "Next Action: [-1.322\n",
      "Step reward: -15.996349287689265, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.460478930102\n",
      "Episode: 49, Step: 64\n",
      "Next Action: [-1.437\n",
      "Step reward: -15.998931540292189, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.4594104703942\n",
      "Episode: 49, Step: 65\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.99385050360951, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.4532609740038\n",
      "Episode: 49, Step: 66\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.993128513067495, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.4463894870712\n",
      "Episode: 49, Step: 67\n",
      "Next Action: [-1.105\n",
      "Step reward: -15.987211386586633, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.4336008736577\n",
      "Episode: 49, Step: 68\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.98268539588925, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.416286269547\n",
      "Episode: 49, Step: 69\n",
      "Next Action: [-0.904\n",
      "Step reward: -15.977539078443554, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.3938253479905\n",
      "Episode: 49, Step: 70\n",
      "Next Action: [-1.281\n",
      "Step reward: -15.97775159543876, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.3715769434293\n",
      "Episode: 49, Step: 71\n",
      "Next Action: [-1.799\n",
      "Step reward: -15.97908235343041, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.3506592968597\n",
      "Episode: 49, Step: 72\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.976483598840353, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.3271428957\n",
      "Episode: 49, Step: 73\n",
      "Next Action: [-1.215\n",
      "Step reward: -15.971074983805053, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.298217879505\n",
      "Episode: 49, Step: 74\n",
      "Next Action: [-1.451\n",
      "Step reward: -15.969268100495848, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.2674859800009\n",
      "Episode: 49, Step: 75\n",
      "Next Action: [-1.282\n",
      "Step reward: -15.974339912920449, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.2418258929213\n",
      "Episode: 49, Step: 76\n",
      "Next Action: [-1.150\n",
      "Step reward: -15.994850736379846, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.2366766293012\n",
      "Episode: 49, Step: 77\n",
      "Next Action: [-1.263\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.2366766293012\n",
      "Episode: 49, Step: 78\n",
      "Next Action: [-1.462\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.2366766293012\n",
      "Episode: 49, Step: 79\n",
      "Next Action: [-1.406\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.2366766293012\n",
      "Episode: 49, Step: 80\n",
      "Next Action: [-1.225\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.2366766293012\n",
      "Episode: 49, Step: 81\n",
      "Next Action: [-1.202\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.2366766293012\n",
      "Episode: 49, Step: 82\n",
      "Next Action: [-1.593\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.2366766293012\n",
      "Episode: 49, Step: 83\n",
      "Next Action: [-1.206\n",
      "Step reward: -15.991420854055013, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.2280974833561\n",
      "Episode: 49, Step: 84\n",
      "Next Action: [-1.556\n",
      "Step reward: -15.98476488391568, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.2128623672718\n",
      "Episode: 49, Step: 85\n",
      "Next Action: [-1.155\n",
      "Step reward: -15.979355199805587, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.1922175670775\n",
      "Episode: 49, Step: 86\n",
      "Next Action: [-0.903\n",
      "Step reward: -15.992907443895586, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.185125010973\n",
      "Episode: 49, Step: 87\n",
      "Next Action: [-0.833\n",
      "Step reward: -15.975184705006177, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.160309715979\n",
      "Episode: 49, Step: 88\n",
      "Next Action: [-1.142\n",
      "Step reward: -15.967962621615142, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1417.1282723375944\n",
      "Episode: 49, Step: 89\n",
      "Next Action: [-1.144\n",
      "Step reward: -15.97642197020524, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1433.1046943077995\n",
      "Episode: 49, Step: 90\n",
      "Next Action: [-1.174\n",
      "Step reward: -15.970012809933799, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1449.0747071177334\n",
      "Episode: 49, Step: 91\n",
      "Next Action: [-0.942\n",
      "Step reward: -15.969893261198552, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1465.044600378932\n",
      "Episode: 49, Step: 92\n",
      "Next Action: [-1.001\n",
      "Step reward: -15.969090701572323, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1481.0136910805043\n",
      "Episode: 49, Step: 93\n",
      "Next Action: [-1.196\n",
      "Step reward: -15.985659858137762, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.999350938642\n",
      "Episode: 49, Step: 94\n",
      "Next Action: [-0.805\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.999350938642\n",
      "Episode: 49, Step: 95\n",
      "Next Action: [-0.886\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.999350938642\n",
      "Episode: 49, Step: 96\n",
      "Next Action: [-0.779\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.999350938642\n",
      "Episode: 49, Step: 97\n",
      "Next Action: [-0.875\n",
      "Step reward: -15.999310593799505, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.9986615324415\n",
      "Episode: 49, Step: 98\n",
      "Next Action: [-0.864\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.9986615324415\n",
      "Episode: 49, Step: 99\n",
      "Next Action: [-0.861\n",
      "Step reward: -15.994845648286951, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.9935071807283\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 10.285759925842285\n",
      "Actor loss: 37.2756462097168\n",
      "Critic loss: 8.566934585571289\n",
      "Actor loss: 41.59325408935547\n",
      "Critic loss: 4.583348751068115\n",
      "Actor loss: 52.51081085205078\n",
      "Critic loss: 5.919903755187988\n",
      "Actor loss: 51.277809143066406\n",
      "Critic loss: 3.0384345054626465\n",
      "Actor loss: 52.736366271972656\n",
      "Critic loss: 1.6518447399139404\n",
      "Actor loss: 58.42643356323242\n",
      "Critic loss: 1.6566401720046997\n",
      "Actor loss: 56.985748291015625\n",
      "Critic loss: 2.0607314109802246\n",
      "Actor loss: 61.76720428466797\n",
      "Critic loss: 34.84233856201172\n",
      "Actor loss: 35.128868103027344\n",
      "Critic loss: 9.697829246520996\n",
      "Actor loss: 67.50362396240234\n",
      "Episode: 50\n",
      "Episode: 50, Step: 0\n",
      "Next Action: [-0.827\n",
      "Step reward: -12.096008658595036, Next State: [-0.\n",
      "Total episode reward: -12.096008658595036\n",
      "Episode: 50, Step: 1\n",
      "Next Action: [-0.965\n",
      "Step reward: -15.010983249263125, Next State: [-1.\n",
      "Total episode reward: -27.10699190785816\n",
      "Episode: 50, Step: 2\n",
      "Next Action: [-0.831\n",
      "Step reward: -15.666711093844379, Next State: [-1.\n",
      "Total episode reward: -42.773703001702536\n",
      "Episode: 50, Step: 3\n",
      "Next Action: [-1.311\n",
      "Step reward: -15.930166089869916, Next State: [-1.\n",
      "Total episode reward: -58.70386909157245\n",
      "Episode: 50, Step: 4\n",
      "Next Action: [-1.506\n",
      "Step reward: -15.989403108937712, Next State: [-1.\n",
      "Total episode reward: -74.69327220051017\n",
      "Episode: 50, Step: 5\n",
      "Next Action: [-1.207\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Total episode reward: -90.69327220051017\n",
      "Episode: 50, Step: 6\n",
      "Next Action: [-1.050\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Total episode reward: -106.69327220051017\n",
      "Episode: 50, Step: 7\n",
      "Next Action: [-1.383\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Total episode reward: -122.69327220051017\n",
      "Episode: 50, Step: 8\n",
      "Next Action: [-1.669\n",
      "Step reward: -15.99308120840821, Next State: [-1. \n",
      "Total episode reward: -138.68635340891836\n",
      "Episode: 50, Step: 9\n",
      "Next Action: [-1.627\n",
      "Step reward: -15.999333244139034, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.6856866530574\n",
      "Episode: 50, Step: 10\n",
      "Next Action: [-1.544\n",
      "Step reward: -15.999489629679278, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.68517628273668\n",
      "Episode: 50, Step: 11\n",
      "Next Action: [-1.580\n",
      "Step reward: -15.98192329558427, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.66709957832094\n",
      "Episode: 50, Step: 12\n",
      "Next Action: [-1.401\n",
      "Step reward: -15.997543196863283, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.66464277518423\n",
      "Episode: 50, Step: 13\n",
      "Next Action: [-1.564\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.66464277518423\n",
      "Episode: 50, Step: 14\n",
      "Next Action: [-1.168\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.66464277518423\n",
      "Episode: 50, Step: 15\n",
      "Next Action: [-1.456\n",
      "Step reward: -15.99844803147498, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.66309080665923\n",
      "Episode: 50, Step: 16\n",
      "Next Action: [-1.333\n",
      "Step reward: -15.98968356711296, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.65277437377216\n",
      "Episode: 50, Step: 17\n",
      "Next Action: [-1.294\n",
      "Step reward: -15.983784463622568, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.6365588373947\n",
      "Episode: 50, Step: 18\n",
      "Next Action: [-1.296\n",
      "Step reward: -15.992294688005165, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.6288535253999\n",
      "Episode: 50, Step: 19\n",
      "Next Action: [-1.277\n",
      "Step reward: -15.955963866893423, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.5848173922933\n",
      "Episode: 50, Step: 20\n",
      "Next Action: [-1.274\n",
      "Step reward: -15.94840884038244, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.53322623267576\n",
      "Episode: 50, Step: 21\n",
      "Next Action: [-1.281\n",
      "Step reward: -15.962239074538385, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.49546530721415\n",
      "Episode: 50, Step: 22\n",
      "Next Action: [-1.061\n",
      "Step reward: -15.97389014724291, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.46935545445706\n",
      "Episode: 50, Step: 23\n",
      "Next Action: [-0.706\n",
      "Step reward: -15.994389392583127, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.4637448470402\n",
      "Episode: 50, Step: 24\n",
      "Next Action: [-0.496\n",
      "Step reward: -15.987242472612941, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.4509873196531\n",
      "Episode: 50, Step: 25\n",
      "Next Action: [-0.641\n",
      "Step reward: -15.988463926879517, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.4394512465326\n",
      "Episode: 50, Step: 26\n",
      "Next Action: [-0.457\n",
      "Step reward: -15.96785943051022, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.4073106770428\n",
      "Episode: 50, Step: 27\n",
      "Next Action: [-0.789\n",
      "Step reward: -15.970337318624141, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.37764799566696\n",
      "Episode: 50, Step: 28\n",
      "Next Action: [-0.773\n",
      "Step reward: -15.957076367018619, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.33472436268556\n",
      "Episode: 50, Step: 29\n",
      "Next Action: [-0.625\n",
      "Step reward: -15.962736702407705, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.2974610650933\n",
      "Episode: 50, Step: 30\n",
      "Next Action: [-0.540\n",
      "Step reward: -15.968887464594838, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.2663485296881\n",
      "Episode: 50, Step: 31\n",
      "Next Action: [-0.497\n",
      "Step reward: -15.96769004507899, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.2340385747671\n",
      "Episode: 50, Step: 32\n",
      "Next Action: [-0.758\n",
      "Step reward: -15.976245870433615, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.2102844452007\n",
      "Episode: 50, Step: 33\n",
      "Next Action: [-0.935\n",
      "Step reward: -15.97773077799174, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.1880152231925\n",
      "Episode: 50, Step: 34\n",
      "Next Action: [-0.990\n",
      "Step reward: -15.987266527367947, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.1752817505604\n",
      "Episode: 50, Step: 35\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.97903981733388, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.1543215678943\n",
      "Episode: 50, Step: 36\n",
      "Next Action: [-0.877\n",
      "Step reward: -15.972529341083023, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.1268509089773\n",
      "Episode: 50, Step: 37\n",
      "Next Action: [-0.873\n",
      "Step reward: -15.960619208908692, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.087470117886\n",
      "Episode: 50, Step: 38\n",
      "Next Action: [-1.368\n",
      "Step reward: -15.950309199715075, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -618.0377793176011\n",
      "Episode: 50, Step: 39\n",
      "Next Action: [-1.201\n",
      "Step reward: -15.967021853266125, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -634.0048011708673\n",
      "Episode: 50, Step: 40\n",
      "Next Action: [-1.329\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -650.0048011708673\n",
      "Episode: 50, Step: 41\n",
      "Next Action: [-1.554\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -666.0048011708673\n",
      "Episode: 50, Step: 42\n",
      "Next Action: [-1.767\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -682.0048011708673\n",
      "Episode: 50, Step: 43\n",
      "Next Action: [-1.699\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -698.0048011708673\n",
      "Episode: 50, Step: 44\n",
      "Next Action: [-1.616\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -714.0048011708673\n",
      "Episode: 50, Step: 45\n",
      "Next Action: [-1.429\n",
      "Step reward: -15.993333504430929, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -729.9981346752982\n",
      "Episode: 50, Step: 46\n",
      "Next Action: [-1.077\n",
      "Step reward: -15.985546127448684, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -745.9836808027468\n",
      "Episode: 50, Step: 47\n",
      "Next Action: [-1.093\n",
      "Step reward: -15.982857762876831, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -761.9665385656236\n",
      "Episode: 50, Step: 48\n",
      "Next Action: [-1.107\n",
      "Step reward: -15.98852260396215, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -777.9550611695857\n",
      "Episode: 50, Step: 49\n",
      "Next Action: [-1.195\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -793.9550611695857\n",
      "Episode: 50, Step: 50\n",
      "Next Action: [-1.611\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -809.9550611695857\n",
      "Episode: 50, Step: 51\n",
      "Next Action: [-1.537\n",
      "Step reward: -15.979853457605302, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -825.934914627191\n",
      "Episode: 50, Step: 52\n",
      "Next Action: [-1.088\n",
      "Step reward: -15.945156196131025, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -841.8800708233221\n",
      "Episode: 50, Step: 53\n",
      "Next Action: [-1.414\n",
      "Step reward: -15.938609162259295, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -857.8186799855813\n",
      "Episode: 50, Step: 54\n",
      "Next Action: [-1.230\n",
      "Step reward: -15.917853180482282, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -873.7365331660636\n",
      "Episode: 50, Step: 55\n",
      "Next Action: [-1.387\n",
      "Step reward: -15.90195224181352, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -889.6384854078772\n",
      "Episode: 50, Step: 56\n",
      "Next Action: [-1.391\n",
      "Step reward: -15.917475133874197, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -905.5559605417513\n",
      "Episode: 50, Step: 57\n",
      "Next Action: [-1.372\n",
      "Step reward: -15.93117095237565, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -921.487131494127\n",
      "Episode: 50, Step: 58\n",
      "Next Action: [-1.383\n",
      "Step reward: -15.952830231580858, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -937.4399617257078\n",
      "Episode: 50, Step: 59\n",
      "Next Action: [-1.044\n",
      "Step reward: -15.953102273052654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -953.3930639987605\n",
      "Episode: 50, Step: 60\n",
      "Next Action: [-0.735\n",
      "Step reward: -15.970453158899085, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -969.3635171576595\n",
      "Episode: 50, Step: 61\n",
      "Next Action: [-0.869\n",
      "Step reward: -15.949081662982806, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -985.3125988206424\n",
      "Episode: 50, Step: 62\n",
      "Next Action: [-0.849\n",
      "Step reward: -15.948218281387343, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1001.2608171020297\n",
      "Episode: 50, Step: 63\n",
      "Next Action: [-1.097\n",
      "Step reward: -15.958341743529658, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1017.2191588455594\n",
      "Episode: 50, Step: 64\n",
      "Next Action: [-1.330\n",
      "Step reward: -15.979464830674347, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1033.1986236762336\n",
      "Episode: 50, Step: 65\n",
      "Next Action: [-1.536\n",
      "Step reward: -15.966032903024466, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1049.164656579258\n",
      "Episode: 50, Step: 66\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.96745611359685, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1065.132112692855\n",
      "Episode: 50, Step: 67\n",
      "Next Action: [-1.127\n",
      "Step reward: -15.966905117228391, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1081.0990178100833\n",
      "Episode: 50, Step: 68\n",
      "Next Action: [-1.466\n",
      "Step reward: -15.951436854804854, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1097.050454664888\n",
      "Episode: 50, Step: 69\n",
      "Next Action: [-1.592\n",
      "Step reward: -15.938447365107393, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1112.9889020299954\n",
      "Episode: 50, Step: 70\n",
      "Next Action: [-1.485\n",
      "Step reward: -15.95114337558996, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1128.9400454055854\n",
      "Episode: 50, Step: 71\n",
      "Next Action: [-1.318\n",
      "Step reward: -15.980170570252092, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1144.9202159758374\n",
      "Episode: 50, Step: 72\n",
      "Next Action: [-1.183\n",
      "Step reward: -15.986685380311393, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1160.9069013561489\n",
      "Episode: 50, Step: 73\n",
      "Next Action: [-1.104\n",
      "Step reward: -15.999845248883029, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1176.906746605032\n",
      "Episode: 50, Step: 74\n",
      "Next Action: [-0.992\n",
      "Step reward: -15.990635961665026, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1192.897382566697\n",
      "Episode: 50, Step: 75\n",
      "Next Action: [-0.777\n",
      "Step reward: -15.980129234950443, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1208.8775118016474\n",
      "Episode: 50, Step: 76\n",
      "Next Action: [-0.950\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1224.8775118016474\n",
      "Episode: 50, Step: 77\n",
      "Next Action: [-1.189\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1240.8775118016474\n",
      "Episode: 50, Step: 78\n",
      "Next Action: [-1.570\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1256.8775118016474\n",
      "Episode: 50, Step: 79\n",
      "Next Action: [-1.039\n",
      "Step reward: -15.995871905064929, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1272.8733837067123\n",
      "Episode: 50, Step: 80\n",
      "Next Action: [-1.345\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1288.8733837067123\n",
      "Episode: 50, Step: 81\n",
      "Next Action: [-1.357\n",
      "Step reward: -15.984121092427271, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1304.8575047991396\n",
      "Episode: 50, Step: 82\n",
      "Next Action: [-1.627\n",
      "Step reward: -15.980612885752901, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1320.8381176848925\n",
      "Episode: 50, Step: 83\n",
      "Next Action: [-1.709\n",
      "Step reward: -15.986169068506728, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1336.8242867533993\n",
      "Episode: 50, Step: 84\n",
      "Next Action: [-1.537\n",
      "Step reward: -15.976181827907446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1352.8004685813066\n",
      "Episode: 50, Step: 85\n",
      "Next Action: [-1.517\n",
      "Step reward: -15.972915274984635, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1368.7733838562913\n",
      "Episode: 50, Step: 86\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.980872294245277, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1384.7542561505365\n",
      "Episode: 50, Step: 87\n",
      "Next Action: [-1.237\n",
      "Step reward: -15.97724710635872, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1400.7315032568952\n",
      "Episode: 50, Step: 88\n",
      "Next Action: [-1.399\n",
      "Step reward: -15.971587410837609, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1416.7030906677328\n",
      "Episode: 50, Step: 89\n",
      "Next Action: [-1.430\n",
      "Step reward: -15.968389181457486, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1432.6714798491903\n",
      "Episode: 50, Step: 90\n",
      "Next Action: [-1.288\n",
      "Step reward: -15.984766890396317, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1448.6562467395865\n",
      "Episode: 50, Step: 91\n",
      "Next Action: [-1.356\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1464.6562467395865\n",
      "Episode: 50, Step: 92\n",
      "Next Action: [-1.376\n",
      "Step reward: -15.994007501186415, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1480.650254240773\n",
      "Episode: 50, Step: 93\n",
      "Next Action: [-1.593\n",
      "Step reward: -15.999113716147159, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1496.64936795692\n",
      "Episode: 50, Step: 94\n",
      "Next Action: [-1.540\n",
      "Step reward: -15.995614686156337, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1512.6449826430764\n",
      "Episode: 50, Step: 95\n",
      "Next Action: [-1.585\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1528.6449826430764\n",
      "Episode: 50, Step: 96\n",
      "Next Action: [-1.334\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1544.6449826430764\n",
      "Episode: 50, Step: 97\n",
      "Next Action: [-1.450\n",
      "Step reward: -15.998784742115632, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1560.6437673851922\n",
      "Episode: 50, Step: 98\n",
      "Next Action: [-1.549\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1576.6437673851922\n",
      "Episode: 50, Step: 99\n",
      "Next Action: [-1.438\n",
      "Step reward: -15.996835640087879, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1592.6406030252801\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 10.494027137756348\n",
      "Actor loss: 49.070716857910156\n",
      "Critic loss: 1.4353305101394653\n",
      "Actor loss: 59.228485107421875\n",
      "Critic loss: 1.8602430820465088\n",
      "Actor loss: 65.18061828613281\n",
      "Critic loss: 10.006091117858887\n",
      "Actor loss: 31.412410736083984\n",
      "Critic loss: 1.5594161748886108\n",
      "Actor loss: 61.33381271362305\n",
      "Critic loss: 2.1381115913391113\n",
      "Actor loss: 62.18927001953125\n",
      "Critic loss: 1.436457872390747\n",
      "Actor loss: 60.66727828979492\n",
      "Critic loss: 1.9352893829345703\n",
      "Actor loss: 56.75732421875\n",
      "Critic loss: 1.8366477489471436\n",
      "Actor loss: 50.317657470703125\n",
      "Critic loss: 2.9161264896392822\n",
      "Actor loss: 51.22802734375\n",
      "Episode: 51\n",
      "Episode: 51, Step: 0\n",
      "Next Action: [-1.479\n",
      "Step reward: -12.508343854442373, Next State: [-1.\n",
      "Total episode reward: -12.508343854442373\n",
      "Episode: 51, Step: 1\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.189856123140437, Next State: [-1.\n",
      "Total episode reward: -27.69819997758281\n",
      "Episode: 51, Step: 2\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.757551141366692, Next State: [-1.\n",
      "Total episode reward: -43.4557511189495\n",
      "Episode: 51, Step: 3\n",
      "Next Action: [-1.438\n",
      "Step reward: -15.844877348730385, Next State: [-1.\n",
      "Total episode reward: -59.300628467679886\n",
      "Episode: 51, Step: 4\n",
      "Next Action: [-1.408\n",
      "Step reward: -15.90310965170394, Next State: [-1. \n",
      "Total episode reward: -75.20373811938383\n",
      "Episode: 51, Step: 5\n",
      "Next Action: [-1.141\n",
      "Step reward: -15.947711158060274, Next State: [-1.\n",
      "Total episode reward: -91.1514492774441\n",
      "Episode: 51, Step: 6\n",
      "Next Action: [-1.016\n",
      "Step reward: -15.970128049477697, Next State: [-1.\n",
      "Total episode reward: -107.1215773269218\n",
      "Episode: 51, Step: 7\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.955849574973081, Next State: [-1.\n",
      "Total episode reward: -123.07742690189488\n",
      "Episode: 51, Step: 8\n",
      "Next Action: [-0.931\n",
      "Step reward: -15.939171750541622, Next State: [-1.\n",
      "Total episode reward: -139.0165986524365\n",
      "Episode: 51, Step: 9\n",
      "Next Action: [-1.178\n",
      "Step reward: -15.92438299159071, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -154.9409816440272\n",
      "Episode: 51, Step: 10\n",
      "Next Action: [-0.971\n",
      "Step reward: -15.923650562212798, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -170.86463220624\n",
      "Episode: 51, Step: 11\n",
      "Next Action: [-0.962\n",
      "Step reward: -15.9736676743192, Next State: [-1.  \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -186.8382998805592\n",
      "Episode: 51, Step: 12\n",
      "Next Action: [-0.776\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -202.8382998805592\n",
      "Episode: 51, Step: 13\n",
      "Next Action: [-0.592\n",
      "Step reward: -15.990099015965928, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -218.82839889652513\n",
      "Episode: 51, Step: 14\n",
      "Next Action: [-0.697\n",
      "Step reward: -15.98356246329926, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -234.81196135982438\n",
      "Episode: 51, Step: 15\n",
      "Next Action: [-0.849\n",
      "Step reward: -15.980931486596129, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -250.7928928464205\n",
      "Episode: 51, Step: 16\n",
      "Next Action: [-0.852\n",
      "Step reward: -15.996568413346601, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -266.7894612597671\n",
      "Episode: 51, Step: 17\n",
      "Next Action: [-1.032\n",
      "Step reward: -15.994098148654178, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -282.7835594084213\n",
      "Episode: 51, Step: 18\n",
      "Next Action: [-1.138\n",
      "Step reward: -15.982408049167823, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -298.7659674575891\n",
      "Episode: 51, Step: 19\n",
      "Next Action: [-0.908\n",
      "Step reward: -15.974886575270324, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -314.74085403285943\n",
      "Episode: 51, Step: 20\n",
      "Next Action: [-0.774\n",
      "Step reward: -15.970890342137206, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -330.7117443749966\n",
      "Episode: 51, Step: 21\n",
      "Next Action: [-0.722\n",
      "Step reward: -15.97225161559919, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -346.6839959905958\n",
      "Episode: 51, Step: 22\n",
      "Next Action: [-0.637\n",
      "Step reward: -15.97445474016862, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -362.6584507307644\n",
      "Episode: 51, Step: 23\n",
      "Next Action: [-0.837\n",
      "Step reward: -15.994393595250735, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -378.65284432601516\n",
      "Episode: 51, Step: 24\n",
      "Next Action: [-0.620\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -394.65284432601516\n",
      "Episode: 51, Step: 25\n",
      "Next Action: [-0.571\n",
      "Step reward: -15.998032916534427, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -410.6508772425496\n",
      "Episode: 51, Step: 26\n",
      "Next Action: [-0.195\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -426.6508772425496\n",
      "Episode: 51, Step: 27\n",
      "Next Action: [-3.828\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -442.6508772425496\n",
      "Episode: 51, Step: 28\n",
      "Next Action: [-0.635\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -458.6508772425496\n",
      "Episode: 51, Step: 29\n",
      "Next Action: [-0.636\n",
      "Step reward: -15.994884361089628, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -474.64576160363924\n",
      "Episode: 51, Step: 30\n",
      "Next Action: [-0.938\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -490.64576160363924\n",
      "Episode: 51, Step: 31\n",
      "Next Action: [-1.076\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -506.64576160363924\n",
      "Episode: 51, Step: 32\n",
      "Next Action: [-1.131\n",
      "Step reward: -15.997112411109942, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -522.6428740147492\n",
      "Episode: 51, Step: 33\n",
      "Next Action: [-1.419\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -538.6428740147492\n",
      "Episode: 51, Step: 34\n",
      "Next Action: [-1.524\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -554.6428740147492\n",
      "Episode: 51, Step: 35\n",
      "Next Action: [-1.501\n",
      "Step reward: -15.99510952906851, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -570.6379835438177\n",
      "Episode: 51, Step: 36\n",
      "Next Action: [-1.494\n",
      "Step reward: -15.985422451680897, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -586.6234059954985\n",
      "Episode: 51, Step: 37\n",
      "Next Action: [-1.537\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -602.6234059954985\n",
      "Episode: 51, Step: 38\n",
      "Next Action: [-1.640\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -618.6234059954985\n",
      "Episode: 51, Step: 39\n",
      "Next Action: [-1.529\n",
      "Step reward: -15.995675541605612, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -634.6190815371041\n",
      "Episode: 51, Step: 40\n",
      "Next Action: [-1.441\n",
      "Step reward: -15.985575824929718, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -650.6046573620338\n",
      "Episode: 51, Step: 41\n",
      "Next Action: [-1.649\n",
      "Step reward: -15.972157331036433, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -666.5768146930702\n",
      "Episode: 51, Step: 42\n",
      "Next Action: [-1.727\n",
      "Step reward: -15.96733499384753, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -682.5441496869178\n",
      "Episode: 51, Step: 43\n",
      "Next Action: [-1.823\n",
      "Step reward: -15.959960547629025, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -698.5041102345468\n",
      "Episode: 51, Step: 44\n",
      "Next Action: [-1.880\n",
      "Step reward: -15.967444768798611, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -714.4715550033454\n",
      "Episode: 51, Step: 45\n",
      "Next Action: [-1.524\n",
      "Step reward: -15.971554146107774, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -730.4431091494532\n",
      "Episode: 51, Step: 46\n",
      "Next Action: [-1.518\n",
      "Step reward: -15.966281061509253, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -746.4093902109624\n",
      "Episode: 51, Step: 47\n",
      "Next Action: [-1.568\n",
      "Step reward: -15.989046443198765, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -762.3984366541612\n",
      "Episode: 51, Step: 48\n",
      "Next Action: [-1.558\n",
      "Step reward: -15.99378266408156, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -778.3922193182427\n",
      "Episode: 51, Step: 49\n",
      "Next Action: [-1.307\n",
      "Step reward: -15.979683096649172, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -794.3719024148919\n",
      "Episode: 51, Step: 50\n",
      "Next Action: [-1.522\n",
      "Step reward: -15.991958500904296, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -810.3638609157962\n",
      "Episode: 51, Step: 51\n",
      "Next Action: [-1.459\n",
      "Step reward: -15.994879126839125, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -826.3587400426353\n",
      "Episode: 51, Step: 52\n",
      "Next Action: [-1.219\n",
      "Step reward: -15.98317304692828, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -842.3419130895636\n",
      "Episode: 51, Step: 53\n",
      "Next Action: [-1.225\n",
      "Step reward: -15.967582164720557, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -858.3094952542841\n",
      "Episode: 51, Step: 54\n",
      "Next Action: [-0.975\n",
      "Step reward: -15.988225444856573, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -874.2977206991407\n",
      "Episode: 51, Step: 55\n",
      "Next Action: [-0.596\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -890.2977206991407\n",
      "Episode: 51, Step: 56\n",
      "Next Action: [-0.891\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -906.2977206991407\n",
      "Episode: 51, Step: 57\n",
      "Next Action: [-0.722\n",
      "Step reward: -16.0, Next State: [-1.  1. -1. -1. -\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -922.2977206991407\n",
      "Episode: 51, Step: 58\n",
      "Next Action: [-0.973\n",
      "Step reward: -15.997209159590176, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -938.2949298587308\n",
      "Episode: 51, Step: 59\n",
      "Next Action: [-1.015\n",
      "Step reward: -15.999806981423486, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -954.2947368401543\n",
      "Episode: 51, Step: 60\n",
      "Next Action: [-0.962\n",
      "Step reward: -15.996979785944689, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -970.2917166260991\n",
      "Episode: 51, Step: 61\n",
      "Next Action: [-1.158\n",
      "Step reward: -15.993162465473397, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -986.2848790915724\n",
      "Episode: 51, Step: 62\n",
      "Next Action: [-0.905\n",
      "Step reward: -15.979340855222693, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1002.2642199467952\n",
      "Episode: 51, Step: 63\n",
      "Next Action: [-0.754\n",
      "Step reward: -15.968214301954712, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1018.2324342487499\n",
      "Episode: 51, Step: 64\n",
      "Next Action: [-0.427\n",
      "Step reward: -15.974992040475472, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1034.2074262892254\n",
      "Episode: 51, Step: 65\n",
      "Next Action: [-0.479\n",
      "Step reward: -15.969865512974938, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1050.1772918022004\n",
      "Episode: 51, Step: 66\n",
      "Next Action: [-0.137\n",
      "Step reward: -15.950167021268442, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1066.127458823469\n",
      "Episode: 51, Step: 67\n",
      "Next Action: [-0.326\n",
      "Step reward: -15.941774885358836, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1082.0692337088278\n",
      "Episode: 51, Step: 68\n",
      "Next Action: [-0.779\n",
      "Step reward: -15.937582115450653, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1098.0068158242784\n",
      "Episode: 51, Step: 69\n",
      "Next Action: [-0.586\n",
      "Step reward: -15.93681873607558, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1113.943634560354\n",
      "Episode: 51, Step: 70\n",
      "Next Action: [-0.717\n",
      "Step reward: -15.951129216320654, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1129.8947637766746\n",
      "Episode: 51, Step: 71\n",
      "Next Action: [-0.867\n",
      "Step reward: -15.980156970693585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1145.8749207473682\n",
      "Episode: 51, Step: 72\n",
      "Next Action: [-1.071\n",
      "Step reward: -15.97725033896261, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1161.8521710863308\n",
      "Episode: 51, Step: 73\n",
      "Next Action: [-1.241\n",
      "Step reward: -15.978577149694019, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1177.8307482360249\n",
      "Episode: 51, Step: 74\n",
      "Next Action: [-1.283\n",
      "Step reward: -15.96044607967454, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1193.7911943156994\n",
      "Episode: 51, Step: 75\n",
      "Next Action: [-0.974\n",
      "Step reward: -15.960431222213247, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1209.7516255379126\n",
      "Episode: 51, Step: 76\n",
      "Next Action: [-0.691\n",
      "Step reward: -15.96872466170937, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1225.7203501996219\n",
      "Episode: 51, Step: 77\n",
      "Next Action: [-0.683\n",
      "Step reward: -15.973942115007665, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1241.6942923146296\n",
      "Episode: 51, Step: 78\n",
      "Next Action: [-0.947\n",
      "Step reward: -15.985793545248034, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1257.6800858598776\n",
      "Episode: 51, Step: 79\n",
      "Next Action: [-0.945\n",
      "Step reward: -15.988071686884627, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1273.6681575467624\n",
      "Episode: 51, Step: 80\n",
      "Next Action: [-0.977\n",
      "Step reward: -15.985234527351944, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1289.6533920741142\n",
      "Episode: 51, Step: 81\n",
      "Next Action: [-0.979\n",
      "Step reward: -15.989345757326875, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1305.642737831441\n",
      "Episode: 51, Step: 82\n",
      "Next Action: [-1.046\n",
      "Step reward: -15.993737395777984, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1321.636475227219\n",
      "Episode: 51, Step: 83\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.980887991208585, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1337.6173632184277\n",
      "Episode: 51, Step: 84\n",
      "Next Action: [-1.084\n",
      "Step reward: -15.968971277531503, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1353.5863344959591\n",
      "Episode: 51, Step: 85\n",
      "Next Action: [-0.692\n",
      "Step reward: -15.983063587248848, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1369.569398083208\n",
      "Episode: 51, Step: 86\n",
      "Next Action: [-0.791\n",
      "Step reward: -15.997582245697755, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1385.5669803289056\n",
      "Episode: 51, Step: 87\n",
      "Next Action: [-1.031\n",
      "Step reward: -15.98965659019078, Next State: [-1. \n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1401.5566369190965\n",
      "Episode: 51, Step: 88\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.981830097129366, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1417.538467016226\n",
      "Episode: 51, Step: 89\n",
      "Next Action: [-1.101\n",
      "Step reward: -15.975819671129267, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1433.5142866873553\n",
      "Episode: 51, Step: 90\n",
      "Next Action: [-1.202\n",
      "Step reward: -15.973803342247978, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1449.4880900296032\n",
      "Episode: 51, Step: 91\n",
      "Next Action: [-1.291\n",
      "Step reward: -15.989168076164827, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1465.477258105768\n",
      "Episode: 51, Step: 92\n",
      "Next Action: [-1.400\n",
      "Step reward: -15.992679328038893, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1481.469937433807\n",
      "Episode: 51, Step: 93\n",
      "Next Action: [-1.373\n",
      "Step reward: -15.990658701896278, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1497.4605961357033\n",
      "Episode: 51, Step: 94\n",
      "Next Action: [-1.224\n",
      "Step reward: -15.986688446361464, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1513.4472845820649\n",
      "Episode: 51, Step: 95\n",
      "Next Action: [-0.915\n",
      "Step reward: -15.995046427731047, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1529.442331009796\n",
      "Episode: 51, Step: 96\n",
      "Next Action: [-0.936\n",
      "Step reward: -15.977338134377446, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1545.4196691441734\n",
      "Episode: 51, Step: 97\n",
      "Next Action: [-0.961\n",
      "Step reward: -15.975554237420269, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1561.3952233815937\n",
      "Episode: 51, Step: 98\n",
      "Next Action: [-1.063\n",
      "Step reward: -15.984963341842544, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1577.3801867234363\n",
      "Episode: 51, Step: 99\n",
      "Next Action: [-1.139\n",
      "Step reward: -15.984469227843586, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -1593.36465595128\n",
      "Training with 10 episode sequences.\n",
      "Critic loss: 2.27411150932312\n",
      "Actor loss: 50.72624969482422\n",
      "Critic loss: 1.5159188508987427\n",
      "Actor loss: 49.255882263183594\n",
      "Critic loss: 17.91126251220703\n",
      "Actor loss: 43.396324157714844\n",
      "Critic loss: 1.8428188562393188\n",
      "Actor loss: 61.754920959472656\n",
      "Critic loss: 3.5582268238067627\n",
      "Actor loss: 62.71942138671875\n",
      "Critic loss: 130.5714569091797\n",
      "Actor loss: 23.9442081451416\n",
      "Critic loss: 3.8071129322052\n",
      "Actor loss: 46.76055908203125\n",
      "Critic loss: 450.5292663574219\n",
      "Actor loss: 55.17510986328125\n",
      "Critic loss: 4.13071346282959\n",
      "Actor loss: 60.09480667114258\n",
      "Critic loss: 2.110229015350342\n",
      "Actor loss: 44.51411056518555\n",
      "Episode: 52\n",
      "Episode: 52, Step: 0\n",
      "Next Action: [-1.089\n",
      "Step reward: -11.508562098451709, Next State: [-1.\n",
      "Total episode reward: -11.508562098451709\n",
      "Episode: 52, Step: 1\n",
      "Next Action: [-1.254\n",
      "Step reward: -14.97571107865928, Next State: [-1. \n",
      "Total episode reward: -26.48427317711099\n",
      "Episode: 52, Step: 2\n",
      "Next Action: [-1.057\n",
      "Step reward: -15.581300381048326, Next State: [-1.\n",
      "Total episode reward: -42.065573558159315\n",
      "Episode: 52, Step: 3\n",
      "Next Action: [-1.176\n",
      "Step reward: -15.730306796084816, Next State: [-1.\n",
      "Total episode reward: -57.79588035424413\n",
      "Episode: 52, Step: 4\n",
      "Next Action: [-1.288\n",
      "Step reward: -15.868355931993417, Next State: [-1.\n",
      "Total episode reward: -73.66423628623755\n",
      "Episode: 52, Step: 5\n",
      "Next Action: [-1.508\n",
      "Step reward: -15.924760286256452, Next State: [-1.\n",
      "Total episode reward: -89.58899657249401\n",
      "Episode: 52, Step: 6\n",
      "Next Action: [-1.518\n",
      "Step reward: -15.943058998215413, Next State: [-1.\n",
      "Total episode reward: -105.53205557070942\n",
      "Episode: 52, Step: 7\n",
      "Next Action: [-1.363\n",
      "Step reward: -15.994494308077178, Next State: [-1.\n",
      "Total episode reward: -121.5265498787866\n",
      "Episode: 52, Step: 8\n",
      "Next Action: [-1.231\n",
      "Step reward: -15.960676825111547, Next State: [-1.\n",
      "Total episode reward: -137.48722670389813\n",
      "Episode: 52, Step: 9\n",
      "Next Action: [-1.216\n",
      "Step reward: -15.959125081626441, Next State: [-1.\n",
      "Adding episode sequence, size: 10\n",
      "Total episode reward: -153.44635178552457\n",
      "Episode: 52, Step: 10\n"
     ]
    }
   ],
   "source": [
    "for episode_number in range(TOTAL_EPISODES):\n",
    "    print(f\"Episode: {episode_number}\")\n",
    "    state = env.reset()  # Initial state.\n",
    "    episode_reward = 0\n",
    "    episode = []  # This will store sequences of experiences\n",
    "    for step_number in range(MAX_STEPS_PER_EPISODE):\n",
    "        print(f\"Episode: {episode_number}, Step: {step_number}\")\n",
    "\n",
    "        # Select an action using the Actor model. Add noise for exploration.\n",
    "        action = actor.model(state.reshape(1, -1))[0].numpy() + noise_process.noise()\n",
    "        print(f\"Next Action: {action}\"[:20])\n",
    "        \n",
    "        # Take the action and observe the next state and reward.\n",
    "        next_state, reward, done = env.step(action)\n",
    "        print(f\"Step reward: {reward}, Next State: {next_state}\"[:50])\n",
    "\n",
    "        # Append the transition to the episode\n",
    "        episode.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Check if the episode has accumulated enough steps\n",
    "        if len(episode) >= STEPS_FORWARD:\n",
    "            print(f\"Adding episode sequence, size: {len(episode)}\")\n",
    "            buffer.add(episode[:STEPS_FORWARD])  # Add only the first STEPS_FORWARD transitions\n",
    "            episode.pop(0)  # Remove the oldest transition to maintain a moving window\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        print(f\"Total episode reward: {episode_reward}\")\n",
    "\n",
    "        # If the episode is done, break.\n",
    "        if done:\n",
    "            print(\"End of Episode!\")\n",
    "            print(f\"Episode: {episode_number + 1}, Step: {step_number + 1}, Reward: {episode_reward}\")\n",
    "            break\n",
    "\n",
    "    # Periodically train the model.\n",
    "    if len(buffer.buffer) >= TRAIN_BUFFER_SIZE:\n",
    "        batch = buffer.sample(TRAIN_BUFFER_SIZE)\n",
    "        train_actor_and_critic(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cda86-08d3-49db-801d-34b22ac1dbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
